\chapter{Substructural logical specifications}
\label{chapter-framework}

In this chapter, we discuss the derivation of a logical framework
\sls. The framework \sls~is justified as a fragment of the
logic \ollll~from Chapter~3, though we modify the presentation 
significantly, both to simplify the appearance of proof terms and to
facilitate talking about partial proofs: we explain partial 

The specifics of the typed term language of \ollll~were omitted; in
Section~\ref{sec:sls-termlanguage} we give a careful presentation of
the term language for \sls, Spine Form LF. In
Section~\ref{sec:slsframework} we present \sls~as a fragment of
\ollll, and in Section~\ref{sec:framework-concurrenteq} we discuss
{\it concurrent equivalence}, a coarser equivalence on \sls~proof
terms than the one given by focused \ollll. In
Section~\ref{sec:sls-adequate} we adopt the methodology of adequate
encoding to \sls. In Section~\ref{sec:prototype} we cover the SLS
prototype implementation. Finally, in
Section~\ref{sec:designdecisions}, we discuss some of the design
decisions reflected in the design of \sls~and how they could have been
made differently.

\section{Spine Form LF as a term language}
\label{sec:sls-termlanguage}

Other substructural logical frameworks, like Cervesato and Pfenning's
LLF \cite{cervesato02linear}, Polakow's OLF \cite{polakow01ordered},
and Watkins et al.'s CLF \cite{watkins02concurrent} are {\it
  fully-dependent type theories}: the language of terms, the domain of
first-order quantification, is the same as the language of proof
terms, the representatives of logical derivations (we will call the
domain of quantification the {\it object terms} when ``terms'' would
be ambiguous). The logical framework \sls~presented in this chapter
breaks from this tradition. The language of first-order
quantification, which was left unspecified in Chapter 3, will be
presently described as Spine Form LF, a well-understood logical
framework derived from the normal forms of the purely persistent type
theory LF \cite{harper93framework}.

All the information in this section is standard and adequately
presented between Harper, Honsell, and Plotkin's original presentation
of LF \cite{harper93framework}, Cervesato and Pfenning's discussion of
spine form terms \cite{cervesato02linear}, Watkins et al.'s
presentation of the canonical forms of CLF \cite{watkins02concurrent},
Nanevski et al.'s dependent contextual modal type theory
\cite{nanevski08contextual}, Harper and Licata's discussion of
Canonical LF \cite{harper07mechanizing}, and Reed's spine form
presentation of HLF \cite{reed09hybrid}. 

It would be entirely consistent for us to appropriate Harper and
Licata's Canonical LF presentation instead of presenting Spine Form
LF. Nevertheless, a spine-form presentation of canonical LF serves to
make our presentation more uniform, as spines are used in the proof
term language of \sls. Canonical term languages like Canonical LF
correspond to normal natural deduction presentations of logic, whereas
spine form term languages correspond to focused sequent calculus
presentations like the ones we have considered thus far.

\subsection{Core syntax}

The syntax of Spine Form LF is extended in two places to handle \sls:
rules ${\sf r} : A^-$ in the signature contain negative \sls~types
$A^-$ (though it would be possible to separate out the LF portion of
signatures from the \sls~rules). % We also add four additional kinds,
% ${\sf prop}$, which classifies negative ordered atomic types $p^-$,
% ${\sf prop}\,{\sf ord}$, which classifies positive ordered atomic
% types $p^+$, ${\sf prop}\,{\sf lin}$, which classifies positive
% linear/mobile/ephemeral atomic types $p^+_\meph$, and ${\sf
%   prop}\,{\sf ord}$, which classifies positive persistent atomic types
% $p^+_\mpers$. 
% Other than the extra kinds classifying atomic \sls~propositions, kinds
% $\kappa$ are otherwise exactly as they are in other presentations of
% LF; kinds classify types $\tau$, and types $\tau$ classify normal
% terms $\lf{t}$ and spines $\lf{\spi}$. Kinds $\kappa$ and types $\tau$
% are both treated as syntactic refinements of {\it classifiers} $\nu$. 
\begin{align*}
& \mbox{Signatures} & \Sigma & ::= \cdot 
  \mid \Sigma, \lf{\sf c} : \tau
  \mid \Sigma, {\sf a} : \kappa
  \mid \Sigma, {\sf r} : A^-
\\
& \mbox{Variables} & \lf{a}, \lf{b} & ::= \ldots
\\
& \mbox{Variable contexts} & \Psi & ::= \cdot
  \mid \Psi, \lf{a} {:} \tau 
\\
& \mbox{Classifiers} & \nu & ::= \lfpi{x}{\nu}{\nu'} \mid {\sf type}
  \mid {\sf prop}
  \mid {\sf prop}\,{\sf ord}
  \mid {\sf prop}\,{\sf lin}
  \mid {\sf prop}\,{\sf pers}
  \mid \lfroot{\sf a}{\spi}
\\
& \mbox{Heads} & \lf{h} & ::= \lf{a} \mid \lf{\sf c}
\\
& \mbox{Normal terms} & \lf{t}, \lf{s} & ::= \lf{\lambda a.t}
  \mid \lf{\lfroot{h}{\spi}}
\\
& \mbox{Spines} & \lf{\spi} & ::= \lf{t; \spi} \mid \lf{\lfnil}
\\
& \mbox{Substitutions} & \lf{\sigma} & ::= \lf{\cdot}
  \mid \lf{t/a, \sigma}
  \mid \lf{b/\!\!/a, \sigma}
\end{align*}

\noindent
Classifiers $\nu$ can be divided into three refinements.  Types $\tau$
are either function types $\lfpi{a}{\tau}{\tau'}$ or base types
$\lfroot{\sf a}{\spi}$.  Kinds $\kappa$ are either families
$\lfpi{a}{\tau}{\kappa}$ or one of the base kinds: ${\sf prop}$, ${\sf
  prop}\,{\sf ord}$, ${\sf prop}\,{\sf lin}$, or ${\sf prop}\,{\sf
  pers}$. Atomic classifiers $p$ have the form $\lfroot{\sf a}{\spi}$;
they can be atomic types of LF or atomic propositions of \sls.

LF spines $\lf\spi$ are just sequences of terms $\lf{(t_1; (\ldots;
  (t_n;())\ldots))}$; we will follow common convention and write
$\lf{h\,t_1\ldots t_n}$ as a convenient shorthand for the atomic term
$\lf{\lfroot{h}{(t_1; \ldots; (t_n;())\ldots)}}$; similarly, we will
write ${\sf a}\,\lf{t_1\ldots t_n}$ as a shorthand for atomic
classifiers $\lfroot{\sf a}{(t_1;
  (\ldots; (t_n;())\ldots))}$. % This shorthand evokes a canonical-forms
% presentation, as an atomic term, type, or proposition is a head
% $\lf{h}$ or ${\sf a}$ with the terms $\lf{t_1\ldots t_n}$ applied to
% it.

\begin{figure}[t]
\begin{align*}
\fbox{$\subst{\lf{t}}{\lf{\spi}}$}
\\
\subst{(\lf{\lambda a. t'})}{(\lf{t; \spi})}
 & = \subst{\rsubst{\lf{t}}{\lf{a}}{\lf{t'}}}{\lf{\spi}}
\\
\subst{\lfroot{\lf h}{\spi}}{\lfnil}
 & = \lfroot{\lf h}{\spi}
\end{align*}\begin{align*}
\fbox{$\rsubst{\lf{t}}{\lf{a}}{\lf{\spi}}$}&
&
\fbox{$\rsubst{\lf{t}}{\lf{a}}{\lf{t'}}$}&
&
\\
\rsubst{\lf t}{\lf a}{(\lf{t'; \spi})}
 & = \lf{\no{\rsubst{\lf t}{\lf a}{\lf{t'}}}; 
         \no{\rsubst{\lf t}{\lf a}{\lf{\spi}}}} &
\rsubst{\lf t}{\lf a}(\lf{\lambda y. t'})
 & = \lf{\lambda b.\, \no{\rsubst{\lf t}{\lf a}{\lf{t'}}}} 
      & (\lf a \neq \lf b) 
\\
\rsubst{\lf t}{\lf a}{\lfnil} 
 & = \lfnil &
\rsubst{\lf t}{\lf a}{(\lf{\lfroot{a}{\spi}})}
 & = \subst{\lf t}{\rsubst{\lf t}{\lf a}{\lf{\spi}}}
\\
& & 
\rsubst{\lf t}{\lf a}{(\lf{\lfroot{h}{\spi}})}
 & = \lfroot{\lf h}{\no{\rsubst{\lf t}{\lf a}{\lf{\spi}}}}
      & ({\it if}~ \lf{h} \neq \lf{a})
\end{align*}
\caption{Hereditary substitution on terms, spines, and classifiers}
\label{fig:lf-hsubst}
\end{figure}

\subsection{Simple types and hereditary substitution}

In addition to LF types like $\lfpi{a}{(\lfpi{z}{(\lfroot{\sf
      a1}{\spi_1})}{\,(\lfroot{\sf
      a2}{\spi_2})})}{\,\lfpi{y}{(\lfroot{\sf
      a3}{\spi_3})}{\,(\lfroot{\sf a4}{\spi_4})}}$, both Canonical LF
and Spine Form LF take {\it simple types} into consideration. The
simple type corresponding to the type above is $({\sf a1} \supset {\sf
  a2}) \supset {\sf a3} \supset {\sf a4}$, where ${\supset}$
associates to the right. The simple type associated with
$\tau$ can is given by the function ${\mid}\tau{\mid}^- = \tau_s$, where
${\mid}\lfroot{\sf a}{\spi}{\mid}^- = {\sf a}$ and
${\mid}\lfpi{a}{\tau}{\tau'}{\mid}^- = {\mid}\tau{\mid}^- \supset
{\mid}\tau'{\mid}^-$. 


\begin{figure}
\begin{align*}
\fbox{$\lf{\sigma}(\lf{\spi})$}&
&
\fbox{$\lf{\sigma}(\lf{t'})$}
\\
\lf{\sigma}(\lf{t'; \spi}) 
 & = \lf{\no{\lf{\sigma}(\lf{t'})}; \no{\lf{\sigma}(\lf{\spi})}} &
\lf{\sigma}(\lf{\lambda a.t'}) 
 & = \lf{\lambda a.\,\no{\lf{(\sigma, a/\!\!/a)}(\lf{t'})}}
 & (\lf{a} \# \lf{\sigma})
\\
\lf{\sigma}\lfnil 
 & = \lfnil &
\lf{\sigma}(\lf{\lfroot{a}{\spi}}) 
 & = \subst{\lf t}{\no{\lf{\sigma}(\lf{\spi})}}
      & \lf{t/a} \in \lf{\sigma} 
\\
& &
\lf{\sigma}(\lf{\lfroot{a}{\spi}}) 
 & = \lf{\lfroot{b}{\no{\lf{\sigma}(\lf{\spi})}}} 
      & \lf{b/\!\!/a} \in \lf{\sigma} 
\\
& &
\lf{\sigma}(\lf{\lfroot{\sf c}{\spi}}) 
 & = \lf{\lfroot{\sf c}{\no{\lf{\sigma}(\lf{\spi})}}} 
\end{align*}\begin{align*}
\fbox{$\lf{\sigma}{\nu}$} &
\\
\lf{\sigma}(\lfpi{b}{\nu}{\nu'})
 & = \lfpi{b}{\lf{\sigma}\nu}{\,\lf{(\sigma, b/\!\!/b)}\nu'}
     \qquad (\lf a \neq \lf b) 
\\
\lf{\sigma}({\sf type})
  & = {\sf type}
\\ 
\lf{\sigma}({\sf prop}) 
 & = {\sf prop} 
\\
\lf{\sigma}({\sf prop}\,{\sf ord}) 
 & = {\sf prop}\,{\sf ord} 
\\
\lf{\sigma}({\sf prop}\,{\sf lin}) 
 & = {\sf prop}\,{\sf lin} 
\\
\lf{\sigma}({\sf prop}\,{\sf pers}) 
 & = {\sf prop}\,{\sf pers} 
\\
\lf{\sigma}(\lfroot{\sf a}{\spi}) 
 & = \lfroot{\sf a}{\no{\rsubst{\lf{t}}{\lf{a}}{\lf{\spi}}}} 
\end{align*}
\caption{Simultaneous substitution on terms, spines, and classifiers}
\label{fig:simsubst}
\end{figure}

Variables and constants can be treated as having an intrinsic simple
type; these intrinsic simple types are sometimes written explicitly as
annotations $\lf{a}^{\tau_s}$ or $\lf{\sf c}^{\tau_s}$ (see, for
example, \cite{pfenning08church}), but we will leave them implicit.
An atomic term $\lf{h\,t_1\ldots t_n}$ must have an an simple atomic
type ${\sf a}$. This means that the head $\lf h$ must have simple type
$\tau_{s1} \supset \ldots \supset \tau_{sn} \supset {\sf a}$ and each
$\lf{t_i}$ much have simple type $\tau_{si}$. Similarly, a lambda term
$\lf{\lambda a. t}$ must have simple type $\tau_s \supset \tau_s'$
where $\lf a$ is a variable with simple type $\tau_s$ and $\lf t$ has
simple type $\tau_s'$.  

Simple types, which are treated with more care elsewhere care
elsewhere \cite{harper07mechanizing,reed09hybrid}, are critical
because they allow us to define hereditary substitution in
Figure~\ref{fig:lf-hsubst}. Spine Form LF terms correspond to the
proof terms for a focused presentation of minimal logic, and
hereditary substitution $\rsubst{\lf t}{\lf a}{\lf{t'}}$, which is
implicitly indexed by the simple type $\tau_s$ of $\lf t$ and $\lf a$,
is the computational content of cut admissibility on these proof
terms. 

\subsection{Judgments}

Hereditary substitution is necessary to define simultaneous
substitution into types and terms in Figure~\ref{fig:simsubst}.  We
will treat simultaneous substitutions in a mostly informal way, our
treatment follows the careful treatment of Nanevski et
al.~\cite{nanevski08contextual}. In particular, we write $\lf{[t/a]}$
for a simultaneous substitution that acts as the identity on all
variables except for $\lf{a}$; this notation is used in the definition
of LF typing in in Figure~\ref{fig:lf-form}, which is adapted to Spine
Form LF from Harper and Licata's Canonical LF presentation
\cite{harper07mechanizing}. The judgments
$\lf{a}\#\lf{\sigma}$, $\lf{a}\#\Psi$, $\lf{\sf c}\#\Sigma$, ${\sf
  a}\#\Sigma$, and ${\sf r}\#\Sigma$ assert that the relevant variable
or constant does not already appear in the context $\Psi$ (as a
binding $\lf{a}{:}\tau$), the signature $\Sigma$ (as a declaration
$\lf{\sf c} : \tau$, ${\sf a} : \nu$, or ${\sf r} : A^-$), or the
substitution $\lf{\sigma}$ (as a binding $\lf{t/a}$ or
\mbox{$\lf{b/\!\!/a}$}).

\begin{figure}
\fbox{$\vdash_\subord \Sigma\,{\sf sig}$}\vspace{-10pt}
\[
\infer
{\vdash_\subord \cdot\,{\sf sig} \mathstrut}
{}
\quad
\infer
{\vdash_\subord (\Sigma, \lf{\sf c} : \tau)\,{\sf sig} \mathstrut}
{\vdash_\subord \Sigma\,{\sf sig} 
 &
 \cdot \vdash_{\Sigma,\subord} \tau\,{\sf type}
 &
 \tau \prec_\subord \tau
 &
 \lf{\sf c} \# \Sigma \mathstrut}
\]
\[
\infer
{\vdash_\subord (\Sigma, {\sf a} : \kappa)\,{\sf sig} \mathstrut}
{\vdash_\subord \Sigma\,{\sf sig}
 &
 \vdash_{\Sigma, \subord} \kappa \,{\sf kind}
 &
 {\sf a} \sqsubset_\subord \kappa 
 &
 {\sf a} \# \Sigma\mathstrut}
\quad
\infer
{\vdash_\subord (\Sigma, {\sf r} : A^-)\,{\sf sig} \mathstrut}
{\vdash_\subord \Sigma\,{\sf sig}
 &
 \vdash_{\Sigma, \subord} A^- \,{\sf prop}^-
 &
 {\sf r} \# \Sigma \mathstrut}
\]

\medskip
\fbox{$\vdash_{\Sigma,\subord} \Psi\,{\sf ctx}$} -- presumes
  $\vdash_{\subord} \Sigma\,{\sf sig}$\vspace{-10pt}
\[
\infer
{\vdash_{\Sigma,\subord} \cdot\,{\sf ctx} \mathstrut}
{}
\quad
\infer
{\vdash_{\Sigma,\subord} (\Psi, \lf{a}{:}\tau)\,{\sf ctx} \mathstrut}
{\vdash_{\Sigma,\subord} \Psi\,{\sf ctx}
 &
 \Psi \vdash_{\Sigma, \subord} \tau\,{\sf type}
 &
 a \# \Psi}
\]

\medskip
\fbox{$\Psi \vdash_{\Sigma,\subord} \kappa\,{\sf kind}$} -- presumes
  $\vdash_{\Sigma, \subord} \Psi\,{\sf ctx}$
\[
\infer
{\Psi \vdash_{\Sigma,\subord} (\lfpi{a}{\tau}{\kappa})\,{\sf kind} \mathstrut}
{\Psi \vdash_{\Sigma,\subord} \tau\,{\sf type}
 &
 \Psi, \lf{a}{:}\tau \vdash_{\Sigma,\subord} \kappa\,{\sf kind}}
\quad
\infer{\Psi \vdash_{\Sigma,\subord} {\sf type}\,{\sf kind} \mathstrut}{}
\quad
\infer{\Psi \vdash_{\Sigma,\subord} {\sf prop}\,{\sf kind} \mathstrut}{}
\]
\[
\infer
{\Psi \vdash_{\Sigma,\subord} ({\sf prop}\,{\sf ord})\,{\sf kind}\mathstrut}{}
\quad
\infer
{\Psi \vdash_{\Sigma,\subord} ({\sf prop}\,{\sf lin})\,{\sf kind}\mathstrut}{}
\quad
\infer
{\Psi \vdash_{\Sigma,\subord} ({\sf prop}\,{\sf pers})\,{\sf kind}\mathstrut}{}
\]

\medskip
\fbox{$\Psi \vdash_{\Sigma,\subord} \tau\,{\sf type}$} -- presumes
  $\vdash_{\Sigma, \subord} \Psi\,{\sf ctx}$
\[
\infer
{\Psi \vdash_{\Sigma,\subord}(\lfpi{a}{\tau}{\tau'})\,{\sf type} \mathstrut}
{\Psi \vdash_{\Sigma,\subord} \tau\,{\sf type}
 &
 \Psi, \lf{a}{:}\tau \vdash_{\Sigma,\subord} \tau'\,{\sf type}
 &
 \tau \preceq_\subord \tau' \mathstrut}
\quad
\infer
{\Psi \vdash_{\Sigma,\subord}(\lfroot{\sf a}{\spi})\,{\sf type} \mathstrut}
{a{:}\kappa \in \Sigma
 &
 \Psi, [\kappa] \vdash_{\Sigma,\subord} \lf{\spi} : {\sf type}
 \mathstrut}
\]

\medskip
\fbox{$\Psi \vdash_{\Sigma,\subord} t : \tau$} -- presumes 
  $\Psi \vdash_{\Sigma,\subord} \tau\,{\sf type}$
\[
\infer
{\Psi \vdash_{\Sigma,\subord} \lf{\lambda a.t} : \lfpi{x}{\tau}{\tau'}\mathstrut}
{\Psi, \lf{a}{:}\tau \vdash_{\Sigma,\subord} \lf{t} : \tau'\mathstrut}
\quad
\infer
{\Psi \vdash_{\Sigma,\subord} \lf{\lfroot{\sf c}{\spi}} : \lfroot{\sf a}{\spi'}
 \mathstrut}
{\lf{\sf c} : \tau \in {\Sigma}
 &
 \Psi; [\tau] \vdash_{\Sigma,\subord} \spi : \tau'
 &
 \tau' = \lfroot{\sf a}{\spi'}\mathstrut}
\]
\[
\infer
{\Psi \vdash_{\Sigma,\subord} \lf{\lfroot{a}{\spi}} : \lfroot{\sf a}{\spi'}
 \mathstrut}
{\lf{a} {:} \tau \in {\Psi}
 &
 \Psi; [\tau] \vdash_{\Sigma,\subord} \spi : \tau'
 &
 \tau' = \lfroot{\sf a}{\spi'}\mathstrut}
\]

\medskip
\fbox{$\Psi, [\nu] \vdash_{\Sigma,\subord} \lf{\spi} : \nu_0$} --
presumes that either $\Psi \vdash_{\Sigma,\subord} \nu\, {\sf type}$
or that $\Psi \vdash_{\Sigma,\subord} \nu\, {\sf kind}$
\[
\infer
{\Psi, [\nu] \vdash_{\Sigma,\subord} \lfnil : \nu \mathstrut}
{}
\quad
\infer
{\Psi, [\lfpi{a}{\tau}{\nu}] \vdash_{\Sigma,\subord} \lf{t; \spi} : \nu_0
 \mathstrut}
{\Psi \vdash_{\Sigma,\subord} \lf{t} : \tau
 &
 \lf{[t/a]}\nu = \nu'
 &
 \Psi, [\nu'] \vdash_{\Sigma,\subord} \lf{\spi} : \nu_0 \mathstrut}
\]

\medskip
\fbox{$\Psi \vdash \lf{\sigma} : \Psi'$} -- presumes
 $\vdash_{\Sigma,\subord} \Psi\,{\sf ctx}$
 and
 $\vdash_{\Sigma,\subord} \Psi'\,{\sf ctx}$
\[
\infer
{\Psi \vdash_{\Sigma,\subord} \cdot : \cdot \mathstrut}
{}
\quad
\infer
{\Psi \vdash_{\Sigma,\subord} (\lf{\sigma, t/a}) : \Psi', \lf{a}{:}\tau
  \mathstrut}
{\Psi \vdash_{\Sigma,\subord} \lf{\sigma} : \Psi' 
 &
 \Psi \vdash_{\Sigma,\subord} \lf{t} : \lf{\sigma}\tau 
  \mathstrut}
\quad
\infer
{\Psi \vdash_{\Sigma,\subord} (\lf{\sigma, b/\!\!/a}) : \Psi', \lf{a}{:}\tau
  \mathstrut}
{\Psi \vdash_{\Sigma,\subord} \lf{\sigma} : \Psi'
 &
 \lf{y}{:}\lf{\sigma}\tau \in \Psi
  \mathstrut}
\]

\caption{LF formation judgments.}
\label{fig:lf-form}
\end{figure}

All the judgments in Figure~\ref{fig:lf-form} are indexed by a
transitive {\it subordination relation} $\subord$, similar to the one
introduced by Virga in \cite{virga99higherorder}. We treat $\subord$
as a binary relation on type family constants.  Let ${\sf head}(\tau)
= {\sf a}$ if $\tau =
\lfpi{a_1}{\tau_{1}}{\,.\,.\lfpi{a_{m}}{\tau_{m}}{\,\lfroot{\sf
      a}{\spi}}}$. The signature formation operations depend on three
judgments. The index subordination judgment, 
$\kappa \sqsubset_\subord {\sf a}$, relates type
family constants to types. It is always the case that $\kappa =
\lfpi{a_1}{\tau_1}{\ldots\lfpi{a_n}{\tau_n}{\sf type}}$.  The judgment
$\kappa \sqsubset_\subord {\sf a}$ holds if $({\sf head}(\tau_i), {\sf
  a}) \in \subord$ for $1 \leq i \leq n$. The type subordination
judgment $\tau \prec \tau'$ holds if $({\sf head}(\tau),
{\sf head}(\tau')) \in \subord$, and the judgment $\tau \preceq \tau'$ 
is the transitive closure of this relation.

There are a number of well-formedness theorems that we need to
consider, such as the fact that substitutions compose in a
well-behaved way and that hereditary substitution is always
well-typed.  However, as these theorems are adequately covered
elsewhere, we will proceed with using LF as a term language and will 
treat term-level operations like substitution somewhat informally.

\subsection{Adequacy}
\label{sec:lf-adequacy}

{\it Adequacy} was the name given by Harper, Honsell, and Plotkin to the
methodology of connecting the inductive definitions we write in paper
to the canonical forms of a particular type family in LF. Consider,
as a standard example, the untyped lambda calculus, which is generally
specified by a BNF grammar such as the following:
\[
\obj{e} ::= \obj{x} \mid \obj{\lambda x.e} \mid \obj{e_1\,e_2}
\]
We can adequately encode this language of terms into LF (with a
subordination relation $\subord$ such that $({\sf exp}, {\sf
  exp}) \in \subord$) by giving the following signature:
\begin{align*}
\Sigma & = \cdot, 
\\
 & ~\quad {\sf exp} : {\sf type}, 
\\
 & ~\quad \lf{\sf app} : 
     \lfpi{a}{{\sf exp}}{\,\lfpi{b}{\sf exp}{\,\sf exp}},
\\
 & ~\quad \lf{\sf lam} : 
     \lfpi{a}{(\lfpi{b}{\sf exp}{\,\sf exp})}{\,\sf exp}
\end{align*}
Note that the variables $\lf{a}$ and $\lf{b}$ are bound by
$\Pi$-binders in the declaration of ${\sf app}$ and ${\sf lam}$ but
never used. The usual convention is to abbreviate
$\lfpi{a}{\tau}{\tau'}$ as $\tau \rightarrow \tau'$ when $\lf{a}$ is
not free in $\tau'$, which would give $\lf{\sf app}$ type ${\sf exp}
\rightarrow {\sf exp} \rightarrow {\sf exp}$ and $\lf{\sf lam}$ type
$({\sf exp} \rightarrow {\sf exp}) \rightarrow {\sf exp}$.

\bigskip
\begin{theorem}[Adequacy for terms]\label{thm:expadequacy}
  Up to standard $\alpha$-equivalence, there is a bijection between
  expressions $\obj{e}$ (with free variables in the set
  $\{\obj{x_1},\ldots,\obj{x_n}\}$) and Spine Form LF terms $\lf{t}$ such
  that $\lf{x_1}{:}\mathsf{exp}, \ldots, \lf{x_n}{:}\mathsf{exp} \vdash
  \lf{t} : \mathsf{exp}$. 
\end{theorem}

\begin{proof}
By induction on the structure of the inductive definition of $\obj{e}$
in the forward direction and by induction on the structure of 
terms $\lf{t}$ with type ${\sf exp}$ in the reverse direction.
\end{proof}

We express the constructive content of this theorem as a bijective
function $\interp{e} = \lf{t}$ from object language terms $\obj{e}$ to
representations LF terms $\lf{t}$ of type ${\sf exp}$. If we had also
defined substitution $\obj{[e/x]e'}$ on terms, it would be necessary
to show that the bijection is compositional: that is, that
$[\interp{e}/\lf{x}]\interp{e'} = \interp{[e/x]e'}$.  Note that
adequacy critically depends on the context having the form
$\lf{x_1}{:}{\sf exp},\ldots,\lf{x_n}{:}{\sf exp}$. If we had a
context with a variable $\lf{y}{:}({\sf exp} \rightarrow {\sf exp})$,
then we could form a term $\lf{y\,({\sf lam}\,\lambda x. x)}$ with
type ${\sf exp}$ that does {\it not} adequately encode any term
$\obj{e}$ in the untyped lambda calculus.

One of the reasons subordination is important in practice is that it
allows us to consider the adequate encoding of expressions in contexts
$\Psi$ that have other variables $\lf{x}{:}\tau$ as long as $({\sf
  head}(\tau),{\sf exp}) \notin \subord$. If $\Psi,\lf{x}{:}\tau
\vdash_{\Sigma,\subord} \lf{t} : {\sf exp}$ and $\tau
\not\preceq_\subord {\sf exp}$, then $\lf{x}$ cannot be free in
$\lf{t}$, so $\Psi \vdash_{\Sigma,\subord} \lf{t} : {\sf exp}$ holds as
well. By iterating this procedure, it may be possible to strengthen a
context $\Psi$ into one of the form $\lf{x_1}{:}{\sf
  exp},\ldots,\lf{x_n}{:}{\sf exp}$, in which case we can conclude
that $\lf t = \interp{e}$ for some untyped lambda calculus term $\obj
e$.



\section{The logical framework \sls}
\label{sec:slsframework}

In this section, we will describe a restricted set of polarized
\ollll~propositions and focused \ollll~proof terms that make up the
logical framework \sls. For the remainder of the thesis, we will work
exclusively with the following positive and negative
\sls~propositions, which are a syntactic refinement of the positive
and negative propositions of polarized \ollll:
\begin{align*}
A^+, B^+, C^+ & ::= p^+ \mid p^+_\meph \mid p^+_\mpers \mid {\downarrow}A^-
  \mid {\gnab}A^- \mid {!}A^- \mid \one \mid A^+ \fuse B^+
  \mid \exists \lf{a}{:}\tau.A^+ \mid \lf{t} \doteq_\tau \lf{s}
\\
A^-, B^-, C^- & ::= p^- \mid {\ocircle}A^+ \mid A^+ \lefti B^- 
  \mid A^+ \righti B^- \mid A^- \with B^-
  \mid \forall \lf{a}{:}\tau.A^-
\end{align*}
%Aside from the type annotation $\tau$ on unification $\lf{t}
%\doteq_\tau \lf{s}$ and on the quantifiers $\forall \lf{x}{:}\tau. A^-$
%and $\exists \lf{x}{:}\tau. A^+$, which we will in general leave implicit,
%this is exactly a refinement of the  
%Notably missing from this refinement are
%upshifts ${\uparrow}A^+$ and right-permeable atomic propositions
%$p^-_\mlax$.
% We will also continue to
% avoid using variable names $x$ with inverting positive propostiions
% and focused negative propositions. In the case of focused
% propositions, there is a straightforward justification: the form of
% context ensures there is at most one of them. In the case of positive
% propositions, we are justified by the convention discussed in the last
% chapter that we only ever frame off the leftmost positive proposition
% in the context.


Positive ordered atomic propositions
$p^+$ are atomic classifiers ${\sf a}\,\lf{t_1}\ldots\lf{t_n}$ with
kind ${\sf prop}\,{\sf ord}$, positive linear atomic propositions
$p^+_\meph$ and $p^+_\mpers$ are (respectively) atomic classifiers
with kind ${\sf prop}\,{\sf lin}$ and ${\sf prop}\,{\sf pers}$, and
negative ordered atomic propositions $p^-$ are atomic classifiers
with kind ${\sf prop}$.  From this point on,
we will unambiguously refer to atomic propositions $p^-$ as negative
atomic propositions, omitting ``ordered.'' Similarly, we will refer to
atomic propositions $p^+$, $p^+_\meph$, and $p^+_\mpers$ collectively
as positive atomic propositions but individually as ordered, linear,
and persistent propositions, respectively, omitting ``positive.''
(``Mobile'' and ``ephemeral'' will continue to be used as synonyms for
``linear.'')

\input{figs/fig-sls-propform}

The formation judgments for \sls~types are given in
Figure~\ref{fig:sls-propform}. Much of the complexity of the
presentation -- in particular the introduction of a subset $\mathcal
S$ of the LF context that is treated with a multiset discipline, is
designed to make sure that whenever we decompose a positive
proposition $\lf{s} \doteq \lf{t}$ we have that $\lf{s} = \lf{a}$ for
some variable $\lf{a}$ in the context. When this is the case,
$\lf{[t/a]}$, is always a most general unifier of $\lf{s} = \lf{a}$ and
$\lf{t}$, which in turn means that the higher-order unification rule
from \ollll
\[
\infer[{\doteq}_L]
{\foc{\Psi}{\frameoff{\Theta}{\lf{t} \doteq \lf{s}}}{U}}
{\forall(\Psi' \vdash \lf{\sigma} : \Psi).
 &
 \lf{\sigma t} = \lf{\sigma s}
 &
 \longrightarrow
 &
 \foc{\Psi'}{\tackon{\lf{\sigma}\Theta}{\cdot}}{\lf{\sigma} U}
 }
\]
is equivalent to a much simpler rule:
\[
\infer[{\doteq}_L]
{\foc{\Psi, \lf{a}{:}\tau, \Psi'}{\frameoff{\Theta}{\lf{a} \doteq \lf{t}}}{U}}
{\foc{\Psi, \lf{[t/a]}\Psi'}{\tackon{\lf{[t/a]}\Theta}{\cdot}}{\lf{[t/a]} U}}
\]

There are two distinct conditions under which we can be sure that
unification problems always have a most general solution.  \smallskip
\begin{enumerate}
\item Unification at an atomic type $p$ that is {\it not subordinate
    to itself} ($p \not\prec_\subord p$) is always allowed.  This is
  reflected in the first formation rule for $\lf{t} \doteq
  \lf{s}$ in Figure~\ref{fig:sls-propform}. 

  Types that are not self-subordinate can only be inhabited by
  variables: that is, if $p \not\prec_\subord p$ and $\Psi
  \vdash_{\Sigma,\subord} \lf{t} : p$, then $\lf{t} = \lf{{a}}$ where
  $\lf{a}:p \in \Psi$. For any such unification problem $\lf{{a}}
  \doteq \lf{{b}}$, both $\lf{[{a}/b]}$ and $\lf{[{b}/a]}$ are most
  general unifiers.

\item The second condition under which unification is allowed is when
  it is effectively used as a {\it notational definition}
  \cite{pfenning99algorithms}. Because of the way the subset $\mathcal
  S$ of the LF context is handled as an a multiset, each variable
  bound by an existential quantifier $\exists \lf{a}{:}p.\,A^+$ or a
  universal quantifier $\forall \lf{a}{:}p.\,A^-$ can be associated
  with at most one proposition $\lf{a} \doteq \lf{t}$, where $\lf{t}$
  is an arbitrary term that appears in the same focusing phase. This
  condition is handled by the second formation rule for $\lf{t} \doteq
  \lf{s}$ in Figure~\ref{fig:sls-propform}.

  The proposition $\forall \lf{a}.\, \forall \lf{b}.\,
  {\downarrow}({\sf p}\,\lf{a}) \lefti \lf{a} \doteq \lf{b} \lefti
  {\sf p}\,\lf{b}$ satisfies this condition but $\forall
  \lf{a}.\,\forall \lf{b}.\,{\ocircle}(\lf{a} \doteq \lf{b})$ does not
  ($\ocircle$ breaks focus), and the proposition ${\ocircle}(\exists
  \lf{a}. \lf{a} \doteq \lf{t})$ satisfies this condition but
  ${\ocircle}(\exists \lf{a}.  {\uparrow}(\lf{a} \doteq \lf{t} \lefti
  {\sf p}\,\lf{a}))$ does not (${\uparrow}$ breaks focus).  The
  proposition ${\ocircle}(\exists \lf{a}.\, \lf{a} \doteq \lf{t} \fuse
  \lf{a} \doteq \lf{s})$ does not satisfy this condition because the
  introduced variable $\lf{a}$ is associated with the left-hand side
  of two different equalities, $\lf{a} \doteq \lf{t}$ and
  $\lf{a} \doteq \lf{s}$, that together encode an arbitrary unification
  problem $\lf{t} \doteq \lf{s}$. 

  This restriction ensures that, if the unification appears on the
  left, the left-hand side of the unification will always be a
  variable $\lf{a}$, meaning that $\lf{[t/a]}$ is always a most
  general unifier.  This usage of unification is essentially just a
  notational definition \cite{pfenning99algorithms}.
\end{enumerate}
\smallskip

\input{figs/fig-sls-ctxform}

The proposition formation judgments are used to specify the formation
of stable, inverting, and in-focus \sls~contexts in
Figure~\ref{fig:sls-ctxform}. This restrictions we make to contexts
justify our continued practice of omitting the $\mtrue$ annotation
when talking about inverting positive propositions $A^+$ or focused
negative propositions $[A^-]$ in the context.

We could stop here and use the refinement of \ollll~proof terms that
corresponds to our refinement of propositions as the language of
\sls~proof terms. This is not ideal for two reasons. First,
the proof terms of focused \ollll~make it inconvenient (though not
impossible) to talk about concurrent equality
(Section~\ref{sec:framework-concurrenteq}). Second, one of our primary
uses of \sls~in this thesis will be to talk about {\it traces}, which
correspond roughly to partial proofs
\[
\deduce
{\foc{\Psi}{\Delta}{\islax{A^+}}\mathstrut}
{\deduce{\vdots\mathstrut}
  {\vspace{-4pt}\foc{\Psi'}{\Delta'}{\islax{A^+}}}}
\]
in \ollll, where both the top and bottom sequents are stable and where
$A^+$ is some unspecified, parametric positive proposition. Using
\ollll-derived proof terms makes it difficult to talk about about and
manipulate proofs of this form.

In the remainder of this section, we will present a proof term
assignment for \sls~that facilitates discussing concurrent equality
and partial proofs. \sls~proof terms are in bijective correspondence
with \ollll~proof terms when we consider complete (deductive) proofs,
but the introduction of patterns and traces changes the structure
of derivations and proof terms.




\subsection{Process states}

A {\it process state} is a disembodied left-hand side of a sequent that
we use to describe the intermediate states of concurrent systems. The 
goal of concurrent traces is to capture the structure of partial proofs
\[
\deduce
{\foc{\Psi}{\Delta}{\islax{A^+}}\mathstrut}
{\deduce{\vdots\mathstrut}
  {\vspace{-4pt}\foc{\Psi'}{\Delta'}{\islax{A^+}}}}
\]
where $A^+$ is a parametric conclusion as the relation between two 
process states. As a first cut, we can represent the initial state as 
$(\Psi; \Delta)$ and the final state as $(\Psi'; \Delta')$, and we can
omit $\Psi$ and just write $\Delta$ when that is sufficiently clear.

The first cut is not quite enough due to the presence of unification
in \sls~-- in particular, the first use-case of unification where we
unify members of a type only inhabited by variables. Consider the
following partial proof:
\[
\deduce
{\foc{\lf{a}{:}p, \lf{b}{:}p}
  {~~x{:}\iseph{{\ocircle}(\lf{a} \doteq \lf{b})}, ~~
   z{:}\iseph{\susp{{\sf foo}\,\lf{a}\,\lf{a}}}}
  {\islax{({\sf foo}\,\lf{a}\,\lf{b})}}\mathstrut}
{\deduce{\vdots\mathstrut}
  {\vspace{-4pt}\foc{\lf{b}{:}p}
   {~~ z{:}\iseph{\susp{{\sf foo}\,\lf{b}\,\lf{b}}}}
   {\islax{({\sf foo}\,\lf{b}\,\lf{b})}}}}
\]
This partial proof can be constructed in one focusing stage by left
focusing on $x$. It is insufficient to capture the first process
state as 
$\left({\lf{a}{:}p, \lf{b}{:}p}; ~~
 {x{:}{\ocircle}(\lf{a} \doteq \lf{b}), ~~
  z{:}\iseph{\susp{{\sf foo}\,\lf{a}\,\lf{a}}}}\right)$
and the second process state as
$\left({\lf{b}{:}p};~~
 {z{:}\iseph{\susp{{\sf foo}\,\lf{b}\,\lf{b}}}}\right)$, as this would fail to 
capture that the succedent $\islax{({\sf foo}\,\lf{b}\,\lf{b})}$
associated with the second process state is a substitution instance of
the succedent
$\islax{({\sf foo}\,\lf{a}\,\lf{b})}$ associated with the first. In general,
if the overall proof proves some proposition $\islax{A^+}$, then 
the missing subproof has a succedent $\islax{\lf{[b/a]}A^+}$.

A process state is therefore written as $(\Psi; \Delta)_{\lf{\sigma}}$ 
and is well-formed under
signature $\Sigma$ and subordination relation $\subord$ if 
$\Psi \vdash_{\Sigma,\subord} \Delta\,{\sf stable}$ (which presumes that
$\vdash_{\Sigma,\subord} \Psi\,{\sf sctx}$) and if 
$\Psi \vdash \lf{\sigma} : \Psi_0$, where $\Psi_0$ is some unspecified third 
context that represents a starting point. 
\[
\infer
{\vdash_{\Sigma,\subord} (\Psi; \Delta)_{\lf{\sigma}}\,{\sf state}
 \mathstrut}
{\vdash_{\Sigma,\subord} \Psi : {\sf ctx}
 &
 \Psi \vdash_{\Sigma,\subord} \Delta\,{\sf inv}
 &
 \vdash_{\Sigma,\subord} \Psi_0 : {\sf ctx}
 &
 \Psi \vdash \lf{\sigma} : \Psi_0
 \mathstrut}
\]
Taking $\Psi_0 = \lf{a}{:}p, \lf{b}{:}p$, the partial proof above can
thus be represented as a step (Section~\ref{sec:framework-concurrent})
between these two process states:
\[
\left({\lf{a}{:}p, \lf{b}{:}p}; ~~
 {x{:}{\ocircle}(\lf{a} \doteq \lf{b}),  ~~
  z{:}\iseph{\susp{{\sf foo}\,\lf{a}\,\lf{a}}}}\right)_{\lf{(a/a,\,b/b)}}
\leadsto_{\Sigma,\subord}
\left({\lf{b}{:}p}; ~~
 {z{:}\iseph{\susp{{\sf foo}\,\lf{b}\,\lf{b}}}}\right)_{\lf{(b/a,\,b/b)}}
\]

Substitutions are necessary to describe process states as an
integral part of the \sls~framework. However, in most cases we
will take $\Psi_0 = \cdot$ and $\lf{\sigma} = \cdot$, which
corresponds to the case where the parametric conclusion $A^+$ is a
closed proposition. When the substitution is not mentioned it can be
assumed to be empty. Additionally, when the LF context $\Psi$ is empty
or simply irrelevant, we will omit it as well. One further
simplification is that we will occasionally omit the judgment $\mlvl$
associated with a suspended positive atomic proposition
$\islvl{\susp{p^+_\mlvl}}$, but only when it is unambiguous from the
current signature that $p^+_\mlvl$ is an ordered, linear, or
persistent positive atomic proposition. In the examples above, we
tacitly assumed that ${\sf foo}$ was given kind $p \rightarrow p
\rightarrow {\sf prop}\,{\sf lin}$ in the signature $\Sigma$ when we
tagged the suspended atomic propositions with the judgment $\meph$, but if
it was clear that ${\sf foo}$ was linear this judgment
could have been omitted. 

%In almost all cases, the substitution $\lf\sigma$ associated with a process
%state is the identity and will be omitted, and the LF context $\Psi$
%will frequently be omitted as well. 

\begin{figure}
\fbox{$P :: (\Psi; \Delta)_{\lf{\sigma}}
         \Longrightarrow_{\Sigma,\subord}
           (\Psi'; \Delta')_{\lf{\sigma}}$}
 -- presumes
 $\vdash_{\Sigma,\subord} (\Psi; \Delta)_{\lf{\sigma}}\,{\sf state}$
\[
\infer[()]
{() :: (\Psi; \Delta)_{\lf{\sigma}}
           \Longrightarrow_{\Sigma,\subord} 
       (\Psi; \Delta)_{\lf{\sigma}}}
{\Psi \vdash_{\Sigma,\subord} \Delta\,{\sf stable}}
\]
\[
\infer[\eta^+]
{z,P :: (\Psi; \frameoff{\Theta}{p^+_\mlvl})_{\lf{\sigma}}
           \Longrightarrow_{\Sigma,\subord} 
        (\Psi'; \Delta')_{\lf{\sigma'}}}
{P :: (\Psi; \tackon{\Theta}{z{:}\islvl{\susp{p^+_\mlvl}}})_{\lf{\sigma}}
           \Longrightarrow_{\Sigma,\subord}
        (\Psi'; \Delta')_{\lf{\sigma'}}}
\quad
\infer[{\downarrow}_L]
{x,P :: (\Psi; \frameoff{\Theta}{{\downarrow}A^-})_{\lf{\sigma}}
           \Longrightarrow_{\Sigma,\subord}
        (\Psi'; \Delta')_{\lf{\sigma'}}}
{P :: (\Psi; \tackon{\Theta}{x{:}\istrue{A^-}})_{\lf{\sigma}}
           \Longrightarrow_{\Sigma,\subord}
        (\Psi'; \Delta')_{\lf{\sigma'}}}
\]
\[
\infer[{\gnab}_L]
{x,P :: (\Psi; \frameoff{\Theta}{{\gnab}A^-})_{\lf{\sigma}}
           \Longrightarrow_{\Sigma,\subord}
        (\Psi'; \Delta')_{\lf{\sigma'}}}
{P :: (\Psi; \tackon{\Theta}{x{:}\iseph{A^-}})_{\lf{\sigma}}
           \Longrightarrow_{\Sigma,\subord}
        (\Psi'; \Delta')_{\lf{\sigma'}}}
\quad
\infer[{\bang}_L]
{x,P :: (\Psi; \frameoff{\Theta}{{!}A^-})_{\lf{\sigma}}
           \Longrightarrow_{\Sigma,\subord}
        (\Psi'; \Delta')_{\lf{\sigma'}}}
{P :: (\Psi; \tackon{\Theta}{x{:}\ispers{A^-}})_{\lf{\sigma}}
           \Longrightarrow_{\Sigma,\subord}
        (\Psi'; \Delta')_{\lf{\sigma'}}}
\]
\[
\infer[{\one}_L]
{P :: (\Psi; \frameoff{\Theta}{\one})_{\lf{\sigma}}
           \Longrightarrow_{\Sigma,\subord}
        (\Psi'; \Delta')_{\lf{\sigma'}}}
{P :: (\Psi; \tackon{\Theta}{\cdot})_{\lf{\sigma}}
           \Longrightarrow_{\Sigma,\subord}
        (\Psi'; \Delta')_{\lf{\sigma'}}}
\quad 
\infer[{\fuse}_L]
{P :: (\Psi; \frameoff{\Theta}{A^+ \fuse B^+})_{\lf{\sigma}}
           \Longrightarrow_{\Sigma,\subord}
        (\Psi'; \Delta')_{\lf{\sigma'}}}
{P :: (\Psi; \tackon{\Theta}{\mkconj{A^+}{B^+}})_{\lf{\sigma}}
           \Longrightarrow_{\Sigma,\subord}
        (\Psi'; \Delta')_{\lf{\sigma'}}}
\]
\[
\infer[{\exists}_L]
{\lf{a},P :: (\Psi; \frameoff{\Theta}{\exists \lf{a}{:}\tau. A^+})_{\lf{\sigma}}
           \Longrightarrow_{\Sigma,\subord}
        (\Psi'; \Delta')_{\lf{\sigma'}}}
{P :: (\Psi, \lf{a}{:}\tau; \tackon{\Theta}{A^+})_{\lf{\sigma}}
           \Longrightarrow_{\Sigma,\subord}
        (\Psi'; \Delta')_{\lf{\sigma'}}}
\]
\[
\infer[{\doteq}_L]
{\lf{t/a},P :: 
  (\Psi, \lf{a}{:}\tau, \Psi'; \frameoff{\Theta}{\lf{a} \doteq \lf{t}}
   )_{\lf{\sigma}}
           \Longrightarrow_{\Sigma,\subord}
        (\Psi'; \Delta')_{\lf{\sigma'}}}
{P :: (\Psi, \lf{[t/a]}\Psi'; {\tackon{\lf{[t/a]}\Theta}{\cdot}}
       )_{\lf{[t/a]\sigma}}
           \Longrightarrow_{\Sigma,\subord}
        (\Psi'; \Delta')_{\lf{\sigma'}}}
\]
\caption{\sls~patterns}
\label{fig:sls-patterns}
\end{figure}


\subsection{Patterns}
\label{sec:framework-patterns}

 % With the restrictions we have made to unificaiton, we can 
% treat $\doteq_L$ as always having one premise, so all remaining
% left rules for positive atomic propositions have exactly one 
% premise. 

Our restrictions on \sls~propositions give derivations the property
that every positive proposition has exactly one premise. 
The connectives $\zero$ and $A^+ \oplus B^+$ were excluded
from \sls~with this goal in mind, and it is also our reason for
restricting unification to always have a most general unifier. 
A {\it pattern} is a syntactic entity that captures the linear structure of
left inversion on positive propositions. Instead of having a somewhat
inscrutable proof term of the form
${\tlaml{\texistsl{\lf{a}}
   {\tfusel{\tfusel{\tetap{x}{\tgnabl{y}{\tdownl{z}{N}}}}}}}}$,
for the proposition 
%
$(\exists \lf{a}.\,{\sf p}\,\lf{a} 
             \fuse {\gnab}A^-
             \fuse {\downarrow}B^-) \lefti C^-$
%
the \sls~proof term associated with this proposition, which uses patterns, is
$(\lambda \lf{a},x,y,z.\, N)$. The pattern $P = \lf{a}, x,y,z$ captures
the structure of left inversion on the positive proposition 
$\exists \lf{a}.\,{\sf p}\,\lf{a} 
             \fuse {\gnab}A^-
             \fuse {\downarrow}B^-$.

The grammar of patterns is simple.
% 
Inversion on positive propositions
can only have the effect of introducing new bindings (either LF
variables $\lf{a}$ or \sls~variables $x$) or handling a unification
$\lf{a} \doteq \lf{t}$, which by our discussion above can always be
resolved by the most general unifier $\lf{[t/a]}$, so the pattern associated
with a proposition $\lf{a} \doteq \lf{t}$ is $\lf{t/a}$. 
\[
P ::= () \mid x, P \mid \lf{a}, P \mid \lf{t/a}, P
\] 
For sequences with one or more elements, we omit the trailing
comma and $()$, writing $x, \ldots, z$ 
instead of $x, \ldots, z, ()$. 

\sls~patterns are associated with a decomposition relation on
\sls~process states: $P :: (\Psi; \Delta)_{\lf{\sigma}}
\Longrightarrow_{\Sigma,\subord} (\Psi'; \Delta')_{\lf{\sigma'}}$.
The typing rules for \sls~patterns are given in
Figure~\ref{fig:sls-patterns}. We preserve the side conditions from
the previous chapter: when we frame off a inverting positive
proposition in the process state, it is required to be the left-most
one. As with focused \ollll, this justifies our omission of the
variables associated with positive propositions: the positive
proposition we frame off are uniquely identified not by their
associated but by their position in the context. 

Note that, with patterns, there no longer appears to be a one-to-one
correspondence between proof terms and rules: ${\downarrow}_L$,
${\gnab}_L$, and ${!}_L$ appear to have the same proof term, and
${\one}_L$ and ${\fuse}_L$ appear to have no proof term at all. To
view patterns as being intrinsically typed -- that is, to view them as
actual representatives of derivations -- we must think of patterns as
carrying extra annotations that allow them to continue matching the
structure of proof rules.

\subsection{Values, Terms, and Spines}
\label{sec:framework-deductive}

Notably missing from the \sls~types are the upshifts ${\uparrow}A^+$
and right-permeable negative atomic propositions $p^-_\mlax$. The
removal of these to propositions effectively means that the succedent
of a stable \sls~sequent can only be $\istrue{\susp{p^-}}$ or
$\islax{A^+}$. The \sls~framework only considers {\it complete} proofs
of judgments $\istrue{\susp{p^-}}$, whereas traces, associated with
proofs of $\islax{A^+}$ and introduced below in
Section~\ref{sec:framework-concurrent}, are a proof term assignment
for partial proofs. Taken together, the values, terms, and spines that
stand for complete proofs will be referred to as the {\it deductive
  fragment} of \sls.
\begin{align*}
&\mbox{\sls~values (Figure~\ref{fig:sls-values})}& 
V & ::= z
   \mid N
   \mid \tgnabr{N}
   \mid \tbangr{N}
   \mid \toner
   \mid \tfuser{V_1}{V_2}
   \mid \texistsr{\lf{t}}{V}
   \mid \tunifr
\\
&\mbox{\sls~atomic terms (Figure~\ref{fig:sls-atomic-terms})}&
R & ::= \tfocusl{x}{\Sp} 
   \mid \tfocusl{\sf r}{\Sp} 
\\
&\mbox{\sls~terms (Figure~\ref{fig:sls-terms})}&
N & ::= R
   \mid \lambda P.N 
   \mid N_1 \with N_2
   \mid \tforallr{\lf{a}}{N}
   \mid 
\\
&\mbox{\sls~spines (Figure~\ref{fig:sls-spines})}&
\Sp & ::= \tnil 
   \mid V; \Sp
   \mid \pi_1; \Sp 
   \mid \pi_2; \Sp
   \mid \lf{t}; \Sp
\end{align*}

In contrast to \ollll, we distinguish the refinement $R$ of atomic
terms $R$ that correspond to stable sequents from terms $N$. As with
patterns, we appear to conflate the proof terms associated with
different proof rules -- we have a single $\lambda P.N$ constructor
and a single $V;\Sp$ spine rather than a term $\tlamr{N}$ and spine
$\tappr{V}{\Sp}$ associated with propositions $A^+ \righti B^-$ and
another term $\tlaml{N}$ and spine $\tappl{V}{\Sp}$ associated with
propositions $A^+ \lefti B^-$.  As with patterns, it is possible to
think of these terms as just having extra annotations ($\lambda^>$ or
$\lambda^<$) that we have omitted.  Without these annotations, proof
terms carry less information than derivations, and the rules for
values, terms, and spines in
Figures~\ref{fig:sls-values}--\ref{fig:sls-spines} must be seen as
typing rules. With these extra implicit annotations, values, terms,
and spines can continue to be seen as representatives of derivations.

\input{figs/fig-sls-deductive}

There are two typing rules in
Figures~\ref{fig:sls-values}--\ref{fig:sls-spines} that do not have
exact analogues as rules in \ollll, the rule ${\it rule}$
(Figure~\ref{fig:sls-atomic-terms}) corresponding to an atomic term
$\tfocusl{\sf r}{\Sp}$ and the rule ${\ocircle}_R$
(Figure~\ref{fig:sls-terms}) corresponding to the term
$\tlet{T}{V}$. We will discuss the latter outlier, let-expressions, in
the next section. The first outlier, the typing rule labeled ${\it
  rule}$, accounts for the fact that there is an additional source of
persistent facts in \sls, the signature $\Sigma$, that is not present
in \ollll.  To preserve the bijective correspondence between
\ollll~and \sls~proof terms, we need to either place every rule ${\sf
  r} : A^-$ in the \sls~signature $\Sigma$ into the corresponding
\ollll~context as a persistent proposition.

As with LF terms, we will use a shorthand for atomic terms
$\tfocusl{x}{\Sp}$ and $\tfocusl{\sf r}{\Sp}$, writing $({\sf
  foo}\,\lf{t}\,\lf{s}\,V\,V')$ instead of $\tfocusl{\sf
  foo}{(\lf{t};\lf{s};V;V';\tnil)}$ when we are not concerned with the
fact that atomic terms consist of a variable and a spine.

\subsection{Steps and traces}
\label{sec:framework-concurrent}
\label{sec:framework-substprop}

The deductive proofs in
Figures~\ref{fig:sls-values}--\ref{fig:sls-spines} cover the rules of
every \sls~proposition except for the lax modality
${\ocircle}A^+$. It is in the context of the lax modality that we will
present proof terms corresponding to partial proofs.
\begin{align*}
&\mbox{Steps}&
S & ::= \tstep{P}{x}{\Sp}
\\
&\mbox{Traces}&
T & ::= \emptytrace \mid T_1; T_2 \mid S
\end{align*}

\begin{figure}
\fbox{$P :: (\Psi; \Delta)_{\lf{\sigma}} 
   \leadsto_{\Sigma,\subord}
  (\Psi'; \Delta')_{\lf{\sigma'}}$} -- presumes
  $\vdash_{\Sigma,\subord} (\Psi; \Delta)_{\lf{\sigma}}\,{\sf state}$
  and $\Psi \vdash_{\Sigma,\subord} \Delta\,{\sf stable}$
\[
\infer
{\trstep{P}{R} :: 
  (\Psi; \frameoff{\Theta}{\Delta})_{\lf{\sigma}} 
   \leadsto_{\Sigma,\subord}
  (\Psi'; \Delta')_{\lf{\sigma'}}}
{\foctx{\Psi}{\Delta}{R}{\istrue{\susp{{\ocircle}{B^+}}}}
 &
 P :: (\Psi,\tackon{\Theta}{B^+})_{\lf{\sigma}}
   \Longrightarrow_{\Sigma,\subord}
      (\Psi'; \Delta')_{\lf{\sigma'}}}
\]
\caption{\sls~steps}
\label{fig:sls-steps}
\end{figure}

\begin{figure}
\fbox{$T :: (\Psi; \Delta)_{\lf{\sigma}} 
   \leadsto^*_{\Sigma,\subord}
  (\Psi'; \Delta')_{\lf{\sigma'}}$} -- presumes
  $\vdash_{\Sigma,\subord} (\Psi; \Delta)_{\lf{\sigma}}\,{\sf state}$
  and $\Psi \vdash_{\Sigma,\subord} \Delta\,{\sf stable}$
\[
\infer
{\emptytrace :: (\Psi; \Delta)_{\lf{\sigma}} 
               \leadsto^*_{\Sigma,\subord}
             (\Psi; \Delta)_{\lf{\sigma}}}
{}
\quad
\infer
{S :: (\Psi; \Delta)_{\lf{\sigma}}
               \leadsto^*_{\Sigma,\subord}
             (\Psi'; \Delta')_{\lf{\sigma'}}}
{S :: (\Psi; \Delta)_{\lf{\sigma}}
               \leadsto_{\Sigma,\subord}
             (\Psi'; \Delta')_{\lf{\sigma'}}}
\]
\[
\infer
{T; T' :: (\Psi_1; \Delta_1)_{\lf{\sigma_1}}
               \leadsto^*_{\Sigma,\subord}
             (\Psi_3; \Delta_3)_{\lf{\sigma_3}}}
{T :: (\Psi_1; \Delta_1)_{\lf{\sigma_1}}
               \leadsto^*_{\Sigma,\subord}
             (\Psi_2; \Delta_2)_{\lf{\sigma_2}}
&
T' :: (\Psi_2; \Delta_2)_{\lf{\sigma_2}}
               \leadsto^*_{\Sigma,\subord}
             (\Psi_3; \Delta_3)_{\lf{\sigma_3}}}
\]
\caption{\sls~traces}
\label{fig:sls-traces}
\end{figure}

A step $S = \tstep{P}{x}{\Sp}$ corresponds precisely to the notion of
a {\it synthetic inference rule} discussed in
Section~\ref{sec:linsynthetic}. 
 Whereas each typing rule for a
pattern, value, term, or spine corresponded to a rule in \ollll, a
step in \sls~corresponds to a use of left focus, a use of the
left rule for the lax modality, and a use of the 
admissible focal substitution lemma in \ollll:
\[
\infer-[{\it subst}^-]
{\foc{\Psi}{\frameoff{\Theta'}{\Delta}}{\islax{\lf{\sigma}A^+}}}
{\infer[{\it focus}_L]
 {\foc{\Psi}{\Delta}{\istrue{\susp{{\ocircle}B^+}}}}
 {\Delta\,{\it matches}\,\frameoff{\Theta'}{x{:}A^-}
  &
  \deduce
  {\foc{\Psi}{\tackon{\Theta'}{[A^-]}}{\istrue{\susp{{\ocircle}B^+}}}}
  {\vdots\mathstrut}}
 &
 \infer[{\ocircle}_L]
 {\foc{\Psi}{\tackon{\Theta}{[{\ocircle}B^+]}}{\islax{\lf{\sigma}A^+}}}
 {\deduce
  {\foc{\Psi}
    {\tackon{\Theta}{B^+}}
    {\islax{\lf{\sigma}A^+}}\mathstrut} 
  {\deduce{\vdots\mathstrut}
    {\vspace{-4pt}\foc{\Psi'}
     {\Delta'}
     {\islax{\lf{\sigma'}A^+}}}}}}
\]
The spine $\Sp$ corresponds to the complete proof of
$\foc{\Psi}{\tackon{\Theta'}{[A^-]}}{\istrue{\susp{{\ocircle}B^+}}}$,
and the pattern $P$ corresponds to the partial proof of $\foc{\Psi}
{\tackon{\Theta}{B^+}} {\islax{\lf{\sigma}A^+}}$. The typing rules for
steps are given in Figure~\ref{fig:sls-steps}.  Because we understand
these synthetic inference rules as relations between process states,
we can also call steps {\it synthetic transitions}. Traces $T$ are
monoids over steps -- $\emptytrace$ is an empty trace, $S$ is a trace
consisting of a single step, and $T_1; T_2$ is the sequential
composition of traces. The typing rules for traces in
Figure~\ref{fig:sls-traces} straightforwardly reflect this monoid structure.

Steps incorporate left focus and the left rule for ${\ocircle}$, and
{\it let-expressions} $\tlet{T}{V}$, which include traces in deductive
terms, incorporate right focus and the right rule for
${\ocircle}$. The trace $T$ represents the entirety of the partial
proof that proceeds by repeated use of synthetic transitions --
left-focus followed by left-inversion -- and the eventual conclusion
$V$ represents the right focus that follows the series of synthetic
transitions. An \ollll~term that is analogous to an
\sls~let-expression would also be a series of left-focusing phases
followed by a right focus, but the value $V$ would be buried deep
within the structure of the term.

\subsection{Presenting traces}

To present traces in a readable way, we can use a notation that
interleaves process states among the pieces of a trace, a common
practice in Hoare-style reasoning \cite{hoare71proof}.  As an example,
recall the series of transitions that our money-store-battery-robot
system took Section~\ref{sec:linlogtrans}: 
%
\input{figs/test-2trans}
%
This evolution can now be precisely captured as a trace in \sls:
\begin{align*}
&\qquad\qquad
\left(
 x{:}\iseph{\susp{\sf 6bucks}}, ~~
 f{:}\iseph{({\sf battery} \lefti \{ {\sf robot} \})}, ~~
 g{:}\ispers{({\sf 6bucks} \lefti \{ {\sf battery} \})}
\right)
\\
&\trstep{y}{g\,x};
\\
&\qquad\qquad
\left(
 y{:}\iseph{\susp{\sf battery}}, ~~
 f{:}\iseph{({\sf battery} \lefti \{ {\sf robot} \})}, ~~
 g{:}\ispers{({\sf 6bucks} \lefti \{ {\sf battery} \})}
\right)
\\
&\trstep{z}{f\,y}
\\
&\qquad\qquad
\left(
 z{:}\iseph{\susp{\sf robot}}, ~~
 g{:}\ispers{({\sf 6bucks} \lefti \{ {\sf battery} \})}
\right)
\end{align*}

\section{Concurrent equality}
\label{sec:linconcurrenteq}
\label{sec:framework-concurrenteq}

Concurrent equality is a notion of equivalence that operates on
traces, and represents an intermediate point between focusing and
multifocusing \cite{chaudhuri08canonical}.  Consider the following
\sls~signature:
\begin{align*}
 \Sigma = \cdot, 
~&{\sf a} : {\sf prop}\,{\sf ord},
~ {\sf b} : {\sf prop}\,{\sf ord},
~ {\sf c} : {\sf prop}\,{\sf ord},
~ {\sf d} : {\sf prop}\,{\sf ord},
~ {\sf e} : {\sf prop}\,{\sf ord},
~ {\sf f} : {\sf prop}\,{\sf ord},
\\ & 
  {\sf first}  : a^+ \lefti {\ocircle}(b^+ \fuse c^+), 
\\ &
  {\sf left}  : b^+ \lefti {\ocircle}d^+, ~
\\ &
  {\sf right} : c^+ \lefti {\ocircle}e^+, ~
\\ &
  {\sf last} : d^+ \fuse e^+ \lefti {\ocircle}f^+
\end{align*}
With the signature $\Sigma$ and a suitable subordination relation $\subord$, 
we can create two traces with the type
$x_a{:}\susp{\sf a} \leadsto^*_{\Sigma,\subord} x_f{:}\susp{\sf f}$:
\[
\begin{array}{rcl}
T_1 & = 
 & \trstep{x_b, x_c}{\sf first}\,x_a;\\
&& \trstep{x_d}{\sf left}\,x_b;\\
&& \trstep{x_e}{\sf right}\,x_c;\\
&& \trstep{x_f}{\sf last}\,(\tfuser{x_d}{x_e})
\end{array}
\quad
\mbox{versus}
\quad
\begin{array}{rcl}
T_2 & = 
 & \trstep{x_b, x_c}{\sf first}\,x_a;\\
&& \trstep{x_e}{\sf right}\,x_c;\\
&& \trstep{x_d}{\sf left}\,x_b;\\
&& \trstep{x_f}{\sf last}\,(\tfuser{x_d}{x_e})
\end{array}
\]
In both cases, there is an $x_a{:}\susp{\sf a}$ resource
that transitions to a $x_b{:}\susp{\sf b}$ resource and a
$x_c{:}\susp{\sf c}$ resource, and then $x_b{:}\susp{\sf b}$
transitions to $x_d{:}\susp{\sf d}$ while, independently,
$x_c{}\susp{\sf c}$ transitions to $x_d{:}\susp{\sf d}$. Then,
finally, the $x_d{:}\susp{\sf d}$ and $x_e{:}\susp{\sf e}$ combine to
transition to $x_f{:}\susp{\sf f}$, which completes the trace. 

The independence here is key: if two steps consume different
resources, then we want to treat them as independent concurrent steps
that could have equivalently happened in the other order. However, if
we define equivalence only in terms of the $\alpha$-equivalence of
\ollll~derivations, the two traces above are distinct. In this
section, we introduce a coarser equivalence relation, {\it concurrent
  equality}, that allows us to treat traces that differ only in the
interleaving of independent and concurrent steps as being equal.  The
previous section considered the proof terms of \sls~as a fragment of
\ollll~better able to talk about partial proofs.  The introduction of
concurrent equality takes a step beyond \ollll, because it breaks the
bijective correspondence between \ollll~proofs and \sls~proofs. As the
example above indicates, there are simply more \ollll~proofs than
\sls~proofs when we factor the latter moduo concurrent equality
and declare $T_1$ and $T_2$ to be (concurrently) equal. 

Concurrent equality was first was introduced and explored in the
context of CLF \cite{watkins02concurrent}, but our presentation
follows the reformulation in \cite{cervesato12trace}, which defines
concurrent equivalence based on an analysis of the variables that are
used (inputs) and introduced (outputs) by a given step.  Specifically,
our strategy will be to take a particular well-typed trace $T$ and
define a set $I$ of pairs of states $(S_1, S_2)$ with the property
that, if $S_1; S_2$ is a well-typed trace, then $S_2; S_1$ is a
concurrently equivalent and well-typed trace.  This {\it independency
  relation} allows us to treat the trace $T$ as a {\it trace
  monoid}, which implies the existence of a decidable notion of
equivalence \cite{diekert90combinatorics}.

\subsection{Interfaces}

\begin{figure}
\begin{align*}
{\it FV}(()) & = \emptyset
\\
{\it FV}(\lf{a}, P) & = {\it FV}(P)/\{\lf{a}\} 
\\
{\it FV}(x, P) & = {\it FV}(P)
\\
{\it FV}(\lf{t/a}, P) & =  {\it FV}(P) \cup {\it FV}(\lf{t}) \cup \{ \lf{a} \}
\end{align*}
\caption{Free variables of a pattern}
\label{fig:freevarspat}
\end{figure}

We define the independency relation in terms of three functions on
steps $S = (\tstep{P}{x}{\Sp})$: the input variables ${\bullet}S$,
the output variables $S{\bullet}$, and the unified variables
${\ast}S$. Together, the input, output, and unified variables make
up the step's {\it interface}. 

\smallskip
\begin{itemize}
\item The {\it input variables} of a step, denoted ${\bullet}S$, are
  intuitively the set of \lf~and \sls~variables free in the step. The
  notion of what it means to be a free variable is mostly standard,
  but \sls~has one unusual corner case. Our use of unification, and
  the corresponding pattern form $\lf{t/a}, P$, means that
  \sls~patterns can contain free LF variables, as defined in
  Figure~\ref{fig:freevarspat}.  Therefore, the input interface of a
  step ${\bullet}(\tstep{P}{x}{\Sp}) = \{ x \} \cup {\it FV}(P) \cup
  {\it FV}(\Sp)$.

% The {\it consumed variables} of a step $S = (\tstep{P}{x}{\Sp})$,
% denoted by ${\ast}S$, are the subset of the input variables that no
% longer appear in the process state after a step. Every \sls~variable
% in ${\bullet}S$ associated with a judgment $\istrue{T}$ or $\iseph{T}$
% is a consumed variable. Any LF variable $\lf{a}$ that is subject to
% unification $\lf{t/a}$ in the pattern $P$ is also a consumed variable
% if $\lf{a}$ is a free variable of the pattern. If $\lf{a}$ is free in
% $P$, then $\lf{t}$ must also be a variable $\lf{b}$, and if $\lf{b}$
% is likewise free in $P$ it is included in the set of consumed
% variables.

\item The {\it output variables} of a step $S = (\tstep{P}{x}{\Sp})$,
  denoted by $S{\bullet}$, are all the variables $\lf{a}$ or $x$ bound
  by the pattern $P$ that are not subsequently consumed by a
  unification $\lf{t/a}$ in the same pattern.

\item The {\it unified variables} of a step, denoted by ${\ast}S$, are
  the free variables of a step that are modified by unification. If
  $\lf{t/a}$ appears in a pattern and $\lf{a}$ is free in the pattern,
  then $\lf{t} = \lf{b}$ for some other variable $\lf{b}$; both
  $\lf{a}$ and $\lf{b}$ (if the latter is free in the pattern) are
  included in the step's unified variables.
\end{itemize}
\smallskip

\subsection{Independency}

Consider a well-typed trace $S_1; S_2$ with two steps. It is possible,
by renaming bound variables, to ensure that $\emptyset = {\bullet}S_1
\cap S_2{\bullet} = {\bullet}S_1 \cap S_1{\bullet} = {\bullet}S_1 \cap
S_1{\bullet}$, so we will generally assume that this is the case.  The
order of $S_1$ and $S_2$ is fixed if $S_1$ introduces variables that
are used by $S_2$ -- that is, if $\emptyset \neq S_1{\bullet} \cap
{\bullet}S_2$. For example, if $S_1 = (\trstep{x_b, x_c}{\sf
  first}\,x_a)$ and $S_2 = (\trstep{x_d}{\sf left}\,x_b)$, then
$\{x_b\} = S_1{\bullet} \cap {\bullet}S_2$, and the two steps cannot
be reordered relative to one another. 

Conversely, the condition that $\emptyset = S_1{\bullet} \cap
{\bullet}S_2$ is sufficent to allow reordering in a CLF-like framework
\cite{cervesato12trace}. In \sls, however, unification can have subtle
effects. Consider the following two-step trace:
\begin{align*}
& \qquad\qquad
(\lf{a}{:}p, \lf{b}{:}p; ~~ x{:}\iseph{\left({\ocircle}(\lf{b} \doteq \lf{a})\right)}, ~~
 y{:}\iseph{\left({\sf foo}\,\lf{b} 
                 \lefti {\ocircle}({\sf bar}\, \lf{a})\right)}, ~~
 z{:}\iseph{\susp{{\sf foo}\,\lf{a}}})
\\
& \trstep{\lf{b/a}}{x};
\\
& \qquad\qquad
(\lf{b}{:}p; ~~ 
 y{:}\iseph{\left({\sf foo}\,\lf{b} 
                 \lefti {\ocircle}({\sf bar}\, \lf{b})\right)}, ~~
 z{:}\iseph{\susp{{\sf foo}\,\lf{b}}})
\\
& \trstep{w}{y\,z} 
\\
& \qquad\qquad
(\lf{b}{:}p; ~~ w{:}\iseph{\susp{{\sf bar}\,\lf{b}}})
\intertext{
%
  This trace cannot be reordered even though $\emptyset = \emptyset
  \cap \{ y, z \} = (\trstep{\lf{b/a}}{x}){\bullet} \cap
  {\bullet}(\trstep{w}{y\,z})$, because the atomic term $y\,z$ is only
  well typed after the LF variables $\lf{a}$ and $\lf{b}$ are unified.
  To avoid this situation, it is sufficient to require that $\emptyset
  = {\ast}S_1 \cap {\bullet}S_2$. In the other direction, consider the
  following very similar trace:
%
}
& \qquad\qquad
(\lf{a}{:}p, \lf{b}{:}p; ~~ x{:}\iseph{\left({\ocircle}(\lf{a} \doteq \lf{b})\right)}, ~~
 y{:}\iseph{\left(\forall{\lf{a'}}{:}p.\,
                {\sf foo}\,\lf{a'} 
                 \lefti {\ocircle}({\sf bar}\, \lf{a'})\right)}, ~~
 z{:}\iseph{\susp{{\sf foo}\,\lf{a}}})
\\
& \trstep{w}{y\,\lf{a}\,z};
\\
& \qquad\qquad
(\lf{a}{:}p, \lf{b}{:}p; ~~ x{:}\iseph{\left({\ocircle}(\lf{a} \doteq \lf{b})\right)}, ~~
 w{:}\iseph{\susp{{\sf bar}\,\lf{a}}})
\\
& \trstep{\lf{b/a}}{x}
\\
& \qquad\qquad
(\lf{b}{:}p; ~~ w{:}\iseph{\susp{{\sf bar}\,\lf{b}}})
\end{align*}
The reordered trace $\trstep{\lf{b/a}}{x}; \trstep{w}{y\,\lf{a}\,z}$
is not well-typed: the second step has $\lf{a}$ free, and after
$\lf{a}$ and $\lf{b}$ are unified $\lf{a}$ is no longer free in the
process state.  We could consider applying the substitution
$\lf{[b/a]}$ to the second process state to restore its well-typedness
when the two steps are reordered, but it is much simpler to just
preclude reordering in this instance by requiring that 
$\emptyset = {\bullet}S_1 \cap {\ast}S_2$.

These three conditions establish independency: given a well-typed
trace $S_1; S_2$ where $\emptyset = {\bullet}S_1 \cap S_2{\bullet} =
{\bullet}S_1 \cap S_1{\bullet} = {\bullet}S_1 \cap S_1{\bullet}$
(which is always possible by renaming bound variables), if $\emptyset
= S_1{\bullet} \cap {\bullet}S_2 = {\bullet}S_1 \cap {\ast}S_2 =
{\ast}S_1 \cap {\bullet}S_2$, then $S_2; S_1$ is also a well-typed
trace.  We can calculate $S_1{\bullet} \cap {\bullet}S_2$ and
${\bullet}S_1 \cap S_2{\bullet}$ for any two steps in the same trace,
even if their composition is not well-typed. Therefore, given a
well-typed and sufficiently-renamed trace $T$, we can let the
independency relation $I$ be the set of pairs of steps $(S, S')$ in
$T$ where $\emptyset = S_1{\bullet} \cap {\bullet}S_2 = {\bullet}S_1
\cap {\ast}S_2 = {\ast}S_1 \cap {\bullet}S_2$. Letting $I$ be the
independency relation of a trace monoid, we get the set of traces that
are concurrently equal to $T$: all traces $T'$ that are
concurrently equal to $T$ are well-typed with the same type as $T$.

\section{Adequate encoding}
\label{sec:sls-adequate}

In Section~\ref{sec:lf-adequacy} we discussed LF the encoding of
$\lambda$-calculus terms in to LF terms of type ${\sf exp}$, captured by
the invertible function $\interp{e}$. Adequacy was extended to Linear
LF (LLF) by Cervesato and Pfenning \cite{cervesato02linear} and was
extended to Ordered LF (OLF) by Polakow \cite{polakow01ordered}. The
deductive fragment of \sls~approximately extends both LLF and OLF, and
the adequacy arguments made by Cervesato and Polakow extend
straightforwardly to \sls.

A critical aspect of any adequacy arguments is an understanding of the
structure of the LF context. In the statement of adequacy for untyped
$\lambda$-calculus terms, for instance, it is necessary to require
that the context $\Psi$ take the form $\lf{a_1}{:}{\sf
  exp},\ldots,\lf{a_n}{:}{\sf exp}$. In the adequacy theorems that
have been presented for deductive logical frameworks, the structure
of the context is {\it regular} -- 

that Polakow
and Cervesato discuss, we only need to

 to impose a {\it regular} structure
on the context. 

\subsection{Adequacy for LF and deductive terms}

Adequacy in both Linear LF \cite{cervesato02linear} and Ordered LF 
\cite{polakow01ordered} requires that we be able to capture contexts
with 

\subsection{Adequacy for concurrent traces}


\section{The \sls~implementation}
\label{sec:prototype}

The prototype implementation of \sls~contains 
Following CLF and the Celf implementation, we write ${\ocircle}A$ in
Celf as \verb|{A}|. The mobile modality ${\gnab}A$ doesn't have an
ASCII representation, so write \verb|$A| when $A$ is
mobile. Upshifts and downshifts are always inferred: this means that
we can't write down ${\uparrow}{\downarrow}A$ or
${\downarrow}{\uparrow}A$, but neither of these \ollll~propositions
are part of the \sls~fragment anyway.

Another change is for the sake of readability: if \verb|P| is a
positive atomic proposition, we can write \verb|!P| wherever \verb|P|
is expected. This allows us to write either \verb|a * b * c| or
\verb|a * $b * !c| in SLS to express the positive proposition ${\sf a}
\fuse {\sf b} \fuse {\sf c}$ where ${\sf a}$, ${\sf b}$, and ${\sf c}$
are ordered, linear, and persistent positive atomic propositions
(respectively).

\section{Logic programming interpretation}
\label{sec:framework-logicprog}

In Chapter 3 I call this distinction one between ``concurrent and deductive''
proofs, operationally it's the difference between forward chaining 
and backward chaining. Maintaining the distinction between these is why
we don't want the class $p^-_\mlax$ in the logic.

We have talked about {\it concurrent} and {\it deductive} proof
objects, and also about the intuitive notion of {\it concurrent
  computation} as the non-backtracking, forward-chaining 

(definitely discuss forward-chaining and backward-chaining as concepts)

Expand on literature review of quiescence from HOSC Section 4

Leave the question of quescense versus eagerly-trying-to-right-focus
versus saturation ambiguous. If you talk about pure saturation the
forward-reference Chapter 8. 

\subsection{Modes and well-moded specifications}
\label{sec:framework-modes}

\section{Design decisions}
\label{sec:designdecisions}

\subsection{Pseudo-positive atoms}
\label{sec:pseudopositive}

One of main features that \sls~has and CLF does not is a treatment of
positive atomic propositions, which can be either ordered, linear, and
persistent. Positive atomic propositions make it easy to characterize
the synthetic transitions associated with a particular rule. For
example, if ${\sf foo}$, ${\sf bar}$, and ${\sf baz}$ are all linear
atomic propositions, then the presence of a rule ${\sf foo} \fuse {\sf
  bar} \lefti \{ {\sf baz} \}$ in the signature is associated with
synthetic transitions of the form
%
$(\Psi; \matchconj{\Delta}{\matchconj{x{:}\iseph{\susp{\sf
        foo}}}{y{:}\iseph{\susp{\sf bar}}}})
 \leadsto
 (\Psi; \mkconj{\Delta}{z{:}\iseph{\susp{\sf baz}}})$.
%
The presence of the
rule $\sf r$ enables steps of this form, and every step made by
focusing on the rule has this form.

CLF has no positive propositions, so the closest analogue that we can
consider is where ${\sf foo}$, ${\sf bar}$, and ${\sf baz}$ are
negative propositions, and the rule ${\gnab}{\sf foo} \fuse
{\gnab}{\sf bar} \lefti \{ {\gnab}{\sf baz} \}$ appears in the
signature. Such a rule is associated with synthetic transitions of the
form
%
$(\Psi; \matchconj{\Delta}{\matchconj{\Delta_1}{\Delta_2}}) \leadsto
(\Psi; \mkconj{\Delta}{z{:}\istrue{{\sf baz}}})$ such that
$\foc{\Psi}{\restrictto{\Delta_1}{\meph}}{\istrue{\susp{\sf foo}}}$
and $\foc{\Psi}{\restrictto{\Delta_2}{\meph}}{\istrue{\susp{\sf
      bar}}}$. It is a relatively simple syntactic criterion to
enforce that a sequent like $\foc{\Psi}{\Delta_1}{\istrue{\susp{\sf
      foo}}}$ can only be derived if $\Delta_1$ matches
$x{:}\sf{foo}$; we must simply ensure that there are no propositions
of the form $\ldots \lefti {\sf foo}$ or $\ldots \righti {\sf foo}$ in
the signature or context. (This is essentially the \sls~version of the
subordination criteria that allowed us to conclude that an LF type was
only inhabited by variables Section~\ref{sec:slsframework}.)
%
When that is the case, we can
associate the rule ${\sf foo} \fuse {\sf bar} \lefti \{ {\sf baz} \}$
with the synthetic transition $(\Psi;
\matchconj{\Delta}{\matchconj{x{:}{\islvl{\sf foo}}}{y{:}{\islvl{\sf
        bar}'}}}) \leadsto (\Psi; \mkconj{\Delta}{z{:}\iseph{\susp{\sf
      baz}}})$ under the condition that neither $\mlvl$ or $\mlvl'$
are $\mtrue$.

A negative atomic proposition can be called {\it pseudo-positive} if
it is only possible to prove the proposition when it is the sole
member of the context. Pseduo-positive atoms can actually be used a
bit more generally than positive atomic propositions. A positive
atomic proposition is necessarily associated with one of the three
judgments $\mtrue$, $\meph$, or $\mpers$, but pseudo-positive
propositions can associate with any of the contexts. This gives
pseudo-positive atoms in CLF or \sls~the flavor of positive atomic
propositions under Andreoli's atom optimization
(Section~\ref{sec:atomopt}).

It is, of course, possible to consistently associate particular
pseudo-positive propositions with particular modalities, which means
that pseduo-positive propositions can subsume the positive
propositions of \sls. The tradeoff between positive and
pseudo-positive propositions could be resolved either way. By
including positive atomic propositions, we made \sls~more complicated,
but in a local way -- we needed a few more kinds and a few more
rules. If we used pseudo-positive propositions, the notion of
synthetic transitions would be intertwined with the subordination-like
analysis that enforces their correct usage.

\subsection{The need for traces}

One of the most important differences between \sls~and its
predecessors, especially CLF, is that traces are treated as
first-class syntactic objects. This allows us to talk about 
partial proofs and thereby encode our earlier 
money-store-battery-robot example as a trace with this type:
\begin{align*}
& \left(
 x{:}\iseph{\susp{\sf 6bucks}}, ~~
 f{:}\iseph{({\sf battery} \lefti \{ {\sf robot} \})}, ~~
 g{:}\ispers{({\sf 6bucks} \lefti \{ {\sf battery} \})}
\right)
\\
\leadsto^* &
\left(
 z{:}\iseph{\susp{\sf robot}}, ~~
 g{:}\ispers{({\sf 6bucks} \lefti \{ {\sf battery} \})}
\right)
\end{align*}
It is also possible to translate the example from Chapter~2
as a {\it complete} proof of the following proposition:
\[
  {\sf 6bucks} 
      \fuse {\gnab}({\sf battery} \lefti \{ {\sf robot} \})
      \fuse {!}({\sf 6bucks} \lefti \{ {\sf battery} \})
     \lefti
     \{ {\sf robot} \}
\]

Generally speaking, we can try to represent a trace $T :: (\Psi;
\Delta) \leadsto^* (\Psi'; \Delta')$ as a closed deductive proof
$\lambda P.\,\tlet{T}{V}$ of the proposition $(\exists
\Psi.\,{\fuse}\Delta) \lefti \{ \exists \Psi'.\,{\fuse}\Delta \}$,
where the pattern $P$ re-creates the initial process state $(\Psi;
\Delta)$ and the all the components of the final state are captured in
the value $V$.  The problem with this approach is that the final
proposition is under no particular obligation to capture the structure
of the final process state. This can be seen in the example above: to
actually capture the structure of the final process state, we should
have concluded ${\sf robot} \fuse {!}({\sf 6bucks} \lefti \{ {\sf
  battery} \})$ insetad of simply ${\sf robot}$. It is also possible
to conclude any of the following:
\smallskip
\begin{itemize}
\item ${\sf robot} \fuse {!}({\sf 6bucks} \lefti \{ {\sf
  battery} \}) \fuse {!}({\sf 6bucks} \lefti \{ {\sf
  battery} \})$, or 
\item ${\sf robot} \fuse {\downarrow}({\sf 6bucks} \lefti \{ {\sf
  battery} \}) \fuse {\gnab}({\sf 6bucks} \lefti \{ {\sf
  battery} \})$, or even
\item ${\sf robot}  \fuse {\gnab}({\sf 6bucks} \fuse {!}({\sf battery} \lefti \{ {\sf robot} \}) \lefti \{ {\sf robot} \})
  \fuse {\downarrow}({\sf robot}
\lefti \{ {\sf robot} \})$.
\end{itemize}
\smallskip 
%
The problem with encoding traces as complete proofs, then, is that
values only precisely capture the structure of contexts when there are
no variables or persistent propositions. Cervesato and Scedrov
approach this problem by severely restricting the logic and changing
the interpretation of the existential quantifier so that it acts like
a nominal quantifier \cite{cervesato09relating}. The introduction of
traces allows us to avoid similar restrictions in \sls.

Despite traces being proper syntactic objects, they are not
first-class concepts in the theory: they are derived from focused
\ollll~terms and interpreted as partial proofs. Because hereditary
substitution, identity expansion, and focalization are only defined on
complete \ollll~proofs, these theorems and operations only apply by
analogy to the deductive fragment \sls; they do not apply to traces.
In joint work with Deng and Cervesato, we considered a presentation of
logic that treats process states and traces as first-class concepts
and refomulates the usual properties of cut and identity in terms of
coinductive simulation relations on process states
\cite{deng12relating}. We hope that this work will eventually read to
a better understanding of traces, but the gap remains quite large.

% In \cite{deng12relating}, we presented the {\it logical preorder} as a
% relation $\Delta_1 \preceq \Delta_2$ between propositional process states
% that holds whenever, for all $\Theta$ and $U$, we have that
% $\tackon{\Theta}{\Delta_1} \vdash U$ implies $\tackon{\Theta}{\Delta_2}
% \vdash U$. An elegant property, {\it harmony}, relates the logical 
% preorder to cut admissibility and identity expansion. 


% \subsection{A logic of traces}

% Traces in \sls~are syntactic objects. They are not, however,
% first-class objects in the theory: they are derived from focused
% \ollll~terms and explained as partial proofs. Because hereditary
% substitution, identity expansion, and focalization are only defined on
% complete \ollll~proofs, these theorems and operations only apply by
% analogy to the deductive fragment \sls; they do not apply to traces.

% In \cite{deng12relating}, we presented the {\it logical preorder} as a
% relation $\Delta_1 \preceq \Delta_2$ between propositional process states
% that holds whenever, for all $\Theta$ and $U$, we have that
% $\tackon{\Theta}{\Delta_1} \vdash U$ implies $\tackon{\Theta}{\Delta_2}
% \vdash U$. An elegant property, {\it harmony}, relates the logical 
% preorder to cut admissibility and identity expansion. 

\subsection{LF as a term language}
\label{sec:why-not-fully-dependent}

The decision to use LF as a first-order domain of quantification
rather than using a fully-dependent system is based on several
considerations. First and foremost, this choice was sufficient for the
purposes of this thesis. In fact, for the purposes of this thesis, we
could used an even simpler term language of simply-typed LF
\cite{pfenning08church}; two other logic programming interpretations
of \sls-like frameworks, Lollimon \cite{lopez05monadic} and Ollibot
\cite{pfenning09substructural}, were based on simply-typed term
languages. Canonical LF and Spine Form LF are, at this point,
well-understood enough that the additional overhead of fully
dependently-typed terms is not a significant burden, and there are
many examples beyond the scope of this thesis where term dependency is
useful.

On a theoretical level, it is a significant simplification when we
restrict ourselves to {\it any} typed term language with a reasonable
notion of equality and simultaneous substitution. The conceptual
priority in this chapter is clear: Section~\ref{sec:sls-termlanguage}
describes LF terms, Section~\ref{sec:slsframework} describes proof
terms as a fragment of focused \ollll, and
Section~\ref{sec:framework-concurrenteq} describes a coarser
equivalence on proof terms, concurrent equality. But if the language
of quantification was \sls~terms, these three considerations would be
mutually dependent -- we would need to characterize concurrent
equality before presenting the logic itself. For the purposes of
showing that a logical framework can be carved out from a focused
logic, the central thesis of this and the previous two chapters, it is
easiest to break this circular dependency. We conjecture that this
complication is no great obstacle, but this thesis avoids the issue.

On a practical level, there are advantages to using a well-understood
term language. The \sls~prototype implementation
(Section~\ref{sec:prototype}) uses the mature type reconstruction
engine of Twelf to reconstruct LF terms. Schack-Nielsen's
implementation of type reconstruction for Celf is complicated by the
requirements of dealing with type reconstruction for a substructural
term language, a consideration that is completly orthogonal to this
thesis \cite{schacknielsen08celf}. 

Finally, it is not clear that the addition of full CLF-like dependency
comes with great expressive benefit. 
% Even in LF and Twelf, many
% interesting specifications could be encoded in a two-level version of
% the language: a simply-typed object term language and a
% dependently-typed proof term language with first-order quantification
% over object terms. This restriction is sufficient for settings such as
% Harper's comprehensive survey of programming language design
% \cite{harper12practical},\footnote{Harper's metatheory also extends LF
%   by drawing a distinction between standard variables and nominal
%   parameters, but this is an orthogonal point.} and it is built in to
% the educational proof assistant SASyLF \cite{aldrich08sasylf}. 
In LF and Twelf, the ability to use full dependent types is critical
in part because it allows us to express {\it metatheorems} -- theorems
about the programming languages and logics we have encoded, like
progress and preservation for a programming language or cut
admissibility for a logic. Substructural logical frameworks like LLF
and CLF, in contrast, have not been successful in capturing
metatheorems with dependent types. Instead, metatheorems have been
done by encoding the substructural derivations in LF
\cite{crary10higher} or in HLF \cite{reed09hybrid}, an extension to LF that uses
equational theories to capture substructural propreties. 

% But in substructural logical frameworks like Linear LF, full
% dependency has been found to be {\it insufficient} for expressing
% metatheorems, which motivated the development of Hybrid LF as a
% framework for writing metatheorems about LF \cite{reed09hybrid}. The
% implementation of Hybrid LF effectively creates a stratification like
% \sls's -- full LF as an object term language, a linear logical
% framework with first-order quantification over object language terms,
% and a hybrid language that can inspect both LF object terms and linear
% proof terms.



\subsection{Variations on concurrent equality}

Concurrent equality is related to the equivalence relation induced by
{\it multifocusing} \cite{chaudhuri08canonical}. Multifocusing has
only been explored carefully in the context of 


It is not obvious that our treatment of the interaction between 
unification and concurrent equivalence is the right one. 

% \subsection{Concurrent equality and multifocusing}

% Concurrent equality is related to the equivalence relation induced by
% {\it multifocusing} \cite{chaudhuri08canonical}. Multifocusing is a
% concept, 

% One reason multifocusing is 

%  that has only been carefully explored in classical linear
% logic; the central change is that the rules which begins a focusing
% phase (in our presentation of MELL there were three: ${\it focus_L}$,
% ${\it focus_R}$, and ${\it copy}$) are allowed to simultaneously pull
% other propositions into focus.  As an illustration, if we reuse our
% notation from Section~\ref{sec:linnote} we can present the following
% plausible candidates for the multifocus rules in an intuitionistic
% system:
% \[
% \infer[{\it focus}_L]
% {\mildseq{\Gamma}{\Delta / A_1^-, \ldots, A_n^- }{U}}
% {n > 1
%  &
%  \mildseq{\Gamma}{\Delta, [A_1^-], \ldots, [A_n^-]}{U}}
% \quad
% \infer[{\it focus}_R]
% {\mildseq{\Gamma}{\Delta / A_1^-, \ldots, A_n^-}{C^+}}
% {n \geq 1
%  &
%  \mildseq{\Gamma}{\Delta, [A_1^-], \ldots, [A_n^-]}{[C^+]}}
% \]
% Multifocusing, however,
% appears to provide an even coarser notion of equivalence on focused
% proofs than concurrent equality does. In particular, the two
% distinct focusing proofs below are not concurrently equal: the proof
% on the right succeeds at proving $\langle c^- \rangle$ in one step,
% but leaves a subgoal in which $b^+$ is proved indirectly, whereas the
% proof at the right first transitions from having $\langle a^+ \rangle$
% and $a^+ \lolli {\uparrow} b^+$ resources to having a $\langle b^+
% \rangle$ resource, and only then proves $\langle c^- \rangle$, leaving
% a subgoal in which $b^+$ is proved directly.
% \[
% \infer
% {\mildseq{\cdot}
%   {~~
%    \langle a^+ \rangle, ~
%    a^+ \lolli {\uparrow}b^+, ~
%    {\downarrow}{\uparrow}b^+ \lolli c^-
%    ~~}
%   {~~\langle c^- \rangle}}
% {\infer
% {\mildseq{\cdot}
%   {~~
%    \langle a^+ \rangle, ~
%    a^+ \lolli {\uparrow}b^+
%    ~~}
%   {b^+}}
% {\infer
% {\mildseq{\cdot}
%   {~~
%    \langle b^+ \rangle
%    ~~}
%   {b^+}}
% {}}}
% \deduce{\mathstrut}
% {\deduce{\mathstrut}
% {\mbox{\it vs.}\mathstrut}}
% \infer
% {\mildseq{\cdot}
%   {~~
%    \langle a^+ \rangle, ~
%    a^+ \lolli {\uparrow}b^+, ~
%    {\downarrow}{\uparrow}b^+ \lolli c^-
%    ~~}
%   {~~\langle c^- \rangle}}
% {\infer
% {\mildseq{\cdot}
%   {~~
%    \langle b^+ \rangle, ~
%    {\downarrow}{\uparrow}b^+ \lolli c^-
%    ~~}
%   {~~\langle c^- \rangle}}
% {\infer
% {\mildseq{\cdot}
%   {~~
%    \langle b^+ \rangle
%    ~~}
%   {~~b^+}}
% {}}}
% \]
% Despite the lack of a full account of intuitionistic multifocusing, we
% can observe that the analogue of this sequent in classical linear
% logic has only one multifocused proof, and it is reasonable to
% conjecture that an account of multifocusing for intuitionistic logic
% would also relate these proofs. In classical linear logic,
% multifocusing offers a very fundamental normal form: any two proofs
% that can be made equal by locally permuting inference rules have the
% same multifocused proof.

% CLF's restricted form of concurrent equality will be sufficient for
% the logical framework in Chapter 4. In fact, for the fragment of the
% the logic in Chapter 3 that comprises our logical framework in Chapter
% 4, I conjecture that concurrent equality and the equality given by
% multifocusing coincide.\footnote{This obviously means that the example
%   above will be outside the logical fragment that comprises the logical
%   framework.}  This conjecture is obviously difficult to make precise,
% much less prove, without a general theory of multifocusing in
% intuitionistic logic.


% \subsection{A warning about normalization}
% \label{sec:warning}

% In our earlier discussion of hereditary substitution and canonical
% forms in Section~\ref{sec:linlogicalframeworks}, we mentioned that the
% normalization theorem provided by hereditary substitution was weaker
% than the so-called weak normalization theorem for LF. That is because
% the weak normalization theorem says that any well-typed term can be
% converted into a canonical ($\beta$-normal and $\eta$-long) term by a
% particular series of $\beta$ and $\eta$ conversions. It is
% self-evident, by this statement of the theorem, that the resulting
% canonical term is equivalent to the original term.

% On the other hand, when we use hereditary substitution in the obvious
% way to obtain a Canonical LF term from an arbitrary non-canonical LF
% term, we gain {\it no guarantees} about the relationship between the
% non-canonical LF term and the Canonical LF term. The statement of the
% theorem does not preclude taking a $\beta$-normal, $\eta$-long LF term
% (like $\lambda x. \lambda y. x$ of type $p \rightarrow p \rightarrow
% p$ for some atomic type $p$) into a structurally different Canonical
% LF term (like $\lambda x. \lambda y. y$, which also has type $p
% \rightarrow p \rightarrow p$). It is possible to gain such a guarantee
% for LF, as Martens and Crary have shown in unpublished work
% \cite{martens11mechanizing}, but this result is a non-trivial statement
% about the constructive content of the normalization theorem. 

% In our setting, we should be concerned that we might take a focused
% proof, turn it into an unfocused proof by the obvious de-focalization
% procedure (the constructive content of
% Theorem~\ref{thm:linfocsound}), and then turn it back into a focused
% proof by focalization (the constructive content of
% Theorem~\ref{thm:linfoccomplete}) only to obtain a proof that was not
% identical or even related. This is not at all a merely hypothetical
% concern. We can run the mechanized structural focalization result from
% \cite{simmons11structural} on a persistent proposition,
% %
%    $a^+ \supset 
%    {\downarrow}(a^+ \supset {\uparrow}b^+) \supset
%    {\downarrow}({\downarrow}{\uparrow}b^+ \supset c^-) \supset
%    c^-$, 
% %
% which is similar to the example from
% Section~\ref{sec:linconcurrenteq}.  In persistent logic (as in
% linear logic) that proposition has two focused propositions that
% are probably multifocusing equivalent (given a reasonable intuitionistic
% notion of multifocusing) but that are not concurrently equivalent
% under the proposed definition of concurrent equality. 
% However, if we take the focused proof that focuses 
% first on $a^+ \supset {\uparrow}b^+$, transform it into an unfocused 
% proof, and then re-focus it, we will get the proof that focuses 
% first on ${\downarrow}{\uparrow}b^+ \supset c^-$. Focalization,
% in other words, is not a partial inverse of de-focalization in the structural
% focalization development, except maybe modulo the (as yet undefined)
% equivalence relation established by multifocusing. 

% This example illustrates why we must be careful, but it is not a fatal
% flaw for two reasons. The first reason is the aforementioned
% conjecture that, for the restricted logical fragment defined in
% Chapter 4 as the basis of our logical framework, the focalizations of
% two proofs are concurrently equal if and only if the original proofs
% are convertible by local permutations of rules, the same condition
% that multifocusing satisfies. If this conjecture holds, it ought to be
% the case that, modulo this coarser equivalence, focalization {\it is}
% a partial inverse of de-focalization. Second, what is really at stake
% here is our ability to write down non-normal proofs in a logical
% framework that then normalizes them -- which is what the Twelf
% implementation of LF and the Celf implementation of CLF do -- with the
% confidence that we can look at a non-normal proof and know its
% corresponding canonical form. In this thesis, we will be content to
% work throughout with focused proofs and their analogues, so we can
% afford to leave questions about convertability and weak normalization
% to future work.

