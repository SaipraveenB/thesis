
\chapter{Linear logic}

Logic as it has been traditionally understood and studied -- both in
its classical and intuitionistic varieties -- treats the truth of a
proposition as a {\it persistent resource}. That is, if we have
evidence for the truth of a proposition, we can ignore that evidence
if it is not needed and reuse the evidence as many times as we need
to. Throughout this thesis, ``logic as it has been traditionally
understood as studied'' will be referred to as {\it persistent} logic
to emphasize this treatment of evidence. 

Linear logic, which was studied and
popularized by Girard \cite{girard87linear},
treats evidence as an {\it ephemeral} resource; the use of an
ephemeral resource consumes it, at which point it is unavailable for
further use.  Linear logic, like persistent logic, comes in classical
and intuitionistic flavors. We will favor intuitionistic linear logic
in part because the propositions of intuitionistic linear logic
(written $A$, $B$, $C$, \ldots) have a more natural correspondence
with our physical intuitions about consumable resources. Linear
conjunction $A \tensor B$ represents the resource built from the
resources $A$ and $B$; if you have both a bowl of soup {\it and} a
sandwich, that resource can be represented by the proposition ${\sf
  soup} \otimes {\sf sandwich}$. Linear implication $A \lolli B$
represents a resource that can interact with another resource $A$ to
produce a resource $B$. One robot with batteries not included could be
represented as the linear resource $({\sf battery} \lolli {\sf
  robot})$, and the linear resource $({\sf 6bucks} \lolli {\sf soup}
\tensor {\sf sandwich})$ represents the ability to use \$6 to obtain
lunch -- but only once!\footnote{Conjunction will always bind more
  tightly than implication, so this is equivalent to the proposition
  ${\sf 6bucks} \lolli ({\sf soup} \tensor {\sf sandwich})$.} Linear
logic also has a modal connective ${!}A$ representing a persistent
resource that can be
used to generate any number of $A$ resources, including zero. The
Panera ``You Pick Two'' menu might be represented as
\[ {!}({\sf 6bucks} \lolli {\sf soup} \tensor {\sf sandwich}) \otimes
{!}({\sf 6bucks} \lolli {\sf soup} \tensor {\sf salad}) \otimes
{!}({\sf 6bucks} \lolli {\sf sandwich} \tensor {\sf salad}),\] as the
menu gives you the opportunity to exchange six dollars for two
distinct members of the set $\{ {\sf soup}, {\sf salad}, {\sf
  sandwich} \}$ any number of times.

\input{figs/fig-linear.tex}

Figure~\ref{fig:linear} presents a standard sequent calculus for
linear logic, in particular the so-called {\it multiplicative,
  exponential} fragment of intuitionistic linear logic (or {\it
  MELL}). It corresponds most closely to Barber's dual intuitionistic
linear logic \cite{barber96dual}, but also to Andreoli's dyadic system
\cite{andreoli92logic} and Chang et al.'s judgmental analysis of
intuitionistic linear logic \cite{chang03judgmental}.

\subsection*{Transitions in linear logic}

The propositions of intuitionistic linear logic, and linear implication
in particular, capture a notion of state change: we can {\it
  transition} from a state where we have both a ${\sf battery}$ and
the battery-less robot (represented, as before, by the linear
implication ${\sf battery} \lolli {\sf robot}$) to a state where we
have the battery-endowed (and therefore presumably functional) robot
(represented by the proposition ${\sf robot}$). In other words, the
proposition
%
\[{\sf battery} \otimes ({\sf battery} \lolli {\sf robot}) \lolli
{\sf robot}\] 
%
is provable in linear logic. These transitions can be chained
together as well: if we start out with ${\sf
  6bucks}$ instead of ${\sf battery}$ but we also have the
persistent ability to turn ${\sf 6bucks}$ into a ${\sf battery}$ --
just like we turned \$6 into a bowl of soup and a salad at Panera --
then we can ultimately get our working robot as well.
Written as a series of transitions, the picture looks like this:
\[
\begin{array}{ccccc}
\begin{array}{c}
\mbox{\it \$6 (1)}\medskip\\ 
\mbox{\it battery-free robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}
\end{array}
& \leadsto &
\begin{array}{c}
\mbox{\it battery  (1)}\medskip\\ 
\mbox{\it battery-free robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}
\end{array}
& \leadsto &
\begin{array}{c}
\mbox{\it robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}\medskip\\~\\
\end{array}
\end{array}
\]
In linear logic, these transitions correspond to the provability
of the proposition
\[{!}({\sf 6bucks} \lolli {\sf battery}) \otimes {\sf 6bucks} \otimes
({\sf battery} \lolli {\sf robot}) \lolli {\sf robot}.\] 
A derivation of this proposition is given in
Figure~\ref{fig:unfocused-robot}.\footnote{In Chapter XXX, I will
  argue that this view isn't quite precise enough, and that the most
  natural representation of state change from the state $A$ to the
  state $B$ isn't really captured by derivations of the proposition $A
  \lolli B$ or by derivations of the hypothetical judgment
  $\seq{\cdot}{A}{B}$.  However, this view remains a simple and useful
  one; Cervesato and Scedrov cover it thoroughly in the context of
  intuitionistic linear logic \cite{cervesato09relating}.}  

\input{figs/fig-unfocused-robot}

It is precisely because linear logic contains this natural notion of
state and state transition that a rich line of work, dating back to
Chirmar's 1995 Ph.D. thesis, has sought to use linear logic as a {\it
  logical framework} for describing stateful systems
\cite{chirimar95proof,cervesato02linear,
  cervesato02concurrent,pfenning04substructural,miller09formalizing,
  pfenning09substructural,cervesato09relating}.  

\subsection*{Logical frameworks}

Generally speaking, logical frameworks use the {\it structure} of
proofs in a logic (like linear logic) to describe the structures we're
really interested in (like the process of obtaining a robot).  There
are two related reasons why linear logic as described in
Figure~\ref{fig:linear} is not immediately useful as a logical
framework. First, the structure of the proof in
Figure~\ref{fig:unfocused-robot} doesn't really match the intuitive
two-step transition that we sketched out above. Second, there are {\it
  lots} of derivations of our example proposition according to the
rules in Figure~\ref{fig:linear}, even though there's only one
``real'' series of transitions that get us to a working robot. The use
of ${!}L$, for instance, could be permuted up past the ${\otimes}L$
and then past the ${\lolli}L$ into the left branch of the proof. These
differences represent inessential nondeterminism in proof construction
or in proof search -- they just get in the way of the structure that
we are trying to capture. 

This is a general problem in the construction of logical frameworks,
and we'll discuss two solutions in the context of LF, a logical
framework based on dependent type theory that has proved to be a
suitable means of encoding a wide variety of deductive systems, such
as logics and programming languages \cite{harper93framework}.  The
first solution is to define an appropriate equivalence class of
proofs, and the second solution is to define an appropriate fragment
of canonical proofs.

Using an appropriate equivalence class of proofs can be an effective
way of defining away the problem of inessential nondeterminism.  In
linear logic as presented above, if the permutability of rules like
${!}_L$ and ${\otimes}_L$ is problematic, we can instead reason about
{\it equivalence classes} of derivations where proofs that differ only
in the ordering of ${!}_L$ and ${\otimes}_L$ rules are treated as
equivalent (that is, as members of the same equivalence class):
\[
\infer[{!}_L]
{\seq{\Gamma}{\Delta, {!}A, B \otimes C}{D}}
{\infer[{\otimes}_L]
 {\seq{\Gamma,A}{\Delta, B \otimes C}{D}}
 {\deduce{\seq{\Gamma,A}{\Delta, B, C}{D}}{\mathcal D}}}
\quad
\deduce{\mathstrut}{\mathstrut{\equiv}}
\quad
\infer[{\otimes}_L]
{\seq{\Gamma}{\Delta, {!}A, B \otimes C}{D}}
{\infer[{!}_L]
 {\seq{\Gamma}{\Delta, {!}A, B, C}{D}}
 {\deduce{\seq{\Gamma,A}{\Delta, B, C}{D}}{\mathcal D}}}
\]

In LF, lambda calculus terms (which correspond to derivations by the
Curry-Howard) are considered modulo the least equivalence class that
includes
\begin{itemize}
\item $\alpha$-equivalence ($\lambda x.N \equiv \lambda y.N[y/x]$ if 
$y \not\in {\it FV}(N)$), 
\item $\beta$-equivalence 
($(\lambda x.\,M)N \equiv M[N/x]$ if $x \not\in {\it FV}(N)$), and 
\item $\eta$-equivalence ($N \equiv \lambda x.N\,x$).
\end{itemize}
The weak normalization property for LF establishes that given any
typed LF term, we can find an equivalent term that is $\beta$-normal
(no $\beta$-redexes of the form $(\lambda x.M) N$ exist) and
$\eta$-long (replacing $N$ with $\lambda x.N\,x$ anywhere would
introduce a $\beta$-redex or make the term ill-typed).  Furthermore,
in any given equivalence class of typed LF terms, all the
$\beta$-normal and $\eta$-long terms are $\alpha$-equivalent.
Therefore, because $\alpha$-equivalence is decidable, the equivalence
of typed LF terms is also decidable. 

The uniqueness of $\beta$-normal and $\eta$-long terms within an
equivalence class of lambda calculus terms (modulo
$\alpha$-equivalence, which we will henceforth take for granted) makes
these terms useful as canonical representatives of equivalence
classes. In Harper, Honsell, and Plotkin's original formulation
of LF, a deductive system was said to be {\it adequately encoded} as
an LF type family in the case that there is a compositional bijection
between the formal objects in the deductive system and these
$\beta$-normal, $\eta$-long representatives of equivalence classes
\cite{harper93framework}.

More modern presentations of LF, such as Harper and Licata's
\cite{harper07mechanizing}, follow the approach developed by Watkins
et al.~\cite{watkins02concurrent} and define the logical framework so
that it only contains these $\beta$-normal, $\eta$-long {\it canonical
  forms} of LF. This presentations of LF is called Canonical LF to
distinguish it from the original presentation of LF in which the
canonical forms are just a subset of the possible terms. A central
component in this approach is {\it hereditary substitution}.
Hereditary substitution also establishes a normalization property for
LF; using hereditary substitution we can easily take a regular LF
term and transform it into a Canonical LF term.\footnote{This process
  is the same as the way we use cut admissibility to prove cut
  elimination.} An oft-overlooked point, which we will return to in
Section~\ref{sec:warning}, is that the normalization theorem we prove
this way is a strictly weaker theorem than so-called weak
normalization.

One analogue to the canonical forms of LF will be the {\it focused
  derivations} of linear logic that are presented in the next
section. In Section~\ref{sec:foclinlog} below, we will present 
focused linear logic and see that there is exactly 
one focused derivation that derives the proposition
\[{!}({\sf 6bucks} \lolli {\sf battery}) \otimes {\sf 6bucks} \otimes
({\sf battery} \lolli {\sf robot}) \lolli {\sf robot}.\] 
%
We will furthermore see that the structure of this derivation matches
the intuitive transition interpretation of the proposition, a point
that is reinforced by the discussion of {\it synthetic inference
  rules} in Section~\ref{sec:linsynthetic}. In
Section~\ref{sec:linhack}, we argue that our focused system, while it
may be the most natural one for linear logic, does not precisely meet
the demands we will place upon it. This, in turn, motivates a
discussion of notation (Section~\ref{sec:linnote}) 
which we will continue in the next chapter.

\section{Focused linear logic}
\label{sec:foclinlog}

Andreoli's original motivation for introducing focusing was not to
describe a logical framework, it was to describe a paradigm of logic
programming based on proof search in classical linear logic
\cite{andreoli92logic}. The existence of multiple proofs that differ
in inessential ways is particularly problematic for proof search, as
inessential differences between derivations correspond to unnecessary
choice points that a proof search procedure will need to backtrack
over. The presentation of focusing for intuitionistic linear logic in
this section most closely resembles Chaudhuri's focused intuitionistic
linear logic \cite{chaudhuri06focused} and my presentation of
polarized intuitionistic persistent logic
\cite{simmons11structural}. The major exception is the treatment of
asynchronous rules as confluent rather than fixed and arbitrary
(discussed in Section~\ref{sec:confluent-v-fixed}).

\subsection{Polarization}
\label{sec:linpolar}

The first step in describing a focused sequent calculus is to classify
connectives into two groups.  Some connectives, such as linear
implication $A \lolli B$, are called {\it asynchronous} because their
right rules can always be applied eagerly, without backtracking,
during bottom-up proof search. Other connectives, such as disjunction
$A \tensor B$, are called {\it synchronous} because their right rules
cannot be applied eagerly. For instance, if we are trying to prove
$\seq{\Gamma}{A \tensor B}{B \tensor A}$, the ${\tensor}R$ rule cannot
be applied eagerly; we first have to decompose $A \tensor B$ on the
left using the ${\tensor}L$ rule.\footnote{Andreoli dealt with a
  one-sided classical sequent calculus; in intuitionistic logics, it
  is common to call asynchronous connectives {\it right}-asynchronous
  and {\it left}-synchronous. Similarly, it is common to call
  synchronous connectives {\it right}-synchronous and {\it
    left}-asynchronous.

  Synchronicity, a property of connectives, is closely connected to
  (and sometimes conflated with) a property of rules called {\it
    invertibility}; a rule is invertible if the conclusion of the rule
  implies the premises. So ${\lolli}R$ is invertible
  ($\seq{\Gamma}{\Delta}{A \lolli B}$ implies $\seq{\Gamma}{\Delta,
    A}{B}$) but ${\lolli}L$ is not ($\seq{\Gamma}{\Delta, A \lolli
    B}{C}$ does not imply that $\Delta = \Delta_1, \Delta_2$ such that
  $\seq{\Gamma}{\Delta_1}{A}$ and $\seq{\Gamma}{\Delta_2, B}{C}$).
  Rules that can be applied eagerly need to be invertible, so
  asynchronous connectives have invertible right rules and synchronous
  connectives have invertible left rules. Therefore, another synonym
  for asynchronous/negative is {\it right-invertible}, and another
  synonym for synchronous/positive is {\it left-invertible}.}  
The nontrivial result of focusing is that it is possible to separate a
proof into phases: inversion phases in which all asynchronous rules
are applied exhaustively, and focused phases where synchronous rules
are applied repeatedly and exhaustively to a single proposition (the
proposition {\it in focus}). 

We call the asynchronous connectives {\it negative} ($\lolli$, $\top$
and $\with$ in full propositional linear logic) and call the
synchronous connectives {\it positive} ($\zero$, $\oplus$, $\one$, and
$\otimes$ in full propositional linear logic). Each atomic proposition
must be assigned to be either positive or negative, though this
assignment can be arbitrary. At this point, there is an important
choice to make. One way forward is to treat positive and negative
propositions as a syntactic refinements of all propositions, in which
case we end up focusing a standard intuitionistic linear logic. The
other way forward is to treat positive and negative propositions as
distinct syntactic classes $A^+$ and $A^-$ with explicit inclusions
between them. In this second case, we end up focusing a {\it
  polarized} linear logic.  These inclusions are traditionally called
{\it shifts}. The positive proposition ${\downarrow}A^-$, pronounced
``downshift $A$,'' has a subterm that is a negative proposition; the
negative proposition ${\uparrow}A^+$, pronounced ``upshift $A$,'' has
a subterm that is a positive proposition.

The choice doesn't make a large difference for our purposes.
Polarized logics are interesting, and polarized linear logic is a bit
more expressive than regular linear logic, as heavily-shifted
propositions like ${\uparrow}{\downarrow}{\uparrow}{\downarrow}A^-$
can be expressed. This extra expressiveness won't help us in
the design of logical frameworks, but the use of shifts is helpful
when explaining identity expansion in Section~\ref{sec:linindentity}, 
so we will focus a polarized linear logic with shifts.

\input{figs/fig-lin-shift}

The relationship between unpolarized and polarized linear logic is
given by two erasure functions $(A^+)^\circ$ and $(A^-)^\circ$ that
wipe away all the shifts; this function is defined in
Figure~\ref{fig:lin-shift}. While shifts turn out to have a profound
impact on the structure of focused proofs, they are intended to have
no impact on provability. Therefore, the strongest statement of the
correctness of focusing is based on erasure: there is an unfocused
derivation of $(A^-)^\circ$ if and only if there is a focused
derivation of $A^-$.\footnote{I chose $A^-$ only to be brief; the
  condition that $(A^+)^\circ$ is derivable iff $A^+$ is could, of
  course, be added.}  However, most proofs of the correctness of
focusing prove a weaker property. Every proposition in linear logic
has an obvious polarized analogue with a minimal number of shifts;
this analogue is formalized as the two functions $A^\oplus$ and
$A^\ominus$ in Figure~\ref{fig:lin-shift}. Note that both of these
functions are partial inverses of erasure: $(A^\oplus)^\circ =
(A^\ominus)^\circ = A$. Almost all proofs of the correctness of
focusing work on the basis of these partial inverses, which we call
{\it polarization strategies}, establishing that there is an unfocused
proof of $A$ if and only if there is a focused proof of
$A^\ominus$.\footnote{The two exceptions are Zeilberger completeness
  proof in classical persistent logic \cite{zeilberger08unity} and my
  proof in intuitionistic persistent logic \cite{simmons11structural}.}
The weaker formulation is sufficient for our current purposes, so we
will discuss the weaker property of polarization-strategy-based
correctness, not erasure-based correctness.

\subsection{Sequent calculus}

Usually, focused logics are described as
having multiple judgments:
\begin{itemize}
\item $\mildrfoc{\Gamma}{\Delta}{A^+}$ (the {\it right focus} sequent, where
the proposition $A^+$ is in focus),
\item $\mildinv{\Gamma}{\Delta}{C}$ (the {\it inversion} sequent), and
\item $\mildlfoc{\Gamma}{\Delta}{A^-}{C}$ (the {\it left focus} sequent,
where the proposition $A^-$ is in focus).
\end{itemize}
Another reasonable presentation of linear logic uses only one sequent
$\mildseq{\Gamma}{\Delta}{U}$, but generalizes what is to allowed to
to appear in the linear context $\Delta$ or in the succeedant, which
we write $U$. We will use this interpretation to understand the logic
described in Figure~\ref{fig:kaustuv-focused}.

\input{figs/fig-kaustuv-focused.tex}

By adding a side condition to the three rules ${\it focus}_R$, ${\it
  focus}_L$, and ${\it copy}$ that neither the context $\Delta$ nor
the succeedant $U$ can contain an in-focus proposition $[A^+]$ or
$[A^-]$, derivations can maintain the invariant that there is always
at most one proposition in focus, effectively restoring the situation
in which there are three distinct judgments.  This restriction alone
gives us what Pfenning calls a {\it chaining} logic
\cite{pfenning02chaining} and which Laurent calls a {\it weakly
  focused} logic \cite{laurent04proof}.\footnote{This is not what I
  called a weakly focused logic \cite{simmons09weak}. That weakly
  focused system had an additional restriction that invertible rules
  could not be applied when any other proposition was in focus; this
  corresponded to what Laurent called a strongly $+$-focused logic.}
We obtain a fully focused logic by further restricting these three
rules so that they only apply when the sequent below the line is {\it
  stable}.  A sequent $\mildseq{\Gamma}{\Delta}{U}$ is stable if the
context $\Delta$ contains only negative propositions $A^-$ and
suspended positive propositions $\langle A^+ \rangle$ and the
succeedant $U$ is either a positive proposition $A^+$ or a suspended
negative proposition $\langle A^- \rangle$. 

We will now turn our attention to the meaning of these suspended
propositions and the four rules that interact with them: ${\it id}^+$,
${\it id}^-$, $\eta^+$, and $\eta^-$.

\subsection{Suspended propositions}

In unfocused sequent calculi, such as the one for linear logic in
Figure~\ref{fig:linear}, initial sequents are restricted to atomic
propositions. All sequent calculi, focused or unfocused, have the
subformula property: every rule breaks down a proposition, either on
the left or the right. Since the logical interpretation of atomic
propositions is that they are stand-ins for unknown propositions, we
are unable to break them down any further. We are therefore only able
to derive an atomic conclusion or use an atomic premise with the {\it
  init} rule that concludes $\seq{\Gamma}{p}{p}$ and has no premises.
This {\it init} rule is the only instance of the admissible identity
theorem $\seq{\Gamma}{A}{A}$ that must be explicitly included as a
proof rule. If we substitute in a proposition for an atomic
propositions, the structure of the proof stays exactly the same,
except that instances of initial sequents become admissible instances
of the general identity theorem.

To my knowledge, all published proof systems for focused logic have
attempted to replicate this initial rule {\it init}. This is a design
error, and it is one that has historically made it enormously (and
unnecessarily) difficult to prove the identity theorem for focused
systems. Our presentation uses {\it suspensions}: suspended positive
propositions $\langle A^+ \rangle$ only appear in the linear context
$\Delta$, and suspended negative propositions $\langle A^- \rangle$
only appear as succeedants. They treated as stable (we never break
down a suspended proposition) and are only used to immediately
prove a proposition in focus with one of the identity rules
${\it id}^+$ or ${\it id}^-$.

Suspended positive propositions act much like regular variables in a
natural deduction system. The positive identity rule ${\it id}^+$
allows us to prove any positive proposition given that positive
proposition appears suspended in the context.  There is a
corresponding substitution principle for focal substitutions that has
a natural-deduction-like flavor: we can substitute a derivation
right-focused on $A^+$ for a suspended positive proposition $\langle
A^+ \rangle$ in a context.

\bigskip
\begin{theorem}[Focal substitution (positive)]\label{thm:fsubst-pos}~\\
For stable $\Delta$,
if $\mildseq{\Gamma}{\Delta}{[A^+]}$ 
and $\mildseq{\Gamma}{\Delta', \langle A^+ \rangle}{U}$, 
then $\mildseq{\Gamma}{\Delta', \Delta}{U}$.
\end{theorem}

\begin{proof}
  Straightforward induction over the second given derivation, as in a
  proof of regular substitution in a natural deduction system. If the
  second derivation is the axiom ${\it id}^+$, the result follows
  immediately using the first given derivation.
\end{proof}

\noindent
Note that, in the statement of Theorem~\ref{thm:fsubst-pos}, the
second premise $\mildseq{\Gamma}{\Delta', \langle A^+ \rangle}{U}$ may
be a right-focused sequent $\mildseq{\Gamma}{\Delta', \langle A^+
  \rangle}{[B^+]}$, a left-focused sequent $\mildseq{\Gamma}{\Delta'',
  [B^-], \langle A^+ \rangle}{U}$, or an inverting sequent. 

Suspended negative propositions are a bit weirder. While a derivation
of $\mildseq{\Gamma}{\Delta', \langle A^+ \rangle}{U}$ is missing a
premise that can be satisfied by a derivation of
$\mildseq{\Gamma}{\Delta}{[A^+]}$, a derivation of 
$\mildseq{\Gamma}{\Delta}{\langle A^- \rangle}$ is missing a 
{\it continuation} that can be satisfied by a derivation of
$\mildseq{\Gamma}{\Delta', [A^-]}{U}$. The focal substitution principle,
however, still takes the basic form of a substitution principle.

\bigskip
\begin{theorem}[Focal substitution (negative)]\label{thm:fsubst-neg}~\\
For stable $\Delta'$ and $U$, 
if $\mildseq{\Gamma}{\Delta}{\langle A^- \rangle}$
and $\mildseq{\Gamma}{\Delta', [A^-]}{U}$, 
then $\mildseq{\Gamma}{\Delta', \Delta}{U}$. 
\end{theorem}

\begin{proof}
  Straightforward induction over the {\it first} given derivation; if
  the first derivation is the axiom ${\it id}^-$, the result follows
  immediately using the second given derivation.
\end{proof}

\noindent
As a regular substitution principle that is inductive over the structure
of the first given proposition, focal substitution is reminiscent of 
the {\it leftist substitutions} introduced by Pfenning and Davies in the 
context of the possibility modality \cite{pfenning01judgmental}.

Unlike cut admissibility, which we discuss in Section~\ref{sec:lincut}, both
of the focal substitution principles are straightforward inductions
over the structure of the derivation containing the suspended
proposition. In the development of structural focalization, I discuss
how, in a focused presentation of persistent intuitionistic logic that
is encoded in LF, a suspended positive premise can be encoded as a
hypothetical right focus. This encoding makes the ${\it id}^+$ rule an
instance of the hypothesis rule provided by LF and establishes
Theorem~\ref{thm:fsubst-pos} ``for free'' as an instance of LF
substitution. This is possible to do for negative focal substitution
as well, but it is somewhat counter-intuitive
\cite{simmons11structural}.

The two substitution
principles can be phrased as admissible rules for building derivations,
which we indicate using a dashed line:
\[
\infer-[{\it subst}^+]
{\mildseq{\Gamma}{\Delta', \Delta}{U}}
{\mildseq{\Gamma}{\Delta}{[A^+]}
 &
 \mildseq{\Gamma}{\Delta', \langle A^+ \rangle}{U}}
\qquad
\infer-[{\it subst}^-]
{\mildseq{\Gamma}{\Delta', \Delta}{U}}
{\mildseq{\Gamma}{\Delta}{\langle A^- \rangle}
 &
 \mildseq{\Gamma}{\Delta', [A^-]}{U}}
\]

\subsection{Identity expansions}
\label{sec:linindentity}

Suspended propositions appear in Figure~\ref{fig:kaustuv-focused} in
two places: first in the identity rules that we have just discussed
and connected with the focal substitution principles, and second in
the rules marked $\eta^+$ and $\eta^-$, which are also the only
mention of atomic propositions in the presentation. It is here that we
need to make an absolutely critical shift of perspective from
unfocused to focused logic. In an unfocused logic, the rules
nondeterministically break down propositions, and the initial rule {\it
  init} puts an end to this process when an atomic proposition is
reached. In a focused logic, the focus and inversion phases {\it must}
break down a proposition all the way until a shift is reached. The two
$\eta$ rules are what put an end to this when an atomic proposition is
reached, and they correspond to the two ${\it id}$ rules that allow
these necessarily suspended propositions to successfully conclude a
right or left focus.

\input{figs/fig-lineta-1}
\input{figs/fig-lineta-2}

Just as the {\it init} rule is a particular instance of the admissible
identity sequent $\seq{\Gamma}{A}{A}$ in unfocused linear logic, the
atomic suspension rules $\eta^+$ and $\eta^-$ are instances of an admissible
identity expansion rule in focused linear logic:
\[
\infer-[\eta^+]
{\mildseq{\Gamma}{\Delta, A^+}{U}}
{\mildseq{\Gamma}{\Delta, \langle A^+ \rangle}{U}}
\qquad
\infer-[\eta^-]
{\mildseq{\Gamma}{\Delta}{A^-}}
{\mildseq{\Gamma}{\Delta}{\langle A^- \rangle}}
\]
This admissible rule must be established by mutual recursion; the
proof is structurally inductive on the structure of the proposition,
and uses focal substitution in a critical way. Most of the cases of
this proof are represented in Figure~\ref{fig:lineta-1}. (Note that in
this figure we omit polarity annotations from propositions as they are
always clear from the context.) The remaining case (for the
multiplicative unit $\one$) is presented in Figure~\ref{fig:lineta-2}
along with the cases for the additive connectives $\zero$, $\oplus$,
$\top$, and $\with$, which are neglected elsewhere in this chapter.

The admissible identity expansion rules fit with an interpretation of
positive atomic propositions as stand-ins for arbitrary positive
propositions and of negative atomic propositions as stand-ins for
negative atomic propositions: if we substitute a proposition in for
some atomic proposition, all the instances of atomic suspension
corresponding to that rule become admissible instances of identity
expansion. Furthermore, the usual identity principles are simple
corollaries of identity expansion:
\[
\infer-[\eta^+]
{\mildseq{\Gamma}{A^+}{A^+}}
{\infer[{\it focus}_R]
 {\mildseq{\Gamma}{\langle A^+ \rangle}{A^+}}
 {\infer[{\it id}^+]
  {\mildseq{\Gamma}{\langle A^+ \rangle}{[A^+]}}
  {}}}
\qquad
\infer-[\eta^-]
{\mildseq{\Gamma}{A^-}{A^-}}
{\infer[{\it focus}_L]
 {\mildseq{\Gamma}{A^-}{\langle A^- \rangle}}
 {\infer[{\it id}^-]
  {\mildseq{\Gamma}{[A^-]}{\langle A^- \rangle}}
  {}}}
\]

\subsection{Cut admissibility}
\label{sec:lincut}

Theorem~\ref{thm:lincut} mostly follows the well-worn contours of a
structural cut admissibility argument \cite{pfenning00structural}, so
we defer a full discussion of cut admissibility until the next
chapter, where the use of structural focalization will allow us to
give a tidier proof of the theorem.\footnote{The main ``untidy''
  aspect of Theorem~\ref{thm:lincut} is that the lack of a forced
  inversion order means that the right commutative cases dealing with
  invertible rules must be repeated in parts 1 and 3, and likewise for
  the left commutative cases dealing with invertible rules and parts 2
  and 4. The repetition of {\it all} cases between parts 3 and 5 will
  also be addressed in the next chapter.}
%
The only important caveat to emphasize about Theorem~\ref{thm:lincut}
is that cut admissibility is only applicable in the absence of any
non-atomic suspended propositions. If we did not make this
restriction, then in Theorem~\ref{thm:lincut}, part 1, we might encounter
a derivation of $\mildseq{\Gamma}{\langle A \tensor B \rangle}{[ A \tensor B ]}$
being cut into the derivation
\[
\infer[{\otimes}_R]
{\mildseq{\Gamma}{\Delta',A \tensor B}{U}}
{\deduce{\mildseq{\Gamma}{\Delta', A, B}{U}}{\mathcal E}}
\]
in which case there is no clear way to proceed and prove 
$\mildseq{\Gamma}{\Delta', \langle A \tensor B \rangle}{U}$. 

\bigskip
\begin{theorem}[Cut admissibility]\label{thm:lincut}
For all $\Gamma$, $A^+$, $A^-$, $\Delta$, $\Delta'$, and $U$ that
do not contain any non-atomic suspended propositions:
\begin{enumerate}
\item If $\mildseq{\Gamma}{\Delta}{[A^+]}$
      and $\mildseq{\Gamma}{\Delta',A^+}{U}$
      (where $\Delta$ is stable), 
      then $\mildseq{\Gamma}{\Delta',\Delta}{U}$.
\item If $\mildseq{\Gamma}{\Delta}{A^-}$
      and $\mildseq{\Gamma}{\Delta', [A^-]}{U}$
      (where $\Delta$, $\Delta'$, and $U$ are stable),
      then $\mildseq{\Gamma}{\Delta',\Delta}{U}$. 
\item If $\mildseq{\Gamma}{\Delta}{A^-}$
      and $\mildseq{\Gamma}{\Delta', A^-}{U}$
      (where $\Delta$ is stable), 
      then $\mildseq{\Gamma}{\Delta',\Delta}{U}$. 
\item If $\mildseq{\Gamma}{\Delta}{A^+}$
      and $\mildseq{\Gamma}{\Delta', A^+}{U}$
      (where $\Delta'$ and $U$ are stable),
      then $\mildseq{\Gamma}{\Delta',\Delta}{U}$. 
\item If $\mildseq{\Gamma}{\cdot}{A^-}$
      and $\mildseq{\Gamma, A^-}{\Delta}{U}$,
      then $\mildseq{\Gamma}{\Delta}{U}$. 
\end{enumerate}
\end{theorem}

\begin{proof}
By lexicographic induction. In each invocation of the induction
hypothesis, either the principal cut formula $A^+$ or $A^-$ gets 
smaller, or else it stays the same and the number of the part
gets smaller (as when we invoke part 2 when proving part 5). 

Within parts 3 and 5, the first two metrics stay the same while the
second given derivation gets smaller, and within part 4, the first two
metrics stay the same while the first given derivation gets smaller.
\end{proof}

\subsection{Correctness of focusing}

Now we will make precise the correctness for a focused, polarized logic
that was discussed in Section~\ref{sec:linpolar}: that there is a unfocused
proof of $A$ if and only if there is a focused proof of $A^\oplus$. The
proof require lifting of our erasure function and our ``obvious'' 
polarization strategy to contexts and succeedents, which is done
in Figure~\ref{fig:lin-shift-ctx}. Soundness is established on the 
basis of erasure as in \cite{simmons11structural}, but as discussed
we give the slightly simpler polarization-strategy-based proof of
completeness.

\bigskip
\begin{theorem}[Soundness of focusing]
If $\mildseq{\Gamma}{\Delta}{U}$, where $\Delta$ and $U$ contain only
atomic suspensions, then $\seq{\Gamma^\circ}{\Delta^\circ}{U^\circ}$.
\end{theorem}

\begin{proof}
  By straightforward induction on the given derivation; in each case,
  the result either follows directly by invoking the induction
  hypothesis or by invoking the induction hypothesis and applying one
  rule from Figure~\ref{fig:linear}.
\end{proof}

\begin{theorem}[Completeness of focusing]
If $\seq{\Gamma}{\Delta}{C}$,
then $\mildseq{\Gamma^\bullet}{\Delta^\bullet}{C^\bullet}$. 
\end{theorem}

\begin{proof}
  By induction on the first given derivation. Each rule in 
  Figure~\ref{fig:linear}

  Rule {\it copy}:
  \[
  \]

  Rule $\lolli_L$:
  \[
  \]

  Rule $\lolli_R$:
  {\small \[
  \infer-[{\it cut}(3)]
  {\mildseq{\Gamma^\bullet}{\Delta^\bullet}
    {{\downarrow}(A^\oplus \lolli B^\ominus)}}
  {\infer[{\lolli}_R]
   {\mildseq{\Gamma^\bullet}{\Delta^\bullet}
     {{\downarrow}{\uparrow}A^\oplus \lolli {\uparrow}{\downarrow}B^\ominus}}  
   {\infer[{\downarrow}_L]
    {\mildseq{\Gamma^\bullet}{\Delta^\bullet, {\downarrow}{\uparrow}A^\oplus}
      {{\uparrow}{\downarrow}B^\ominus}}
    {\infer[{\uparrow}_R]
     {\mildseq{\Gamma^\bullet}{\Delta^\bullet, {\uparrow}A^\oplus}
      {{\uparrow}{\downarrow}B^\ominus}}
     {\mildseq{\Gamma^\bullet}{\Delta^\bullet, {\uparrow}A^\oplus}
      {{\downarrow}B^\ominus}}}}
   &
   \infer[{\it focus}_R]
   {\mildseq{\Gamma}{{\downarrow}{\uparrow}A^\oplus \multimap B^\ominus}
      {{\downarrow}(A^\oplus \lolli B^\ominus)}}
   {\infer[{\downarrow}_R]
    {\mildseq{\Gamma^\bullet}
      {{\downarrow}{\uparrow}A^\oplus \multimap {\uparrow}{\downarrow}B^\ominus}
      {[ {\downarrow}(A^\oplus \lolli B^\ominus)} ]}
    {\infer[{\lolli}_R]
     {\mildseq{\Gamma^\bullet}
       {{\downarrow}{\uparrow}A^\oplus \multimap {\uparrow}{\downarrow}B^\ominus}
       {A^\oplus \lolli B^\ominus}}
     {\infer-[\eta^+]
      {\mildseq{\Gamma^\bullet}
        {{\downarrow}{\uparrow}A^\oplus 
           \multimap {\uparrow}{\downarrow}B^\ominus, 
         A^\oplus}
        {B^\ominus}}
      {\infer-[\eta^-]
       {\mildseq{\Gamma^\bullet}
         {{\downarrow}{\uparrow}A^\oplus
            \multimap {\uparrow}{\downarrow}B^\ominus, 
          \langle A^\oplus \rangle}
         {B^\ominus}}
       {\infer[{\it focus}_L]
        {\mildseq{\Gamma^\bullet}
          {{\downarrow}{\uparrow}A^\oplus 
             \multimap {\uparrow}{\downarrow}B^\ominus, 
           \langle A^\oplus \rangle}
          {\langle B^\ominus \rangle}}
        {\infer[{\lolli}_L]
         {\mildseq{\Gamma^\bullet}
           {[ {\downarrow}{\uparrow}A^\oplus 
              \multimap {\uparrow}{\downarrow}B^\ominus ], 
            \langle A^\oplus \rangle}
           {\langle B^\ominus \rangle}}
         {\infer[{\downarrow}_R]
          {\mildseq{\Gamma^\bullet}{\langle A^\oplus \rangle}
           {[{\downarrow}{\uparrow}A^\oplus]}}
          {\infer[{\uparrow}_R]
           {\mildseq{\Gamma^\bullet}{\langle A^+ \rangle}{{\uparrow}A^+}}
           {\infer[{\it focus}_R]
            {\mildseq{\Gamma^\bullet}{\langle A^+ \rangle}{A^+}} 
            {\infer[{\it id}^+]
             {\mildseq{\Gamma^\bullet}{\langle A^+ \rangle}{[A^+]}}
             {}}}}
          &
          \infer[{\uparrow}_L]
          {\mildseq{\Gamma^\bullet}{[{\uparrow}{\downarrow}B^\ominus]}
             {\langle B^\ominus \rangle}}
          {\infer[{\downarrow}_L]
           {\mildseq{\Gamma^\bullet}{{\downarrow}B^\ominus}
             {\langle B^\ominus \rangle}}
           {\infer[{\it focus}_L]
            {\mildseq{\Gamma^\bullet}{B^\ominus}{\langle B^\ominus \rangle}}
            {\infer[{\it id}^-]
             {\mildseq{\Gamma^\bullet}{[B^\ominus]}{\langle B^\ominus \rangle}}
             {}}}}}}}}}}}}
  \]}
\end{proof}

The correctness of focusing is established as in the structural focalization
methadology, except that we establish a weaker polarization-strategy-based
proof of completeness rather than an erasure-based proof as in 
\cite{simmons11structural}. Both the soundness and completeness of focusing


\input{figs/fig-lin-shift-ctx}



\begin{proof}

The reverse direction is the soundness of focusing. It is completely
straightforward, 
\end{proof}

\subsection{Confluent versus fixed inversion}
\label{sec:confluent-v-fixed}

\subsection{Running example}

\begin{figure}
\[
\infer[{\lolli}_R]
{\mildseq{\cdot}{\cdot}{{!}({\sf 6bucks} \lolli {\sf battery}) \otimes
                    {\sf 6bucks} \otimes 
                    ({\sf battery} \lolli {\sf robot}) \lolli {\sf robot}}}
{\infer[{\otimes}_L]
{\mildseq{\cdot}{{!}({\sf 6bucks} \lolli {\sf battery}) \otimes
                    {\sf 6bucks} \otimes 
                    ({\sf battery} \lolli {\sf robot})}{{\sf robot}}}
{\infer[{!}_L]
{\mildseq{\cdot}{{!}({\sf 6bucks} \lolli {\sf battery}),
                    {\sf 6bucks} \otimes 
                    ({\sf battery} \lolli {\sf robot})}{{\sf robot}}}
{\infer[{\otimes}_L]
{\mildseq{\Gamma}{{\sf 6bucks} \otimes 
                    ({\sf battery} \lolli {\sf robot})}{{\sf robot}}}
{\infer[{\it copy}]
{\mildseq{\Gamma}{{\sf 6bucks}, {\sf battery} \lolli {\sf robot}}{{\sf robot}}}
{\infer[{\lolli}_L]
{\mildseq{\Gamma}{{\sf 6bucks}, {\sf battery} \lolli {\sf robot}, [{\sf 6bucks} \lolli {\sf battery}]}{{\sf robot}}}
{\infer[{\it init}^+]
 {\mildseq{\Gamma}{{\sf 6bucks}}{[{\sf 6bucks}]}}
 {}
 &
 \infer[{\it blur}_L]
 {\mildseq{\Gamma}{{\sf battery} \lolli {\sf robot}, [{\sf battery}]}{{\sf robot}}}
 {\infer[{\it focus}_L]
 {\mildseq{\Gamma}{{\sf battery} \lolli {\sf robot}, {\sf battery}}{{\sf robot}}}
 {\infer[{\lolli}_L]
 {\mildseq{\Gamma}{{\sf battery}, [{\sf battery} \lolli {\sf robot}]}{{\sf robot}}}
 {\infer[{\it init^+}]
  {\mildseq{\Gamma}{{\sf battery}}{[{\sf battery}]}}
  {}
  &
  \infer[{\it blur}_L]
  {\mildseq{\Gamma}{[{\sf robot}]}{{\sf robot}}}
  {\infer[{\it focus}_R]
  {\mildseq{\Gamma}{{\sf robot}}{{\sf robot}}}
  {\infer[{\it init}^+]
  {\mildseq{\Gamma}{{\sf robot}}{[{\sf robot}]}}
  {}}}}}}}}}}}}
\] 
\caption{The single focused transition is possible 
(where we let $\Gamma = {\sf 6bucks} \lolli {\sf battery}$).}
\label{fig:focused-robot}
\end{figure}

Figure

\section{Synthetic inference rules}
\label{sec:linsynthetic}

\section{Hacking the focusing system}
\label{sec:linhack}

\subsection{Atom optimization}

\subsection{Bang optimization}

\subsection{A more primitive logic?}

\paragraph{Adjoint logic}

\paragraph{Tensor logic}

\subsection{Concurrent equality}

\section{Revisiting our notation}
\label{sec:linnote}

\section{A warning about normalization}
\label{sec:warning}

Talk about equivalence, Chris's unpublished work, and where
the focalization theorem given by this approach is deficient -- 