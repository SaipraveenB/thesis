
\chapter{Linear logic}

Logic as it has been traditionally understood and studied -- both in
its classical and intuitionistic varieties -- treats the truth of a
proposition as a {\it persistent resource}. That is, if we have
evidence for the truth of a proposition, we can ignore that evidence
if it is not needed and reuse the evidence as many times as we need
to. Throughout this thesis, ``logic as it has been traditionally
understood as studied'' will be referred to as {\it persistent} logic
to emphasize this treatment of evidence. 

Linear logic, which was popularized by Girard \cite{girard87linear},
treats evidence as an {\it ephemeral} resource; the use of an
ephemeral resource consumes it, at which point it is unavailable for
further use.  Linear logic, like persistent logic, comes in classical
and intuitionistic flavors. We will favor intuitionistic linear logic
in part because the propositions of intuitionistic linear logic
(written $A$, $B$, $C$, \ldots) have a more natural correspondence
with our physical intuitions about consumable resources. Linear
conjunction $A \tensor B$ represents the resource built from the
resources $A$ and $B$; if you have both a bowl of soup {\it and} a
sandwich, that resource can be represented by the proposition ${\sf
  soup} \otimes {\sf sandwich}$. Linear implication $A \lolli B$
represents a resource that can interact with another resource $A$ to
produce a resource $B$. One robot with batteries not included could be
represented as the linear resource $({\sf battery} \lolli {\sf
  robot})$, and the linear resource $({\sf 6bucks} \lolli {\sf soup}
\tensor {\sf sandwich})$ represents the ability to use \$6 to obtain
lunch -- but only once!\footnote{Conjunction will always bind more
  tightly than implication, so this is equivalent to the proposition
  ${\sf 6bucks} \lolli ({\sf soup} \tensor {\sf sandwich})$.} Linear
logic also has a modality ${!}A$ representing a resource that can be
used to generate any number of $A$ resources, including zero. The
Panera ``You Pick Two'' menu might be represented as
\[ {!}({\sf 6bucks} \lolli {\sf soup} \tensor {\sf sandwich}) \otimes
{!}({\sf 6bucks} \lolli {\sf soup} \tensor {\sf salad}) \otimes
{!}({\sf 6bucks} \lolli {\sf sandwich} \tensor {\sf salad}),\] as the
menu gives you the opportunity to exchange six dollars for two
distinct members of the set $\{ {\sf soup}, {\sf salad}, {\sf
  sandwich} \}$ any number of times.

\input{figs/fig-linear.tex}

Figure~\ref{fig:linear} presents a sequent calculus for linear logic,
in particular the so-called {\it multiplicative, exponential} fragment
of intuitionistic linear logic (or {\it MELL}). There is one quirk to
this presentation: I've presented two identical rules for atomic
propositions, $p^+$ and $p^-$, and given two identitial rules for
using them, ${\it init}^+$ and ${\it init}^-$. Other than that,
Figure~\ref{fig:linear} is a standard presentation. It corresponds
closely to Andreoli's dyadic system \cite{andreoli92logic}, Barber's
dual intuitionistic linear logic \cite{barber96dual}, and Chang et
al.'s judgmental analysis of intuitionistic linear logic
\cite{chang03judgmental}.

\subsection*{Transitions in linear logic}

The propositions of intuitionstic linear logic, and linear implication
in particular, capture a notion of state change: we can {\it
  transition} from a state where we have both a ${\sf battery}$ and
the battery-less robot (represented, as before, by the linear
implication ${\sf battery} \lolli {\sf robot}$) to a state where we
have the battery-endowed (and therefore presumably functional) robot
(represented by the proposition ${\sf robot}$). In other words, the
proposition
%
\[{\sf battery} \otimes ({\sf battery} \lolli {\sf robot}) \lolli
{\sf robot}\] 
%
is provable in linear logic. These transitions can be chained
together as well: if we start out with ${\sf
  6bucks}$ instead of ${\sf battery}$ but we also have the
persistent ability to turn ${\sf 6bucks}$ into a ${\sf battery}$ --
just like we turned \$6 into a bowl of soup and a salad at Panera --
then we can ultimately get our working robot as well. This boils down
to the statement that it is possible to prove
\[{!}({\sf 6bucks} \lolli {\sf battery}) \otimes {\sf 6bucks} \otimes
({\sf battery} \lolli {\sf robot}) \lolli {\sf robot}\] in linear
logic; a derivation of this proposition is given in
Figure~\ref{fig:unfocused-robot}.\footnote{In Chapter XXX, I will
  argue that this view isn't quite precise enough, and that the most
  natural representation of state change from the state $A$ to the
  state $B$ isn't really captured by derivations of the proposition $A
  \lolli B$ or by derivations of the hypothetical judgment
  $\seq{\cdot}{A}{B}$.  However, this view remains a simple and useful
  one; Cervesato and Scedrov cover it thoroughly in the context of
  intuitionstic linear logic \cite{cervesato09relating}.}  (Note that
this figure imples that I've assigned all relevant atomic propsitions
in this discussion to the $p^+$ category rather than the $p^-$
category. That was not an arbitrary choice, but it's not a relevant
choice yet.)

\begin{figure}
\[
\infer[{\lolli}_R]
{\seq{\cdot}{\cdot}{{!}({\sf 6bucks} \lolli {\sf battery}) \otimes
                    {\sf 6bucks} \otimes 
                    ({\sf battery} \lolli {\sf robot}) \lolli {\sf robot}}}
{\infer[{\otimes}_L]
{\seq{\cdot}{{!}({\sf 6bucks} \lolli {\sf battery}) \otimes
                    {\sf 6bucks} \otimes 
                    ({\sf battery} \lolli {\sf robot})}{{\sf robot}}}
{\infer[{!}_L]
{\seq{\cdot}{{!}({\sf 6bucks} \lolli {\sf battery}),
                    {\sf 6bucks} \otimes 
                    ({\sf battery} \lolli {\sf robot})}{{\sf robot}}}
{\infer[{\otimes}_L]
{\seq{\Gamma}{{\sf 6bucks} \otimes 
                    ({\sf battery} \lolli {\sf robot})}{{\sf robot}}}
{\infer[{\lolli}_L]
{\seq{\Gamma}{{\sf 6bucks}, {\sf battery} \lolli {\sf robot}}{{\sf robot}}}
{\infer[{\it copy}]
 {\seq{\Gamma}{{\sf 6bucks}}{{\sf battery}}}
 {\infer[{\lolli}_L] 
  {\seq{\Gamma}{{\sf 6bucks}, {\sf 6bucks} \lolli {\sf battery}}{{\sf battery}}}
  {\infer[{\it init}^+]
   {\seq{\Gamma}{{\sf 6bucks}}{{\sf 6bucks}}}
   {}
   &
   \infer[{\it init}^+]
   {\seq{\Gamma}{{\sf battery}}{{\sf battery}}}
   {}}}
 &
 \infer[{\it init}^+]
 {\seq{\Gamma}{{\sf robot}}{{\sf robot}}}
 {}}}}}}
\] 
\caption{Proving that a transition is possible 
(where we let $\Gamma = {\sf 6bucks} \lolli {\sf battery}$).}
\label{fig:unfocused-robot}
\end{figure}

It is precisely because linear logic contains this natural notion of
state and state transition that a rich line of work, dating back to
Chirmar's 1995 Ph.D. thesis, has sought to use linear logic as a {\it
  logical framework} for describing stateful systems
\cite{chirimar95proof,cervesato02linear,
  cervesato02concurrent,pfenning04substructural,miller09formalizing,
  pfenning09substructural,cervesato09relating}.

\subsection*{Linear logical frameworks}

While linear logic has many uses in computer science, I'll be
exclusively concentrating on its use as a logical framework for
describing stateful systems. Logical frameworks use the {\it
  structure} of proofs in a logic to describe the structure we're
really interested in. Linear logic as described in Figure~\ref{fig:linear}
has too much flexibility to be useful as a logical framework. This can be
seen in 
Figure~\ref{fig:unfocused-robot}, which doesn't really match the intuitive
structure of the transitions we skeched out at all:
\[
\begin{array}{ccccc}
\begin{array}{c}
\mbox{\it \$6 (1)}\medskip\\ 
\mbox{\it battery-free robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}
\end{array}
& \leadsto &
\begin{array}{c}
\mbox{\it battery  (1)}\medskip\\ 
\mbox{\it battery-free robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}
\end{array}
& \leadsto &
\begin{array}{c}
\mbox{\it robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}\medskip\\~\\
\end{array}
\end{array}
\]
A different (but related) criticism are that there are {\it lots} of
derivations of our example proposition according to the rules in
Figure~\ref{fig:linear}, even though there's only one ``real'' series
of transitions that get us to a working robot. The use of ${!}L$, for
instance, could be permuted up past the ${\otimes}L$ and then past the
${\lolli}L$ (the latter move would cause ${!}L$ to appear in both
branches of the proof). These differences are completely inessential
nondeterminism in proof construction or in proof search -- they just
get in the way of the structure that we are trying to capture.

To recap: for propositions $A$ and $B$ of the appropriate form, we can
create a derivation of $A \lolli B$ if and only if there
is a sequence of transitions that take us from a state described by 
$A$ to the state described by $B$. However, there may be {\it lots}
of derivations even if there is just one transition, and many of 



We {\it do} have the property that some derivations exist if and only if
a sequence of transitions exists. 

When there are lots of derivations that correspond to the same formal
artifact, a natural solution is to form equivalence classes of derivations
in such a way that there is a one-to-one correspondence between 
the formal artifacts and the equivalence classes of derivations.

see if all the derivations that
correspond to the same formal artifact in an equivalence class.  This
was the approach pioneered by Harper, Honsell, and Plotkin in the
logical framework LF \cite{harper93framework}. Derivations in LF are
represented as typed lambda calculus terms, and two derivations/lambda
calculus terms are in the same equivalence class if they normalize to
the same $\beta$-normal, $\eta$-long terms. These $\beta$-normal,
$\eta$-long terms are called the {\it canonical forms} of LF. In
practice, it turns out that many deductive systems (like logics and
programming languages) can be put into a compositional bijection with
the canonical forms of some LF type; we say that such a deductive
system is {\it adequately encoded} in LF.

One analouge to the canonical forms of LF will be the {\it focused
  derivations} of linear logic that are presented in the next
section. We will see that there is only
one focused derivation of 
\[{!}({\sf 6bucks} \lolli {\sf battery}) \otimes {\sf 6bucks} \otimes
({\sf battery} \lolli {\sf robot}) \lolli {\sf robot}\] and therefore
the focused derivations of linear logic are our first candidate for
adequateny encoding transition systems. In this chapter, we will
survey related work on focusing intuitionstic logic, starting from
Chaudhuri's formulation \cite{chaudhuri06focused}, and issues with
each of these formulations.

\section{Focused logic and synthetic inference rules}

Andreoli's original motivation for introducing focusing was not to
describe a logical framework, it was to describe a logic programming
language based on proof search in classical linear logic
\cite{andreoli92logic}. The existance of multiple proofs that differ
in inessential ways is particularly problematic for proof search, as
inessential differences between derivations correspond to unnecessary
choice points that a proof search procedure will need to backtrack
over. 

The first step in describing a focused sequent calculus is to classify
connectives into two groups.  Some connectives, such as linear
implication $A \lolli B$, are called {\it asynchronous} because their
right rules can always be applied eagerly, without backtracking,
during bottom-up proof search. Other connectives, such as disjunction
$A \tensor B$, are called {\it synchronous} because their right rules
cannot be applied eagerly. For instance, if we are trying to prove
$\seq{\Gamma}{A \tensor B}{B \tensor A}$, the ${\tensor}R$ rule cannot
be applied eagerly; we first have to decompose $A \tensor B$ on the
left using the ${\tensor}L$ rule.\footnote{Andreoli dealt with a
  one-sided classical sequent calculus; in intuitionistic logics, it
  is common to call asynchronous connectives {\it right}-asynchronous
  and {\it left}-synchronous. Similarly, it is common to call
  synchronous connectives {\it right}-synchronous and {\it
    left}-asynchronous.

  Synchronicity, a property of connectives, is closely connected to
  (and sometimes conflated with) a property of rules called {\it
    invertibility}; a rule is invertible if the conclusion of the rule
  implies the premises. So ${\lolli}R$ is invertible
  ($\seq{\Gamma}{\Delta}{A \lolli B}$ implies $\seq{\Gamma}{\Delta,
    A}{B}$) but ${\lolli}L$ is not ($\seq{\Gamma}{\Delta, A \lolli
    B}{C}$ does not imply that $\Delta = \Delta_1, \Delta_2$ such that
  $\seq{\Gamma}{\Delta_1}{A}$ and $\seq{\Gamma}{\Delta_2, B}{C}$).
  Rules that can be applied eagerly need to be invertible, so
  asynchronous connectives have invertible right rules and synchronous
  connectives have invertible left rules. Therefore, another synonym
  for asynchronous/negative is {\it right-invertible}, and another
  synonym for synchronous/positive is {\it left-invertible}.}  We call
the asynchronous connectives {\it negative} and write them as $A^-$,
and call the synchronous connectives {\it positive} and write them as
$A^+$. In the fragment of linear logic presented earlier, the
propositions end up classified like this:
\begin{align*}
A^+ & ::= p^+ \mid {!}A \mid \one \mid A \otimes B\\
A^- & ::= p^- \mid A \lolli B
\end{align*}
Note that our previously indistinguishable atomic propositions $p^+$
and $p^-$ are now distinguishable: $p^+$ is a positive atomic proposition and 
can be treated as a stand-in for an arbitrary positive propositions,
whereas $p^-$ is a negative atomic proposition and can be treated 
as a stand-in for an arbitrary negative propositions.

We next need some extra proof-theoretic machinery that is 
aware of proofs in order to put
propositions {\it in focus} (a proposition in focus will be written as
$[A]$). This extra machinery will then be used to enforce that right
rules can only be applied to positive propositions when they are in
focus, and likewise for negative propositions and left rules.  This
extra machinery is usually described by presenting multiple judgments.
The number differes, but at least three are necessary:

\begin{itemize}
\item $\mildrfoc{\Gamma}{\Delta}{A}$ (the {\it right focus} sequent),
\item $\mildinv{\Gamma}{\Delta}{C}$ (the {\it inversion} sequent), and
\item $\mildlfoc{\Gamma}{\Delta}{A}{C}$ (the {\it left focus} sequent).
\end{itemize}

\input{figs/fig-kaustuv-focused.tex}


Another reasonable presentation of linear logic uses only one sequent
$\mildseq{\Gamma}{\Delta}{U}$, but generalizes what is to allowed to
to appear in the linear context $\Delta$ or as the succeedent, which
we write $U$. The three focusing rules
${\it focus}_R$, ${\it focus}_L$, and ${\it copy}$ carry the extra condition
that the conclusion must be stable: the context $\Delta$ can only contain 
negative propositions $A^-$ and suspended positive propositions 
$\langle A^+ \rangle$,.

\begin{figure}[f]
{\small 
\[
\infer-[\eta^+]
{\mildseq{\Gamma}{\Delta, {\downarrow}A}{U}}
{\deduce
 {\mildseq{\Gamma}{\Delta, \langle {\downarrow}A \rangle}{U}}
 {\mathcal D}}
\quad
\deduce{\mathstrut}{\Longrightarrow}
\quad
\infer[{\downarrow}_L]
{\mildseq{\Gamma}{\Delta, {\downarrow}A}{U}}
{\infer-[{\it subst}^+]
 {\mildseq{\Gamma}{\Delta, A}{U}}
 {\infer[{\downarrow}_R]
  {\mildseq{\Gamma}{A}{[ {\downarrow}A ]}}
  {\infer-[{\eta}^-]
   {\mildseq{\Gamma}{A}{A}}
   {\infer-[{\it focus}_L]
    {\mildseq{\Gamma}{A}{\langle A \rangle}}
    {\infer[{\it id}^-]
     {\mildseq{\Gamma}{[ A ]}{\langle A \rangle}}
     {}}}}
  &
  \deduce
  {\mildseq{\Gamma}{\Delta, \langle {\downarrow}A \rangle}{U}}
  {\mathcal D}}}
\]

\[
\infer-[\eta^+]
{\mildseq{\Gamma}{\Delta,{!}A}{U}}
{\deduce
 {\mildseq{\Gamma}{\Delta, \langle {!}A \rangle}{U}}
 {\mathcal D}}
\quad
\deduce{\mathstrut}{\Longrightarrow}
\infer[{!}_L]
{\mildseq{\Gamma}{\Delta,{!}A}{U}}
{\infer-[{\it subst}^+]
 {\mildseq{\Gamma,A}{\Delta}{U}}
 {\infer[{!}_R]
  {\mildseq{\Gamma, A}{\cdot}{[{!}A]}}
  {\infer-[\eta^-]
   {\mildseq{\Gamma, A}{\cdot}{A}}
   {\infer[\it copy]
    {\mildseq{\Gamma, A}{\cdot}{\langle A \rangle}}
    {\infer[{\it id}^-]
     {\mildseq{\Gamma, A}{[ A ]}{\langle A \rangle}}
     {}}}}
  &
  \infer-[{\it weaken}]
  {\mildseq{\Gamma,A}{\Delta, \langle {!}A \rangle}{U}}
  {\deduce
   {\mildseq{\Gamma}{\Delta, \langle {!}A \rangle}{U}}
   {\mathcal D}}}}
\]

\[
\infer-[\eta^+]
{\mildseq{\Gamma}{\Delta, A \otimes B}{U}}
{\deduce{\mildseq{\Gamma}{\Delta, \langle A \otimes B \rangle}{U}}{\mathcal D}}
\quad
\deduce{\mathstrut}{\Longrightarrow}
\!\!\!\!\!\!\!\!\!
\infer[{\otimes}_L]
{\mildseq{\Gamma}{\Delta, A \otimes B}{U}}
{\infer-[\eta^+]
 {\mildseq{\Gamma}{\Delta, \langle A \rangle, B}{U}}
 {\infer-[{\it subst}^+]
  {\mildseq{\Gamma}{\Delta, \langle A \rangle, \langle B \rangle}{U}}
  {\infer
   {\mildseq{\Gamma}{\langle A \rangle, \langle B \rangle}{[A \otimes B]}}
   {\infer[{\it id}^+]
    {\mildseq{\Gamma}{\langle A \rangle}{[A]}}
    {}
    & 
    \infer[{\it id}^+]
    {\mildseq{\Gamma}{\langle B \rangle}{[B]}}
    {}}
   & 
   \deduce
   {\mildseq{\Gamma}{\Delta, \langle A \otimes B \rangle}{U}}
   {\mathcal D}}}}
\]

\[
\infer-[\eta^-]
{\mildseq{\Gamma}{\Delta}{{\uparrow}A}}
{\deduce
 {\mildseq{\Gamma}{\Delta}{\langle {\uparrow}A \rangle}}
 {\mathcal D}}
\quad
\deduce{\mathstrut}{\Longrightarrow}
\quad
\infer[{\uparrow}_R]
{\mildseq{\Gamma}{\Delta}{{\uparrow}A}}
{\infer-[{\it subst}^-]
 {\mildseq{\Gamma}{\Delta}{A}}
 {\deduce
  {\mildseq{\Gamma}{\Delta}{\langle {\uparrow}A \rangle}}
  {\mathcal D}
  &
  \infer[{\uparrow}_L]
  {\mildseq{\Gamma}{[{\uparrow}A]}{A}}
  {\infer-[\eta^+]
   {\mildseq{\Gamma}{A}{A}}
   {\infer[{\it focus}_R]
    {\mildseq{\Gamma}{\langle A \rangle}{A}} 
    {\infer[{\it id}^+]
     {\mildseq{\Gamma}{\langle A \rangle}{[ A ]}}
     {}}}}}}
\]

\[
\infer-[\eta^-]
{\mildseq{\Gamma}{\Delta}{A \lolli B}}
{\deduce
 {\mildseq{\Gamma}{\Delta}{\langle A \lolli B \rangle}}
 {\mathcal D}}
\quad
\deduce{\mathstrut}{\Longrightarrow}
\!\!\!\!\!\!
\infer[{\lolli}_R]
{\mildseq{\Gamma}{\Delta}{A \lolli B}}
{\infer-[\eta^+]
 {\mildseq{\Gamma}{\Delta, A}{B}}
 {\infer-[\eta^-]
  {\mildseq{\Gamma}{\Delta, \langle A \rangle}{B}}
  {\infer-[{\it subst}^-]
   {\mildseq{\Gamma}{\Delta, \langle A \rangle}{\langle B \rangle}}
   {\deduce
    {\mildseq{\Gamma}{\Delta}{\langle A \lolli B \rangle}}
    {\mathcal D}
    &
    \infer[{\lolli}_L]
    {\mildseq{\Gamma}{\langle A \rangle, [ A \lolli B ]}{\langle B \rangle}}
    {\infer[{\it id}^+]
     {\mildseq{\Gamma}{\langle A \rangle}{[ A ]}}
     {}
     &
     \infer[{\it id}^-]
     {\mildseq{\Gamma}{[ B ]}{\langle B \rangle}}
     {}}}}}}
\]}
\caption{Identity expansion -- restricting $\eta^+$ and $\eta^-$ to atomic 
 propositions.}
\end{figure}




\begin{figure}
{\small

\[
\infer-[\eta^+]
{\mildseq{\Gamma}{\Delta, \one}{U}}
{\deduce
 {\mildseq{\Gamma}{\Delta, \langle \one \rangle}{U}}
 {\mathcal D}}
\quad
\Longrightarrow
\infer[{\one}_L]
{\mildseq{\Gamma}{\Delta, \one}{U}}
{\infer-[{\it subst}^+]
 {\mildseq{\Gamma}{\Delta}{U}}
 {\infer[{\one}_R]
  {\mildseq{\Gamma}{\cdot}{[ \one ]}}
  {}
  &
  \deduce
  {\mildseq{\Gamma}{\Delta, \langle \one \rangle}{U}}
  {\mathcal D}}}
\]

\[
\infer-[\eta^+]
{\mildseq{\Gamma}{\Delta, A \oplus B}{U}}
{\deduce
 {\mildseq{\Gamma}{\Delta, \langle A \oplus B \rangle}{U}}
 {\mathcal D}}
\quad
\Longrightarrow
\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
\infer[{\oplus}_R]
{\mildseq{\Gamma}{\Delta, A \oplus B}{U}}
{\infer-[\eta^+]
 {\mildseq{\Gamma}{\Delta, A}{U}}
 {\infer-[{\it subst}^+]
  {\mildseq{\Gamma}{\Delta, \langle A \rangle}{U}}
  {\infer[{\oplus}_{R1}]
   {\mildseq{\Gamma}{\Delta, \langle A \rangle}{[ A \oplus B ]}}
   {\infer[{\it id}^+]
    {\mildseq{\Gamma}{\langle A \rangle}{[ A ]}}
    {}}
   &
   \deduce
   {\mildseq{\Gamma}{\Delta, \langle A \oplus B \rangle}{U}}
   {\mathcal D}}}
 &
 \deduce
 {\mildseq{\Gamma}{\Delta, B}{U}}
 {\vdots}
 }
\]


\[
\infer-[\eta^-]
{\mildseq{\Gamma}{\Delta}{\top}}
{\deduce
 {\mildseq{\Gamma}{\Delta}{\langle \top \rangle}}
 {\mathcal D}}
\quad
\Longrightarrow
\quad
\infer[{\top}_R]
{\mildseq{\Gamma}{\Delta}{\top}}
{}
\]}

\caption{Identity expansion for units and additive connectives.}
\end{figure}


\newpage


  This presentation, shown in
Figure~\ref{fig:kaustuv-focused}, does not actually reduce the number
of derivations relative to Figure~\ref{fig:linear} until we make a few
extra global restrictions on rules:

\begin{enumerate}
\item The rules that introduces a focus (${\it focus}_R$, ${\it focus}_L$, 
  and ${\it copy}$) only applies if there is not already a proposition 
  in focus in $\Delta$ and if the succeedent $U$ is not in focus. 
\item Every focused rule (${\it focus}_R$, ${\it blur}_L$, ${\it
    blur}_R$, ${\it focus}_R$, ${\it init}^+$, ${\it init}^-$, ${\it
    copy}$, ${!}_R$, ${\one}_R$, ${\otimes}_R$, ${\lolli}_L$) can only
  apply if no non-focused rule applies -- that is, if the
  linear context $\Delta$ contains only negative propositions or positive
  atomic proposition and the succeedent $U$ is either a positive proposition
  or a negative atomic proposition.
\item When two non-focused rules (${!}_L$, ${\one}_L$, ${\tensor}_L$,
  ${\lolli}_R$) both apply, there is some (fixed but arbitrary) way of
  deciding which rule applies.
\end{enumerate}

\noindent
Condition (1) alone ensures that this presentation is equivalent to
the presentation with three judgments. The system with only condition
(1) is what Laurent called a {\it weakly focused} system
\cite{laurent04proof} and what Pfenning calls a {\it chaining} system.
The system with conditions (1) and (2) is what Laurent called a {\it
  strongly +-focused} system and what I have previously called a
weakly focused system (oops). The combination of all three global
conditions gives a fully focused system that is consistent with
Chaudhuri's presentation of focused linear logic
\cite{chaudhuri06focused}.

\subsection{Synthetic inference rules}

\begin{figure}
\[
\infer[{\lolli}_R]
{\mildseq{\cdot}{\cdot}{{!}({\sf 6bucks} \lolli {\sf battery}) \otimes
                    {\sf 6bucks} \otimes 
                    ({\sf battery} \lolli {\sf robot}) \lolli {\sf robot}}}
{\infer[{\otimes}_L]
{\mildseq{\cdot}{{!}({\sf 6bucks} \lolli {\sf battery}) \otimes
                    {\sf 6bucks} \otimes 
                    ({\sf battery} \lolli {\sf robot})}{{\sf robot}}}
{\infer[{!}_L]
{\mildseq{\cdot}{{!}({\sf 6bucks} \lolli {\sf battery}),
                    {\sf 6bucks} \otimes 
                    ({\sf battery} \lolli {\sf robot})}{{\sf robot}}}
{\infer[{\otimes}_L]
{\mildseq{\Gamma}{{\sf 6bucks} \otimes 
                    ({\sf battery} \lolli {\sf robot})}{{\sf robot}}}
{\infer[{\it copy}]
{\mildseq{\Gamma}{{\sf 6bucks}, {\sf battery} \lolli {\sf robot}}{{\sf robot}}}
{\infer[{\lolli}_L]
{\mildseq{\Gamma}{{\sf 6bucks}, {\sf battery} \lolli {\sf robot}, [{\sf 6bucks} \lolli {\sf battery}]}{{\sf robot}}}
{\infer[{\it init}^+]
 {\mildseq{\Gamma}{{\sf 6bucks}}{[{\sf 6bucks}]}}
 {}
 &
 \infer[{\it blur}_L]
 {\mildseq{\Gamma}{{\sf battery} \lolli {\sf robot}, [{\sf battery}]}{{\sf robot}}}
 {\infer[{\it focus}_L]
 {\mildseq{\Gamma}{{\sf battery} \lolli {\sf robot}, {\sf battery}}{{\sf robot}}}
 {\infer[{\lolli}_L]
 {\mildseq{\Gamma}{{\sf battery}, [{\sf battery} \lolli {\sf robot}]}{{\sf robot}}}
 {\infer[{\it init^+}]
  {\mildseq{\Gamma}{{\sf battery}}{[{\sf battery}]}}
  {}
  &
  \infer[{\it blur}_L]
  {\mildseq{\Gamma}{[{\sf robot}]}{{\sf robot}}}
  {\infer[{\it focus}_R]
  {\mildseq{\Gamma}{{\sf robot}}{{\sf robot}}}
  {\infer[{\it init}^+]
  {\mildseq{\Gamma}{{\sf robot}}{[{\sf robot}]}}
  {}}}}}}}}}}}}
\] 
\caption{Proving that a transition is possible 
(where we let $\Gamma = {\sf 6bucks} \lolli {\sf battery}$).}
\label{fig:unfocused-robot}
\end{figure}


\subsection{Atom optimization}

\subsection{Bang optimization}

\subsection{A more primitive logic?}

\section{Adjoint logic}

\section{Tensor logic}


