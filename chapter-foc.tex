
\chapter{Linear logic}

Logic as it has been traditionally understood and studied -- both in
its classical and intuitionistic varieties -- treats the truth of a
proposition as a {\it persistent resource}. That is, if we have
evidence for the truth of a proposition, we can ignore that evidence
if it is not needed and reuse the evidence as many times as we need
to. Throughout this thesis, ``logic as it has been traditionally
understood as studied'' will be referred to as {\it persistent} logic
to emphasize this treatment of evidence. 

Linear logic, which was studied and
popularized by Girard \cite{girard87linear},
treats evidence as an {\it ephemeral} resource; the use of an
ephemeral resource consumes it, at which point it is unavailable for
further use.  Linear logic, like persistent logic, comes in classical
and intuitionistic flavors. We will favor intuitionistic linear logic
in part because the propositions of intuitionistic linear logic
(written $A$, $B$, $C$, \ldots) have a more natural correspondence
with our physical intuitions about consumable resources. Linear
conjunction $A \tensor B$ represents the resource built from the
resources $A$ and $B$; if you have both a bowl of soup {\it and} a
sandwich, that resource can be represented by the proposition ${\sf
  soup} \otimes {\sf sandwich}$. Linear implication $A \lolli B$
represents a resource that can interact with another resource $A$ to
produce a resource $B$. One robot with batteries not included could be
represented as the linear resource $({\sf battery} \lolli {\sf
  robot})$, and the linear resource $({\sf 6bucks} \lolli {\sf soup}
\tensor {\sf sandwich})$ represents the ability to use \$6 to obtain
lunch -- but only once!\footnote{Conjunction will always bind more
  tightly than implication, so this is equivalent to the proposition
  ${\sf 6bucks} \lolli ({\sf soup} \tensor {\sf sandwich})$.} Linear
logic also has a modal connective ${!}A$ representing a persistent
resource that can be
used to generate any number of $A$ resources, including zero. The
Panera ``You Pick Two'' menu might be represented as
\[ {!}({\sf 6bucks} \lolli {\sf soup} \tensor {\sf sandwich}) \otimes
{!}({\sf 6bucks} \lolli {\sf soup} \tensor {\sf salad}) \otimes
{!}({\sf 6bucks} \lolli {\sf sandwich} \tensor {\sf salad}),\] as the
menu gives you the opportunity to exchange six dollars for two
distinct members of the set $\{ {\sf soup}, {\sf salad}, {\sf
  sandwich} \}$ any number of times.

\input{figs/fig-linear.tex}

Figure~\ref{fig:linear} presents a standard sequent calculus for
linear logic, in particular the so-called {\it multiplicative,
  exponential} fragment of intuitionistic linear logic (or {\it
  MELL}). It corresponds most closely to Barber's dual intuitionistic
linear logic \cite{barber96dual}, but also to Andreoli's dyadic system
\cite{andreoli92logic} and Chang et al.'s judgmental analysis of
intuitionistic linear logic \cite{chang03judgmental}.

\subsection*{Transitions in linear logic}

The propositions of intuitionstic linear logic, and linear implication
in particular, capture a notion of state change: we can {\it
  transition} from a state where we have both a ${\sf battery}$ and
the battery-less robot (represented, as before, by the linear
implication ${\sf battery} \lolli {\sf robot}$) to a state where we
have the battery-endowed (and therefore presumably functional) robot
(represented by the proposition ${\sf robot}$). In other words, the
proposition
%
\[{\sf battery} \otimes ({\sf battery} \lolli {\sf robot}) \lolli
{\sf robot}\] 
%
is provable in linear logic. These transitions can be chained
together as well: if we start out with ${\sf
  6bucks}$ instead of ${\sf battery}$ but we also have the
persistent ability to turn ${\sf 6bucks}$ into a ${\sf battery}$ --
just like we turned \$6 into a bowl of soup and a salad at Panera --
then we can ultimately get our working robot as well.
Written as a series of transitions, the picture looks like this:
\[
\begin{array}{ccccc}
\begin{array}{c}
\mbox{\it \$6 (1)}\medskip\\ 
\mbox{\it battery-free robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}
\end{array}
& \leadsto &
\begin{array}{c}
\mbox{\it battery  (1)}\medskip\\ 
\mbox{\it battery-free robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}
\end{array}
& \leadsto &
\begin{array}{c}
\mbox{\it robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}\medskip\\~\\
\end{array}
\end{array}
\]
In linear logic, these transitions correspond to the provability
of the proposition
\[{!}({\sf 6bucks} \lolli {\sf battery}) \otimes {\sf 6bucks} \otimes
({\sf battery} \lolli {\sf robot}) \lolli {\sf robot}.\] 
A derivation of this proposition is given in
Figure~\ref{fig:unfocused-robot}.\footnote{In Chapter XXX, I will
  argue that this view isn't quite precise enough, and that the most
  natural representation of state change from the state $A$ to the
  state $B$ isn't really captured by derivations of the proposition $A
  \lolli B$ or by derivations of the hypothetical judgment
  $\seq{\cdot}{A}{B}$.  However, this view remains a simple and useful
  one; Cervesato and Scedrov cover it thoroughly in the context of
  intuitionstic linear logic \cite{cervesato09relating}.}  

\input{figs/fig-unfocused-robot}

It is precisely because linear logic contains this natural notion of
state and state transition that a rich line of work, dating back to
Chirmar's 1995 Ph.D. thesis, has sought to use linear logic as a {\it
  logical framework} for describing stateful systems
\cite{chirimar95proof,cervesato02linear,
  cervesato02concurrent,pfenning04substructural,miller09formalizing,
  pfenning09substructural,cervesato09relating}.  

\subsection*{Logical frameworks}

Generally speaking, logical frameworks use the {\it structure} of
proofs in a logic (like linear logic) to describe the structures we're
really interested in (like the process of obtaining a robot).  There
are two related reasons why linear logic as described in
Figure~\ref{fig:linear} is not immediately useful as a logical
framework. First, the structure of the proof in
Figure~\ref{fig:unfocused-robot} doesn't really match the intuitive
two-step transition that we sketched out above. Second, there are {\it
  lots} of derivations of our example proposition according to the
rules in Figure~\ref{fig:linear}, even though there's only one
``real'' series of transitions that get us to a working robot. The use
of ${!}L$, for instance, could be permuted up past the ${\otimes}L$
and then past the ${\lolli}L$ into the left branch of the proof. These
differences represent inessential nondeterminism in proof construction
or in proof search -- they just get in the way of the structure that
we are trying to capture. 

This is a general problem in the construction of logical frameworks,
and we'll discuss two solutions in the context of LF, a logical
framework based on dependent type theory that has proved to be a
suitable means of encoding a wide variety of deductive systems, such
as logics and programming languages \cite{harper93framework}.  The
first solution is to define an appropriate equivalence class of
proofs, and the second solution is to define an appropriate fragment
of canonical proofs.

Using an appropriate equvialence class of proofs can be an effective
way of defining away the problem of inessential nondeterminism.  In
linear logic as presented above, if the permutability of rules like
${!}_L$ and ${\otimes}_L$ is problematic, we can instead reason about
{\it equivalence classes} of derivations where proofs that differ only
in the ordering of ${!}_L$ and ${\otimes}_L$ rules are treated as
equivalent (that is, as members of the same equivalence class):
\[
\infer[{!}_L]
{\seq{\Gamma}{\Delta, {!}A, B \otimes C}{D}}
{\infer[{\otimes}_L]
 {\seq{\Gamma,A}{\Delta, B \otimes C}{D}}
 {\deduce{\seq{\Gamma,A}{\Delta, B, C}{D}}{\mathcal D}}}
\quad
\deduce{\mathstrut}{\mathstrut{\equiv}}
\quad
\infer[{\otimes}_L]
{\seq{\Gamma}{\Delta, {!}A, B \otimes C}{D}}
{\infer[{!}_L]
 {\seq{\Gamma}{\Delta, {!}A, B, C}{D}}
 {\deduce{\seq{\Gamma,A}{\Delta, B, C}{D}}{\mathcal D}}}
\]

In LF, lambda calculus terms (which correspond to derivations by the
Curry-Howard) are considered modulo the least equivalence class that
includes
\begin{itemize}
\item $\alpha$-equivalence ($\lambda x.N \equiv \lambda y.N[y/x]$ if 
$y \not\in {\it FV}(N)$), 
\item $\beta$-equivalence 
($(\lambda x.\,M)N \equiv M[N/x]$ if $x \not\in {\it FV}(N)$), and 
\item $\eta$-equivalence ($N \equiv \lambda x.N\,x$).
\end{itemize}
The weak normalization property for LF establishes that given any
typed LF term, we can find an equivalent term that is $\beta$-normal
(no $\beta$-redexes of the form $(\lambda x.M) N$ exist) and
$\eta$-long (replacing $N$ with $\lambda x.N\,x$ anywhere would
introduce a $\beta$-redex or make the term ill-typed).  Furthermore,
in any given equivalence class of typed LF terms, all the
$\beta$-normal and $\eta$-long terms are $\alpha$-equivalent.
Therefore, because $\alpha$-equivalence is decidable, the equvalence
of typed LF terms is also decidable. 

The uniqueness of $\beta$-normal and $\eta$-long terms within an
equivalence class of lambda calculus terms (modulo
$\alpha$-equivalence, which we will henceforth take for granted) makes
these terms useful as canonical representatives of equivalence
classes. In Harper, Honsell, and Plotkin's original formulation
of LF, a deductive system was said to be {\it adequately encoded} as
an LF type family in the case that there is a compositional bijection
between the formal objects in the deductive system and these
$\beta$-normal, $\eta$-long representatives of equivalence classes.

More modern presentations of LF, such as Harper and Licata's
\cite{harper07mechanizing}, follow the approach developed by Watkins
et al.~\cite{watkins02concurrent} and define the logical framework so
that it only contains these $\beta$-normal, $\eta$-long {\it canonical
  forms} of LF. This presentations of LF is called Canonical LF to
distinguish it from the original presentation of LF in which the
canonicla forms are just a subset of the possible terms. A central
component in this approach is {\it hereditary substitution}, which can
also be used to take any regular LF term and transform it into a
Canonical LF term.

We will generally follow this second appraoach, defining our a
restricted form of

One analouge to the canonical forms of LF will be the {\it focused
  derivations} of linear logic that are presented in the next
section. We will see that there is only
one focused derivation of 
\[{!}({\sf 6bucks} \lolli {\sf battery}) \otimes {\sf 6bucks} \otimes
({\sf battery} \lolli {\sf robot}) \lolli {\sf robot}\] and therefore
the focused derivations of linear logic are our first candidate for
adequateny encoding transition systems. In this chapter, we will
survey related work on focusing intuitionstic logic, starting from
Chaudhuri's formulation \cite{chaudhuri06focused}, and issues with
each of these formulations.

\section{Focused linear logic}

Andreoli's original motivation for introducing focusing was not to
describe a logical framework, it was to describe a logic programming
language based on proof search in classical linear logic
\cite{andreoli92logic}. The existance of multiple proofs that differ
in inessential ways is particularly problematic for proof search, as
inessential differences between derivations correspond to unnecessary
choice points that a proof search procedure will need to backtrack
over. 

The first step in describing a focused sequent calculus is to classify
connectives into two groups.  Some connectives, such as linear
implication $A \lolli B$, are called {\it asynchronous} because their
right rules can always be applied eagerly, without backtracking,
during bottom-up proof search. Other connectives, such as disjunction
$A \tensor B$, are called {\it synchronous} because their right rules
cannot be applied eagerly. For instance, if we are trying to prove
$\seq{\Gamma}{A \tensor B}{B \tensor A}$, the ${\tensor}R$ rule cannot
be applied eagerly; we first have to decompose $A \tensor B$ on the
left using the ${\tensor}L$ rule.\footnote{Andreoli dealt with a
  one-sided classical sequent calculus; in intuitionistic logics, it
  is common to call asynchronous connectives {\it right}-asynchronous
  and {\it left}-synchronous. Similarly, it is common to call
  synchronous connectives {\it right}-synchronous and {\it
    left}-asynchronous.

  Synchronicity, a property of connectives, is closely connected to
  (and sometimes conflated with) a property of rules called {\it
    invertibility}; a rule is invertible if the conclusion of the rule
  implies the premises. So ${\lolli}R$ is invertible
  ($\seq{\Gamma}{\Delta}{A \lolli B}$ implies $\seq{\Gamma}{\Delta,
    A}{B}$) but ${\lolli}L$ is not ($\seq{\Gamma}{\Delta, A \lolli
    B}{C}$ does not imply that $\Delta = \Delta_1, \Delta_2$ such that
  $\seq{\Gamma}{\Delta_1}{A}$ and $\seq{\Gamma}{\Delta_2, B}{C}$).
  Rules that can be applied eagerly need to be invertible, so
  asynchronous connectives have invertible right rules and synchronous
  connectives have invertible left rules. Therefore, another synonym
  for asynchronous/negative is {\it right-invertible}, and another
  synonym for synchronous/positive is {\it left-invertible}.}  We call
the asynchronous connectives {\it negative} and write them as $A^-$,
and call the synchronous connectives {\it positive} and write them as
$A^+$. In the fragment of linear logic presented earlier, the
propositions end up classified like this:
\begin{align*}
A^+ & ::= p^+ \mid {!}A \mid \one \mid A \otimes B\\
A^- & ::= p^- \mid A \lolli B
\end{align*}
Note that our previously indistinguishable atomic propositions $p^+$
and $p^-$ are now distinguishable: $p^+$ is a positive atomic proposition and 
can be treated as a stand-in for an arbitrary positive propositions,
whereas $p^-$ is a negative atomic proposition and can be treated 
as a stand-in for an arbitrary negative propositions.

We next need some extra proof-theoretic machinery that is 
aware of proofs in order to put
propositions {\it in focus} (a proposition in focus will be written as
$[A]$). This extra machinery will then be used to enforce that right
rules can only be applied to positive propositions when they are in
focus, and likewise for negative propositions and left rules.  This
extra machinery is usually described by presenting multiple judgments.
The number differes, but at least three are necessary:

\begin{itemize}
\item $\mildrfoc{\Gamma}{\Delta}{A}$ (the {\it right focus} sequent),
\item $\mildinv{\Gamma}{\Delta}{C}$ (the {\it inversion} sequent), and
\item $\mildlfoc{\Gamma}{\Delta}{A}{C}$ (the {\it left focus} sequent).
\end{itemize}

\input{figs/fig-kaustuv-focused.tex}


Another reasonable presentation of linear logic uses only one sequent
$\mildseq{\Gamma}{\Delta}{U}$, but generalizes what is to allowed to
to appear in the linear context $\Delta$ or as the succeedent, which
we write $U$. The three focusing rules
${\it focus}_R$, ${\it focus}_L$, and ${\it copy}$ carry the extra condition
that the conclusion must be stable: the context $\Delta$ can only contain 
negative propositions $A^-$ and suspended positive propositions 
$\langle A^+ \rangle$,.

\begin{figure}[f]
{\small 
\[
\infer-[\eta^+]
{\mildseq{\Gamma}{\Delta, {\downarrow}A}{U}}
{\deduce
 {\mildseq{\Gamma}{\Delta, \langle {\downarrow}A \rangle}{U}}
 {\mathcal D}}
\quad
\deduce{\mathstrut}{\Longrightarrow}
\quad
\infer[{\downarrow}_L]
{\mildseq{\Gamma}{\Delta, {\downarrow}A}{U}}
{\infer-[{\it subst}^+]
 {\mildseq{\Gamma}{\Delta, A}{U}}
 {\infer[{\downarrow}_R]
  {\mildseq{\Gamma}{A}{[ {\downarrow}A ]}}
  {\infer-[{\eta}^-]
   {\mildseq{\Gamma}{A}{A}}
   {\infer-[{\it focus}_L]
    {\mildseq{\Gamma}{A}{\langle A \rangle}}
    {\infer[{\it id}^-]
     {\mildseq{\Gamma}{[ A ]}{\langle A \rangle}}
     {}}}}
  &
  \deduce
  {\mildseq{\Gamma}{\Delta, \langle {\downarrow}A \rangle}{U}}
  {\mathcal D}}}
\]

\[
\infer-[\eta^+]
{\mildseq{\Gamma}{\Delta,{!}A}{U}}
{\deduce
 {\mildseq{\Gamma}{\Delta, \langle {!}A \rangle}{U}}
 {\mathcal D}}
\quad
\deduce{\mathstrut}{\Longrightarrow}
\infer[{!}_L]
{\mildseq{\Gamma}{\Delta,{!}A}{U}}
{\infer-[{\it subst}^+]
 {\mildseq{\Gamma,A}{\Delta}{U}}
 {\infer[{!}_R]
  {\mildseq{\Gamma, A}{\cdot}{[{!}A]}}
  {\infer-[\eta^-]
   {\mildseq{\Gamma, A}{\cdot}{A}}
   {\infer[\it copy]
    {\mildseq{\Gamma, A}{\cdot}{\langle A \rangle}}
    {\infer[{\it id}^-]
     {\mildseq{\Gamma, A}{[ A ]}{\langle A \rangle}}
     {}}}}
  &
  \infer-[{\it weaken}]
  {\mildseq{\Gamma,A}{\Delta, \langle {!}A \rangle}{U}}
  {\deduce
   {\mildseq{\Gamma}{\Delta, \langle {!}A \rangle}{U}}
   {\mathcal D}}}}
\]

\[
\infer-[\eta^+]
{\mildseq{\Gamma}{\Delta, A \otimes B}{U}}
{\deduce{\mildseq{\Gamma}{\Delta, \langle A \otimes B \rangle}{U}}{\mathcal D}}
\quad
\deduce{\mathstrut}{\Longrightarrow}
\!\!\!\!\!\!\!\!\!
\infer[{\otimes}_L]
{\mildseq{\Gamma}{\Delta, A \otimes B}{U}}
{\infer-[\eta^+]
 {\mildseq{\Gamma}{\Delta, \langle A \rangle, B}{U}}
 {\infer-[{\it subst}^+]
  {\mildseq{\Gamma}{\Delta, \langle A \rangle, \langle B \rangle}{U}}
  {\infer
   {\mildseq{\Gamma}{\langle A \rangle, \langle B \rangle}{[A \otimes B]}}
   {\infer[{\it id}^+]
    {\mildseq{\Gamma}{\langle A \rangle}{[A]}}
    {}
    & 
    \infer[{\it id}^+]
    {\mildseq{\Gamma}{\langle B \rangle}{[B]}}
    {}}
   & 
   \deduce
   {\mildseq{\Gamma}{\Delta, \langle A \otimes B \rangle}{U}}
   {\mathcal D}}}}
\]

\[
\infer-[\eta^-]
{\mildseq{\Gamma}{\Delta}{{\uparrow}A}}
{\deduce
 {\mildseq{\Gamma}{\Delta}{\langle {\uparrow}A \rangle}}
 {\mathcal D}}
\quad
\deduce{\mathstrut}{\Longrightarrow}
\quad
\infer[{\uparrow}_R]
{\mildseq{\Gamma}{\Delta}{{\uparrow}A}}
{\infer-[{\it subst}^-]
 {\mildseq{\Gamma}{\Delta}{A}}
 {\deduce
  {\mildseq{\Gamma}{\Delta}{\langle {\uparrow}A \rangle}}
  {\mathcal D}
  &
  \infer[{\uparrow}_L]
  {\mildseq{\Gamma}{[{\uparrow}A]}{A}}
  {\infer-[\eta^+]
   {\mildseq{\Gamma}{A}{A}}
   {\infer[{\it focus}_R]
    {\mildseq{\Gamma}{\langle A \rangle}{A}} 
    {\infer[{\it id}^+]
     {\mildseq{\Gamma}{\langle A \rangle}{[ A ]}}
     {}}}}}}
\]

\[
\infer-[\eta^-]
{\mildseq{\Gamma}{\Delta}{A \lolli B}}
{\deduce
 {\mildseq{\Gamma}{\Delta}{\langle A \lolli B \rangle}}
 {\mathcal D}}
\quad
\deduce{\mathstrut}{\Longrightarrow}
\!\!\!\!\!\!
\infer[{\lolli}_R]
{\mildseq{\Gamma}{\Delta}{A \lolli B}}
{\infer-[\eta^+]
 {\mildseq{\Gamma}{\Delta, A}{B}}
 {\infer-[\eta^-]
  {\mildseq{\Gamma}{\Delta, \langle A \rangle}{B}}
  {\infer-[{\it subst}^-]
   {\mildseq{\Gamma}{\Delta, \langle A \rangle}{\langle B \rangle}}
   {\deduce
    {\mildseq{\Gamma}{\Delta}{\langle A \lolli B \rangle}}
    {\mathcal D}
    &
    \infer[{\lolli}_L]
    {\mildseq{\Gamma}{\langle A \rangle, [ A \lolli B ]}{\langle B \rangle}}
    {\infer[{\it id}^+]
     {\mildseq{\Gamma}{\langle A \rangle}{[ A ]}}
     {}
     &
     \infer[{\it id}^-]
     {\mildseq{\Gamma}{[ B ]}{\langle B \rangle}}
     {}}}}}}
\]}
\caption{Identity expansion -- restricting $\eta^+$ and $\eta^-$ to atomic 
 propositions.}
\end{figure}




\begin{figure}
{\small

\[
\infer-[\eta^+]
{\mildseq{\Gamma}{\Delta, \one}{U}}
{\deduce
 {\mildseq{\Gamma}{\Delta, \langle \one \rangle}{U}}
 {\mathcal D}}
\quad
\Longrightarrow
\infer[{\one}_L]
{\mildseq{\Gamma}{\Delta, \one}{U}}
{\infer-[{\it subst}^+]
 {\mildseq{\Gamma}{\Delta}{U}}
 {\infer[{\one}_R]
  {\mildseq{\Gamma}{\cdot}{[ \one ]}}
  {}
  &
  \deduce
  {\mildseq{\Gamma}{\Delta, \langle \one \rangle}{U}}
  {\mathcal D}}}
\]

\[
\infer-[\eta^+]
{\mildseq{\Gamma}{\Delta, A \oplus B}{U}}
{\deduce
 {\mildseq{\Gamma}{\Delta, \langle A \oplus B \rangle}{U}}
 {\mathcal D}}
\quad
\Longrightarrow
\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
\infer[{\oplus}_R]
{\mildseq{\Gamma}{\Delta, A \oplus B}{U}}
{\infer-[\eta^+]
 {\mildseq{\Gamma}{\Delta, A}{U}}
 {\infer-[{\it subst}^+]
  {\mildseq{\Gamma}{\Delta, \langle A \rangle}{U}}
  {\infer[{\oplus}_{R1}]
   {\mildseq{\Gamma}{\Delta, \langle A \rangle}{[ A \oplus B ]}}
   {\infer[{\it id}^+]
    {\mildseq{\Gamma}{\langle A \rangle}{[ A ]}}
    {}}
   &
   \deduce
   {\mildseq{\Gamma}{\Delta, \langle A \oplus B \rangle}{U}}
   {\mathcal D}}}
 &
 \deduce
 {\mildseq{\Gamma}{\Delta, B}{U}}
 {\vdots}
 }
\]


\[
\infer-[\eta^-]
{\mildseq{\Gamma}{\Delta}{\top}}
{\deduce
 {\mildseq{\Gamma}{\Delta}{\langle \top \rangle}}
 {\mathcal D}}
\quad
\Longrightarrow
\quad
\infer[{\top}_R]
{\mildseq{\Gamma}{\Delta}{\top}}
{}
\]}

\caption{Identity expansion for units and additive connectives.}
\end{figure}


\newpage


  This presentation, shown in
Figure~\ref{fig:kaustuv-focused}, does not actually reduce the number
of derivations relative to Figure~\ref{fig:linear} until we make a few
extra global restrictions on rules:

\begin{enumerate}
\item The rules that introduces a focus (${\it focus}_R$, ${\it focus}_L$, 
  and ${\it copy}$) only applies if there is not already a proposition 
  in focus in $\Delta$ and if the succeedent $U$ is not in focus. 
\item Every focused rule (${\it focus}_R$, ${\it blur}_L$, ${\it
    blur}_R$, ${\it focus}_R$, ${\it init}^+$, ${\it init}^-$, ${\it
    copy}$, ${!}_R$, ${\one}_R$, ${\otimes}_R$, ${\lolli}_L$) can only
  apply if no non-focused rule applies -- that is, if the
  linear context $\Delta$ contains only negative propositions or positive
  atomic proposition and the succeedent $U$ is either a positive proposition
  or a negative atomic proposition.
\item When two non-focused rules (${!}_L$, ${\one}_L$, ${\tensor}_L$,
  ${\lolli}_R$) both apply, there is some (fixed but arbitrary) way of
  deciding which rule applies.
\end{enumerate}

\noindent
Condition (1) alone ensures that this presentation is equivalent to
the presentation with three judgments. The system with only condition
(1) is what Laurent called a {\it weakly focused} system
\cite{laurent04proof} and what Pfenning calls a {\it chaining} system.
The system with conditions (1) and (2) is what Laurent called a {\it
  strongly +-focused} system and what I have previously called a
weakly focused system (oops). The combination of all three global
conditions gives a fully focused system that is consistent with
Chaudhuri's presentation of focused linear logic
\cite{chaudhuri06focused}.


\section{Synthetic inference rules}


\section{Hacking the focusing system}

\subsection{Atom optimization}

\subsection{Bang optimization}

\subsection{A more primitive logic?}

\paragraph{Adjoint logic}

\paragraph{Tensor logic}

\subsection{Concurrent equality}

\section{Revisiting our notation}

\section{A note on }

Talk about equivalence, Chris's unpublished work, and where
the focalization theorem given by this approach is deficient -- 