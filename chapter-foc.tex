\chapter{Linear logic}
\label{chapter-foc}

In this chapter, we present linear logic as a logic with the ability
to express aspects of state and state transition in a natural way.  In
Chapter~\ref{chapter-order} 
we will repeat the development in this chapter in a much
richer and more expressive setting, and in Chapter~\ref{chapter-framework} 
we will carve out
a fragment of this logic to use as the basis of \sls, our logical
framework of substructural logical specifications. These three
chapters contribute to the overall thesis by focusing on the design
of logical frameworks:

\smallskip
\begin{quote} {\bf Thesis (Part I):} {\it The methodology of
    structural focalization facilitates the derivation of logical
    frameworks as fragments of focused logics.}
\end{quote}
\smallskip

\noindent
The purpose of this chapter is to introduce the methodology of
{\it structural focalization};
this development is one of the major contributions of this work. 
Linear logic is a fairly simple logic that nevertheless allows us to
consider many of the issues that will arise in richer substructural
logics like the one considered in Chapter~\ref{chapter-order}.

In Section~\ref{sec:introlinlog} we motivate and discuss a traditional
account of linear logic, and in
Section~\ref{sec:linlogicalframeworks} we discuss why this account is
insufficient as a {\it logical framework} -- derivations in linear
logic suffice to establish the existence of a series of state
transitions but do not adequately capture the structure of those
transitions. Our remedy for this insufficiency comes in the form of
{\it focusing}, Andreoli's restricted normal form for derivations in
linear logic. We discuss focusing for a polarized presentation of
linear logic in Section~\ref{sec:foclinlog}.

With focusing, we can describe {\it synthetic inference rules}
(Section~\ref{sec:linsynthetic}) that succinctly capture the structure
of focused transitions. In Section~\ref{sec:linhack} we discuss a
number of ways of modifying the design of our focused logic to
increase the expressiveness of synthetic inference rules; one of the
alternatives we present, the introduction of {\it permeable atomic
  propositions}, will be generalized and incorporated into the focused
presentation of ordered linear lax logic that we discuss in 
Chapter~\ref{chapter-order}.

% In Section~\ref{sec:linconcurrenteq}, we describe {\it concurrent
%   equality}, an equivalence relation on focused derivations introduced
% by Watkins et al.~that is motivated by the desire to capture
% independent concurrent transitions. We also discuss the limitations of
% concurrent equality in a fully focused setting.  
% In Chapter~\ref{chapter-framework}, when we
% select a fragment of ordered linear lax logic to form \sls, we will
% incorporate a similar notion of concurrent equality.


\section{Introduction to linear logic}
\label{sec:introlinlog}

Logic as it has been traditionally understood and studied -- both in
its classical and intuitionistic varieties -- treats the truth of a
proposition as a {\it persistent resource}. That is, if we have
evidence for the truth of a proposition, we can ignore that evidence
if it is not needed and reuse the evidence as many times as we need
to. Throughout this document, ``logic as it has been traditionally
understood as studied'' will be referred to as {\it persistent} logic
to emphasize this treatment of evidence. 

Linear logic, which was studied and popularized by Girard
\cite{girard87linear}, treats evidence as an {\it ephemeral} resource;
the use of an ephemeral resource consumes it, at which point it is
unavailable for further use.  Linear logic, like persistent logic,
comes in classical and intuitionistic flavors. We will favor
intuitionistic linear logic in part because the propositions of
intuitionistic linear logic (written $A$, $B$, $C$, \ldots) have a
more natural correspondence with our physical intuitions about
consumable resources. Linear conjunction $A \tensor B$ (``$A$ tensor
$B$'') represents the resource built from the resources $A$ and $B$;
if you have both a bowl of soup {\it and} a sandwich, that resource
can be represented by the proposition ${\sf soup} \otimes {\sf
  sandwich}$. Linear implication $A \lolli B$ (``$A$ lolli $B$'')
represents a resource that can interact with another resource $A$ to
produce a resource $B$. One robot with batteries not included could be
represented as the linear resource $({\sf battery} \lolli {\sf
  robot})$, and the linear resource $({\sf 6bucks} \lolli {\sf soup}
\tensor {\sf sandwich})$ represents the ability to use \$6 to obtain
lunch -- but only once.\footnote{Conjunction will always bind more
  tightly than implication, so this is equivalent to the proposition
  ${\sf 6bucks} \lolli ({\sf soup} \tensor {\sf sandwich})$.} Linear
logic also has a connective ${!}A$ (``bang $A$'' or ``of course $A$'')
representing a persistent resource that can be used to generate any
number of $A$ resources, including zero. Your local Panera, which
allows six dollars to be exchanged for both soup and a sandwich any
number of times, can be represented as the resource ${!}({\sf 6bucks}
\lolli {\sf soup} \tensor {\sf sandwich})$.

\input{figs/fig-linear.tex}

Figure~\ref{fig:linear} presents a standard sequent calculus for
linear logic, in particular the {\it multiplicative, exponential}
fragment of intuitionistic linear logic (or {\it MELL}), so called
because the connectives $\one$, $A \tensor B$, and $A \lolli B$ are
considered to be the {\it multiplicative} connectives, and the
connective ${!}A$ is the {\it exponential} connective of
intuitionistic linear logic.\footnote{In this chapter we will mostly
  ignore the {\it additive} connectives of intuitionistic linear logic
  $\zero$, $A \oplus B$, $\top$, and $A \with B$ and will entirely
  ignore the {\it first-order} connectives $\exists x.A$ and $\forall
  x.A$. The ``why not'' connective $\mbox{?}A$ from classical linear
  logic is sometimes treated as a second exponential connective in
  intuitionistic linear logic \cite{chang03judgmental}, but we will
  never ask ``why not?'' in the context of this dissertation.} It
corresponds most closely to Barber's dual intuitionistic linear logic
\cite{barber96dual}, but also to Andreoli's dyadic system
\cite{andreoli92logic} and Chang et al.'s judgmental analysis of
intuitionistic linear logic \cite{chang03judgmental}.

%\subsection{Transitions in linear logic}
%\label{sec:linlogtrans}

The propositions of intuitionistic linear logic, and linear implication
in particular, capture a notion of state change: we can {\it
  transition} from a state where we have both a ${\sf battery}$ and
the battery-less robot (represented, as before, by the linear
implication ${\sf battery} \lolli {\sf robot}$) to a state where we
have the battery-endowed (and therefore presumably functional) robot
(represented by the proposition ${\sf robot}$). In other words, the
proposition
%
\[{\sf battery} \otimes ({\sf battery} \lolli {\sf robot}) \lolli
{\sf robot}\] 
%
is provable in linear logic. These transitions can be chained
together as well: if we start out with ${\sf
  6bucks}$ instead of ${\sf battery}$ but we also have the
persistent ability to turn ${\sf 6bucks}$ into a ${\sf battery}$ --
just like we turned \$6 into a bowl of soup and a salad at Panera --
then we can ultimately get our working robot as well.
Written as a series of transitions, the picture looks like this:
\input{figs/test-2trans}
In linear logic, these transitions correspond to the provability
of the proposition
\[{!}({\sf 6bucks} \lolli {\sf battery}) \otimes {\sf 6bucks} \otimes
({\sf battery} \lolli {\sf robot}) \lolli {\sf robot}.\] A derivation
of this proposition is given in
Figure~\ref{fig:unfocused-robot}.\footnote{In
  Chapter~\ref{chapter-framework} (and Section~\ref{sec:whytraces} in
  particular) we see that this view isn't quite precise enough, and
  that the ``best'' representation of state change from the state
  $A$ to the state $B$ isn't really captured by derivations of the
  proposition $A \lolli B$ or by derivations of the sequent
  $\seq{\cdot}{A}{B}$.  However, this view remains a simple and useful
  one; Cervesato and Scedrov cover it thoroughly in the context of
  intuitionistic linear logic \cite{cervesato09relating}.}

\input{figs/fig-unfocused-robot}

It is precisely because linear logic contains this intuitive notion of
state and state transition that a rich line of work, dating back to
Chirmar's 1995 dissertation, has sought to use linear logic as a {\it
  logical framework} for describing stateful systems
\cite{chirimar95proof,cervesato02linear,
  cervesato02concurrent,pfenning04substructural,miller09formalizing,
  pfenning09substructural,cervesato09relating}.  

\section{Logical frameworks}
\label{sec:linlogicalframeworks}

Generally speaking, logical frameworks use the {\it structure} of
proofs in a logic (like linear logic) to describe the structures we're
interested in (like the process of obtaining a robot).  There are two
related reasons why linear logic as described in
Figure~\ref{fig:linear} is not immediately useful as a logical
framework. First, the structure of the derivation in
Figure~\ref{fig:unfocused-robot} doesn't really match the intuitive
two-step transition that we sketched out above. Second, there are {\it
  lots} of derivations of our example proposition according to the
rules in Figure~\ref{fig:linear}, even though there's only one
``real'' series of transitions that get us to a working robot. The use
of ${!}L$, for instance, could be permuted up past the ${\otimes}L$
and then past the ${\lolli}L$ into the left branch of the proof. These
differences represent inessential nondeterminism in proof construction
-- they just get in the way of the structure that we are trying to
capture.

This is a general problem in the construction of logical frameworks.
We'll discuss two solutions in the context of LF, a logical
framework based on dependent type theory that has proved to be a
suitable means of encoding a wide variety of deductive systems, such
as logics and programming languages \cite{harper93framework}.  The
first solution is to define an appropriate equivalence class of
proofs, and the second solution is to define a complete set
of canonical proofs.

Defining an appropriate equivalence relation on proofs can be an effective
way of handling this inessential nondeterminism.  In
linear logic as presented above, if the permutability of rules like
${!}_L$ and ${\otimes}_L$ is problematic, we can instead reason about
{\it equivalence classes} of derivations. Derivations that differ only
in the ordering of ${!}_L$ and ${\otimes}_L$ rules belong in the
same equivalence class (which means we treat them as equivalent):
\[
\infer[{!}_L]
{\seq{\Gamma}{\Delta, {!}A, B \otimes C}{D}}
{\infer[{\otimes}_L]
 {\seq{\Gamma,A}{\Delta, B \otimes C}{D}}
 {\deduce{\seq{\Gamma,A}{\Delta, B, C}{D}}{\mathcal D}}}
\quad
\deduce{\mathstrut}{\mathstrut{\equiv}}
\quad
\infer[{\otimes}_L]
{\seq{\Gamma}{\Delta, {!}A, B \otimes C}{D}}
{\infer[{!}_L]
 {\seq{\Gamma}{\Delta, {!}A, B, C}{D}}
 {\deduce{\seq{\Gamma,A}{\Delta, B, C}{D}}{\mathcal D}}}
\]

In LF, lambda calculus terms (which correspond to derivations by the
Curry-Howard correspondence) are considered modulo the least
equivalence class that includes
\begin{itemize}
\item $\alpha$-equivalence ($\lambda x.N \equiv \lambda y.N[y/x]$ if 
$y \not\in {\it FV}(N)$), 
\item $\beta$-equivalence 
($(\lambda x.\,M)N \equiv M[N/x]$ if $x \not\in {\it FV}(N)$), and 
\item $\eta$-equivalence ($N \equiv \lambda x.N\,x$).
\end{itemize}
The weak normalization property for LF establishes that, given any
typed LF term, we can find an equivalent term that is $\beta$-normal
(no $\beta$-redexes of the form $(\lambda x.M) N$ exist) and
$\eta$-long (replacing $N$ with $\lambda x.N\,x$ anywhere would
introduce a $\beta$-redex or make the term ill-typed).  
In any given equivalence class of typed LF terms, all the
$\beta$-normal and $\eta$-long terms are $\alpha$-equivalent.
Therefore, because $\alpha$-equivalence is decidable, the equivalence
of typed LF terms is  decidable. 

The uniqueness of $\beta$-normal and $\eta$-long terms within an
equivalence class of lambda calculus terms (modulo
$\alpha$-equivalence, which we will henceforth take for granted) makes
these terms useful as canonical representatives of equivalence
classes. In Harper, Honsell, and Plotkin's original formulation
of LF, a deductive system is said to be {\it adequately encoded} as
an LF type family in the case that there is a compositional bijection
between the formal objects in the deductive system and these
$\beta$-normal, $\eta$-long representatives of equivalence classes
\cite{harper93framework}, a topic we will return to in 
Section~\ref{sec:lf-adequacy}.

Modern presentations of LF, such as Harper and Licata's
\cite{harper07mechanizing}, follow the approach developed by Watkins
et al.~\cite{watkins02concurrent} and define the logical framework so
that it only contains these $\beta$-normal, $\eta$-long {\it canonical
  forms} of LF. This presentation of LF is called Canonical LF to
distinguish it from the original presentation of LF in which the
$\beta$-normal, $\eta$-long terms are just a refinement of terms. A
central component in this approach is {\it hereditary substitution};
in Chapter~\ref{chapter-ordered}, we will make the connections between
hereditary substitution and the focused cut admissibility property we
prove in this chapter more explicit.  Hereditary substitution also
establishes a normalization property for LF. Using hereditary
substitution we can easily take a regular LF term and transform it
into a Canonical LF term. By a separate theorem, we can prove that the
normalized term will be equivalent to the original term
\cite{martens12lf}. %\footnote{This process is the same as the way we use cut
%  admissibility to prove cut elimination.} 
 % An important point is that the normalization theorem we prove
% this way is a strictly weaker theorem than so-called weak
% normalization -- it does not, a priori, imply that the .

Our analogue to the canonical forms of LF will be the {\it focused
  derivations} of linear logic that are presented in the next
section. In Section~\ref{sec:foclinlog} below, we will present 
focused linear logic and see that there is exactly 
one focused derivation that derives the proposition
\[{!}({\sf 6bucks} \lolli {\sf battery}) \otimes {\sf 6bucks} \otimes
({\sf battery} \lolli {\sf robot}) \lolli {\sf robot}.\] 
%
We will furthermore see that the structure of this derivation matches
the intuitive transition interpretation, a point
that is reinforced by the discussion of {\it synthetic inference
  rules} in Section~\ref{sec:linsynthetic}. 

\section{Focused linear logic}
\label{sec:foclinlog}

Andreoli's original motivation for introducing focusing was not to
describe a logical framework, it was to describe a foundational logic
programming paradigm based on proof search in classical linear logic
\cite{andreoli92logic}. The existence of multiple proofs that differ
in inessential ways is particularly problematic for proof search, as
inessential differences between derivations correspond to unnecessary
choice points that a proof search procedure will need to backtrack
over. 

The development in this section introduces {\it structural
  focalization}, a methodology for deriving the correctness of a
focused sequent calculus (Theorem~\ref{thm:linfocsound} and
Theorem~\ref{thm:linfoccomplete}, Section~\ref{sec:lincorrectness}) as
a consequence of the internal completeness (identity expansion,
Theorem~\ref{thm:linidentity}, Section~\ref{sec:linindentity}) and
internal soundness (cut admissibility, Theorem~\ref{thm:lincut},
Section~\ref{sec:lincut}) of the focused system. This
methodology is a substantial refinement of the method used by
Chaudhuri to establish the correctness of focused intuitionistic
linear logic \cite{chaudhuri06focused}, and because it relies on
structural methods, structural focalization is more amenable to
mechanized proof \cite{simmons11structural}.  Our focused sequent
calculus also departs from Chaudhuri's by treating asynchronous rules
as confluent rather than fixed, a point that will be discussed in
Section~\ref{sec:confluent-v-fixed}.

\subsection{Polarity}
\label{sec:linpolar}

The first step in describing a focused sequent calculus is to classify
connectives into two groups \cite{andreoli92logic}.  Some connectives,
such as linear implication $A \lolli B$, are called {\it asynchronous}
because their right rules can always be applied eagerly, without
backtracking, during bottom-up proof search. Other connectives, such
as multiplicative conjunction $A \tensor B$, are called {\it
  synchronous} because their right rules cannot be applied
eagerly. For instance, if we are trying to prove the sequent $A
\tensor B \longrightarrow B \tensor A$, the ${\tensor}R$ rule cannot
be applied eagerly; we first have to decompose $A \tensor B$ on the
left using the ${\tensor}L$ rule.  The terms asynchronous and
synchronous make a bit more sense in a one-sided classical sequent
calculus; in intuitionistic logics, it is common to call asynchronous
connectives {\it right}-asynchronous and {\it
  left}-synchronous. Similarly, it is common to call synchronous
connectives {\it right}-synchronous and {\it left}-asynchronous.  We
will instead use a different designation, calling the
(right-)synchronous connectives {\it positive} (${!}$, $\zero$, $\oplus$,
$\one$, and $\otimes$ in full propositional linear logic) and calling
the (right-)asynchronous connectives {\it negative} ($\lolli$, $\top$
and $\with$ in full propositional linear logic); this assignment is
called the proposition's {\it polarity}. Each atomic proposition must
be assigned to have only one polarity, though this assignment can be
made arbitrarily.

The nontrivial result of focusing is that it is possible to separate a
proof into two strictly alternating phases. In {\it inversion} phases,
positive propositions on the left and negative propositions on the
right are eagerly and exhaustively decomposed using invertible
rules.\footnote{Synchronicity or polarity, a property of connectives,
  is closely connected to (and sometimes conflated with) a property of
  rules called {\it invertibility}; a rule is invertible if the
  conclusion of the rule implies the premises. So ${\lolli}R$ is
  invertible ($\seq{\Gamma}{\Delta}{A \lolli B}$ implies
  $\seq{\Gamma}{\Delta, A}{B}$) but ${\lolli}L$ is not
  ($\seq{\Gamma}{\Delta, A \lolli B}{C}$ does not imply that $\Delta =
  \Delta_1, \Delta_2$ such that $\seq{\Gamma}{\Delta_1}{A}$ and
  $\seq{\Gamma}{\Delta_2, B}{C}$).  Rules that can be applied eagerly
  need to be invertible, so asynchronous connectives have invertible
  right rules and synchronous connectives have invertible left
  rules. Therefore, in the literature a common synonym for
  asynchronous/negative is {\it right-invertible}, and the analogous
  synonym for synchronous/positive is {\it left-invertible}.}  In {\it
  focused} phases, a single proposition is selected (the proposition
{\it in focus}, which is either a positive proposition in right focus
or a negative proposition in left focus). This
proposition is then decomposed repeatedly and
exhaustively using rules that are mostly non-invertible.

If we consider this discipline applied to our robot example where all
atoms have been assigned positive polarity, we would begin with an
inversion phase, decomposing the negative implication on the right and
the positive tensor and exponential on the left:
\[
\infer[{\lolli}_R]
{\seq{\cdot}{\cdot}{{!}({\sf 6bucks} \lolli {\sf battery}) \otimes
                    {\sf 6bucks} \otimes 
                    ({\sf battery} \lolli {\sf robot}) \lolli {\sf robot}}}
{\infer[{\otimes}_L]
{\seq{\cdot}{{!}({\sf 6bucks} \lolli {\sf battery}) \otimes
                    {\sf 6bucks} \otimes 
                    ({\sf battery} \lolli {\sf robot})}{{\sf robot}}}
{\infer[{!}_L]
{\seq{\cdot}{{!}({\sf 6bucks} \lolli {\sf battery}),
                    {\sf 6bucks} \otimes 
                    ({\sf battery} \lolli {\sf robot})}{{\sf robot}}}
{\infer[{\otimes}_L]
{\seq{{\sf 6bucks} \lolli {\sf battery}}{{\sf 6bucks} \otimes 
                    ({\sf battery} \lolli {\sf robot})}{{\sf robot}}}
{\deduce
{\seq{{\sf 6bucks} \lolli {\sf battery}}{{\sf 6bucks}, 
                    {\sf battery} \lolli {\sf robot}}{{\sf robot}}}
{\vdots}}}}}
\]
Once we reach the topmost sequent in the above fragment, we have to
pick a negative proposition on the left or a positive proposition on
the right as our focus in order to proceed. The correct choice in this
context is to pick the negative proposition ${\sf 6bucks} \lolli {\sf
  battery}$ in the persistent context and decompose it using the
non-invertible rule ${\lolli}_L$. Because the subformula ${\sf
  6bucks}$ is positive and ends up on the right side in the
subderivation, the focusing discipline requires that we prove it
immediately with the ${\it id}$ rule. Letting $\Gamma = {\sf 6bucks}
\lolli {\sf battery}$, this looks like this:
\[
\infer[{\it copy}]
{\deduce[\vdots]{}{\seq{\Gamma}{{\sf 6bucks}, 
                    {\sf battery} \lolli {\sf robot}}{{\sf robot}}}}
{\infer[{\lolli}_L]
{\seq{\Gamma}{{\sf 6bucks}, 
                    {\sf battery} \lolli {\sf robot}, 
   {\sf 6bucks} \lolli {\sf battery}}{{\sf robot}}}
{\infer[{\it id}]
 {\seq{\Gamma}{{\sf 6bucks}}{{\sf 6bucks}}}
 {}
 &
 \deduce
 {\seq{\Gamma}{{\sf battery} \lolli {\sf robot}, 
   {\sf battery}}{{\sf robot}}}
 {\vdots}}}
\]
The trace (that is, the pair of a single bottom sequent and 
a set of unproved top sequents) of an inversion phase stacked
on top of a
focused phase is called a {\it synthetic
  inference rule} by Chaudhuri, a point we will return to in
Section~\ref{sec:linsynthetic}.

\subsection{Polarization}

At this point, there is an important choice to make. One way forward
is to treat positive and negative propositions as syntactic
refinements of the set of all propositions, and to develop a focused
presentation for intuitionistic linear logic with the connectives and
propositions that we have already considered, as Chaudhuri did in
\cite{chaudhuri06focused}.  The other way forward is to treat positive
and negative propositions as distinct syntactic classes $A^+$ and
$A^-$ with explicit inclusions, called {\it shifts}, between them.
This is called {\it polarized} linear logic.  The positive proposition
${\downarrow}A^-$, pronounced ``downshift $A$'' or ``down $A$,'' has a
subterm that is a negative proposition; the negative proposition
${\uparrow}A^+$, pronounced ``upshift $A$'' or ``up $A$,'' has a
subterm that is a positive proposition.
\begin{align*}
A^+ & ::= p^+ \mid {\downarrow}A^- \mid {!}A^- \mid \one \mid A^+ \tensor B^+\\
A^- & ::= p^- \mid {\uparrow}A^+ \mid A^+ \lolli B^-
\end{align*}

\input{figs/fig-lin-shift}

The relationship between unpolarized and polarized linear logic is
given by two erasure functions $(A^+)^\circ$ and $(A^-)^\circ$ that
wipe away all the shifts; this function is defined in
Figure~\ref{fig:lin-shift}. In the other direction, every proposition
in unpolarized linear logic has an polarized analogue with a minimal
number of shifts, given by the functions $A^\oplus$ and $A^\ominus$
in Figure~\ref{fig:lin-shift}.  Both of these functions are partial
inverses of erasure, since $(A^\oplus)^\circ = (A^\ominus)^\circ = A$;
we will generally refer to partial inverses of erasure as {\it
  polarization strategies}. The strategies $A^\oplus$ and $A^\ominus$
are minimal, avoiding shifts wherever possible, but there are many
other possible strategies, such as the fully-shifting strategy that
always adds either one or two shifts between every connective, which
we can write as $(A)^{m+} = B^+$ and $(A)^{m-} = B^-$, defined in
Figure~\ref{fig:lin-maxshift}.

\begin{figure}
{\small \[
\begin{array}{rcl|rcl}
(p^+)^{m+} & \!\!\!=\!\!\! & {\downarrow}{\uparrow}p^+ &
(p^+)^{m-} & \!\!\!=\!\!\! & {\uparrow}p^+
\\
({!}A)^{m+} & \!\!\!=\!\!\! & {\downarrow}{\uparrow}{!}(A)^{m-} &
({!}A)^{m-} & \!\!\!=\!\!\! & {\uparrow}{!}(A)^{m-}
\\
(\one)^{m+} & \!\!\!=\!\!\! & {\downarrow}{\uparrow}\one &
(\one)^{m-} & \!\!\!=\!\!\! & {\uparrow}\one 
\\
(A \otimes B)^{m+} & \!\!\!=\!\!\!
    & {\downarrow}{\uparrow}((A)^{m+} \otimes (B)^{m+}) &
(A \otimes B)^{m-} & \!\!\!=\!\!\! & {\uparrow}(A^\oplus \otimes B^\oplus)
\\
& & & & & 
\\
(p^-)^{m+} & \!\!\!=\!\!\! & {\downarrow}p^- &
(p^-)^{m-} & \!\!\!=\!\!\! & {\uparrow}{\downarrow}p^- 
\\
(A \lolli B)^{m+} & \!\!\!=\!\!\! & {\downarrow}((A)^{m+} \lolli (B)^{m-}) &
(A \lolli B)^{m-} & \!\!\!=\!\!\!
     & {\uparrow}{\downarrow}((A)^{m+} \lolli (B)^{m-})
\end{array}
\]}

\caption{Fully-shifting polarization strategy for MELL}
\label{fig:lin-maxshift}
\end{figure}

Shifts turn out to have a profound impact on the structure of focused
proofs, though erasure requires that they have no impact on {\it
  provability}. For instance, the proofs of $A$ in Chaudhuri's focused
presentation of linear logic are isomorphic to the proofs of
$(A)^\oplus$ in the polarized logic discussed below,\footnote{This
  isomorphism holds for Chaudhuri's focused presentation of linear
  logic precisely because his treatment of atomic propositions differs
  from Andreoli's. This isomorphism does not hold relative to focused
  systems that follow Andreoli's design, a point we will return to in
  Section~\ref{sec:linhack}.} whereas the proofs of $(A)^{m+}$ in
polarized logic are isomorphic to the {\it unfocused} proofs of linear
logic as described in Figure~\ref{fig:linear}. Other polarization
strategies correspond to different focused logics, as explored by
Liang and Miller in \cite{liang09focusing}, so the presentation of
polarized linear logic below, like Liang and Miller's LJF, can be seen
in two ways: as a focused logic in its own right, and as a framework
for defining many focused logics (one per polarization strategy). As
such, the strongest statement of the correctness of focusing is based
on erasure: there is an unfocused derivation of $(A^+)^\circ$ or
$(A^-)^\circ$ if and only if there is a focused derivation of $A^+$ or
$A^-$.  Most existing proofs of the completeness of focusing only
verify a weaker property: that there is an unfocused derivation of $A$
if and only if there is a focused derivation of $A^\bullet$, where
$A^\bullet$ is some polarization strategy.  The only exception seems
to be Zeilberger's proof for classical persistent logic
\cite{zeilberger08unity}.

In this dissertation, we will be interested only in the structure of focused
proofs, which corresponds to using the polarization strategy given by
$A^\oplus$ and $A^\ominus$. Therefore, following Chaudhuri, it would
be possible to achieve our objectives without the use of polarization.
Our choice is largely based on practical
considerations: the use of polarized logic simplifies the proof of
identity expansion in Section~\ref{sec:linindentity} and the proof of
completeness in Section~\ref{sec:lincorrectness}. That said, polarized
logic is an independently significant and currently active area of
research. For instance, the Curry-Howard
interpretation of polarized persistent logic has been studied by Levy
as Call-by-Push-Value \cite{levy04call}. The erasable influence of the
shifts on the structure (but not the existence) of proofs is also
important in the context of theorem proving. For instance, a theorem
prover for polarized logic can imitate focused proof search by using
the $(A)^\oplus$ polarization strategy and unfocused proof search by
using the $(A)^{m+}$ polarization strategy
\cite{mclaughlin09efficient}.

\subsection{Focused sequent calculus}
\label{sec:linfocseqcalcdef}

Usually, focused logics are described as having multiple sequent
forms. For intuitionistic logics, there need to be at least three
sequent forms:
\smallskip
\begin{itemize}
\item $\mildrfoc{\Gamma}{\Delta}{A^+}$ (the {\it right focus} sequent, where
the proposition $A^+$ is in focus),
\item $\mildinv{\Gamma}{\Delta}{C}$ (the {\it inversion} sequent), and
\item $\mildlfoc{\Gamma}{\Delta}{A^-}{C}$ (the {\it left focus} sequent,
where the proposition $A^-$ is in focus).
\end{itemize}
\smallskip
It is also possible to distinguish a fourth sequent form, the {\it
  stable} sequents, inversion sequents $\mildinv{\Gamma}{\Delta}{C}$
where no asynchronous inversion remains to be done. A sufficient
condition for stability is that the context $\Delta$ contains only negative 
propositions
$A^-$ and the succedent $C$ is a positive proposition $A^+$.
However, this cannot be a {\it necessary} condition for stability
due to the presence of atomic propositions. 
If the process of inversion reaches a positive atomic
proposition $p^+$ on the left or a negative atomic proposition $p^-$
on the right, the proposition can be decomposed no further. When we
reach an atomic proposition, we are therefore forced to {\it suspend}
decomposition, either placing a suspended positive atomic proposition
$\susp{p^+}$ in $\Delta$ or placing a suspended negative proposition
$\susp{p^-}$ as the succedent. For technical reasons discussed below
in Section~\ref{sec:lin-suspended}, our sequent calculus can handle
arbitrary suspended propositions, not just  suspended atomic
propositions, and suspended propositions are always treated as stable,
so $\mildseq{\Gamma}{A^-, B^-, C^-}{D^+}$ and
$\mildseq{\Gamma}{\susp{A^+}, B^-, \susp{C^+}}{\susp{D^-}}$ are both
stable sequents.

Another reasonable presentation of linear logic, and the one we will
adopt in this section, uses only one sequent form,
$\mildseq{\Gamma}{\underline{\Delta}}{\underline{U}}$, that
generalizes what is allowed to appear in the linear context
$\underline{\Delta}$ or in the succedent $\underline{U}$. We will use
this interpretation to understand the logic described in
Figure~\ref{fig:kaustuv-focused}. In addition to propositions $A^+$,
$A^-$ and positive suspended positive propositions $\langle A^+ \rangle$, 
the grammar
of contexts $\underline{\Delta}$ allows them to contain left focuses
$[ A^- ]$.  Likewise, a succedent $\underline{U}$ can be a stable
positive proposition $A^+$, a suspended negative proposition
$\susp{A^-}$, a focused positive proposition $[ A^+ ]$, or an
inverting negative proposition $A^-$. We will henceforth write
$\Delta$ and $U$ to indicate the refinements of $\underline{\Delta}$
and $\underline{U}$ that do not contain any focus.

\input{figs/fig-kaustuv-focused.tex}

By adding a side condition to the three rules ${\it focus}_R$, ${\it
  focus}_L$, and ${\it copy}$ that neither the context $\Delta$ nor
the succedent $U$ can contain an in-focus proposition $[A^+]$ or
$[A^-]$, derivations can maintain the invariant that there is always
at most one proposition in focus in any sequent, effectively restoring
the situation in which there are three distinct
judgments. %\footnote{Treating these extra conditions as a side
%  condition is strictly a cosmetic matter; we don't want to have to
%  write them constantly when we .}  
Therefore, from this point on, we will only consider sequents
$\mildseq{\Gamma}{\underline{\Delta}}{\underline{U}}$ with at most one
focus. Pfenning, who developed this construction in
\cite{pfenning12chaining}, calls this invariant the {\it focusing
  constraint}.
% and writes $\delta$ and $\gamma$ instead of $\underline{\Delta}$ and
% $\underline{U}$.
The focusing constraint alone gives us what Pfenning calls a {\it
  chaining} logic \cite{pfenning12chaining} and which Laurent calls a
{\it weakly focused} logic
\cite{laurent04proof}.\footnote{Unfortunately, I made the meaning of
  ``weak focusing'' less precise by calling a different sort of logic
  weakly focused in \cite{simmons11weak}.  That weakly focused system
  had an additional restriction that invertible rules could {\it not}
  be applied when any other proposition was in focus, which is what
  Laurent called a strongly $+$-focused logic.}  We obtain a fully
focused logic by further restricting the three critical rules ${\it
  focus}_R$, ${\it focus}_L$, and ${\it copy}$ so that they only apply
when the sequent below the line is stable. In light of this
additional restriction, whenever we consider a focused sequent
$\mildseq{\Gamma}{\Delta, [A^-]}{U}$ or
$\mildseq{\Gamma}{\Delta}{[A^+]}$, we can assume that $\Delta$ and $U$
are stable.

The persistent context of a focused derivation can always be weakened
by adding more persistent resources.  This weakening property can be
phrased as an admissible rule, which we indicate using a dashed line:
\[
\infer-[\it weaken]
{\mildseq{\Gamma, \Gamma'}{\underline \Delta}{\underline U}}
{\mildseq{\Gamma}{\underline \Delta}{\underline U}}
\]
In developments following Pfenning's structural cut admissibility
methodology \cite{pfenning00structural}, it is critical that the
weakening theorem {\it does not} change the structure of proofs: that the
structure of the derivation $\mildseq{\Gamma}{\underline
  \Delta}{\underline U}$ is unchanged when we weaken it to
$\mildseq{\Gamma, \Gamma'}{\underline \Delta}{\underline U}$. It turns
out that the development in this chapter does not rely on this property.
% In
% contrast, the Structural Focalization development that this chapter
% follows does not rely on this property \cite{simmons11structural}.

Suspended propositions ($\langle A^+ \rangle$ and $\langle A^-
\rangle$) and the four rules that interact with suspended propositions
(${\it id}^+$, ${\it id}^-$, $\eta^+$, and $\eta^-$) are the main
nonstandard aspect of this presentation.  The $\eta^+$ and $\eta^-$
rules, which allow us to stop decomposing a proposition that we are
eagerly decomposing with invertible rules, are restricted to atomic
propositions, and there is no other way for suspended propositions to
be introduced into the context with rules. It seems reasonable to
restrict the two rules that capture the identity principles, ${\it
  id}^+$ and ${\it id}^-$, to atomic propositions as well.  However,
the seemingly unnecessary generality of these two identity rules makes
it much easier to establish the standard metatheory of this sequent
calculus. To see why this is the case, we will turn our attention to
suspended propositions and the four admissible rules (two focal
substitution principles and two identity expansion principles) that
interact with suspended propositions.


\subsection{Suspended propositions}
\label{sec:lin-suspended}

In unfocused sequent calculi, it is generally possible to restrict the
${\it id}$ rule to atomic propositions (as shown in
Figure~\ref{fig:linear}). The general ${\it id}$ rule,
which concludes $\seq{\Gamma}{A}{A}$ for all propositions $A$, is
admissible just as the ${\it cut}$ rule is admissible. But while the
${\it cut}$ rule can be eliminated completely, the atomic ${\it id}$
rule must remain. This is related to the logical interpretation of
atomic propositions as stand-ins for unknown propositions.  All
sequent calculi, focused or unfocused, have the subformula property:
every rule breaks down a proposition, either on the left or the right
of the turnstile ``$\vdash$'', when read from bottom to top. We are
unable to break down atomic propositions any further (they are
unknown), thus the ${\it id}$ rule is necessary at atomic
propositions.  If we substitute a concrete proposition for some atomic
proposition, the structure of the proof stays exactly the same, except
that instances of initial sequents become admissible instances of the
identity theorem.

To my knowledge, all published proof systems for focused logic have
incorporated a focused version of the ${\it id}$ rule that also
applies only to atomic propositions. This treatment is not incorrect
and is obviously analogous to the ${\it id}$ rule from the unfocused
system. Nevertheless, I believe this to be a design error, and it is
one that has historically made it unnecessarily difficult to prove the
identity theorem for focused systems. The alternative developed in
this chapter is the use of suspensions. Suspended positive
propositions $\langle A^+ \rangle$ only appear in the linear context
$\Delta$, and suspended negative propositions $\langle A^- \rangle$
only appear as succedents. They are treated as stable (we never break
down a suspended proposition) and are only used to immediately prove a
proposition in focus with one of the identity rules ${\it id}^+$ or
${\it id}^-$. The rules ${\it id}^+$ and ${\it id}^-$ are more general
focused versions of the unfocused ${\it id}$ rule. This extra
generality does not influence the structure of proofs because
suspended propositions can only be introduced into the context or the
succedent by the $\eta^+$ and $\eta^-$ rules, and those rules {\it
  are} restricted to atomic propositions.
% The restriction of these
% identity rules to suspended propositions allows us to maintain the
% structure of the logic, but the generalization of the identity rules
% to arbitrary suspended propositions g.

\paragraph{Suspended positive propositions} act much like regular variables in a
natural deduction system. The positive identity rule ${\it id}^+$
allows us to prove any positive proposition given that the positive
proposition appears suspended in the context.  There is a
corresponding substitution principle for focal substitutions that has
a natural-deduction-like flavor: we can substitute a derivation
right-focused on $A^+$ for a suspended positive proposition $\langle
A^+ \rangle$ in a context.

\bigskip
\begin{theorem}[Focal substitution (positive)]\label{thm:fsubst-pos}~\\
If $\mildseq{\Gamma}{\Delta}{[A^+]}$ 
and $\mildseq{\Gamma}{\underline{\Delta'}, \langle A^+ \rangle}
      {\underline{U}}$, 
then $\mildseq{\Gamma}{\underline{\Delta'}, \Delta}{\underline{U}}$.
\end{theorem}

\begin{proof}
  Straightforward induction over the second given derivation, as in a
  proof of regular substitution in a natural deduction system. If the
  second derivation is the axiom ${\it id}^+$, the result follows
  immediately using the first given derivation.
\end{proof}

\noindent
As discussed above in Section~\ref{sec:linfocseqcalcdef}, because we
only consider focused sequents that are otherwise stable, we assume
that $\Delta$ in the statement of Theorem~\ref{thm:fsubst-pos} is
stable by virtue of it appearing in the focused sequent
$\mildseq{\Gamma}{\Delta}{[A^+]}$. The second premise
$\mildseq{\Gamma}{\underline{\Delta'}, \langle A^+
  \rangle}{\underline{U}}$, on the other hand, may be a right-focused
sequent $\mildseq{\Gamma}{\Delta', \langle A^+ \rangle}{[B^+]}$, a
left-focused sequent $\mildseq{\Gamma}{\Delta'', [B^-], \langle A^+
  \rangle}{U}$, or an inverting sequent.

\paragraph{Suspended negative propositions} are a bit less intuitive
than suspended positive propositions. While a derivation of
$\mildseq{\Gamma}{\underline{\Delta'}, \langle A^+
  \rangle}{\underline{U}}$ is missing a premise that can be satisfied
by a derivation of $\mildseq{\Gamma}{\Delta}{[A^+]}$, a derivation of
$\mildseq{\Gamma}{\underline{\Delta}}{\langle A^- \rangle}$ is missing
a {\it continuation} that can be satisfied by a derivation of
$\mildseq{\Gamma}{\Delta', [A^-]}{U}$. The focal substitution
principle, however, still takes the basic form of a substitution
principle.

\bigskip
\begin{theorem}[Focal substitution (negative)]\label{thm:fsubst-neg}~\\
If $\mildseq{\Gamma}{\underline{\Delta}}{\langle A^- \rangle}$
and $\mildseq{\Gamma}{\Delta', [A^-]}{U}$, 
then $\mildseq{\Gamma}{\Delta', \underline{\Delta}}{U}$. 
\end{theorem}

\begin{proof}
  Straightforward induction over the {\it first} given derivation; if
  the first derivation is the axiom ${\it id}^-$, the result follows
  immediately using the second given derivation.
\end{proof}

% \noindent
% As a regular substitution principle that is inductive over the structure
% of the first given proposition, focal substitution is reminiscent of 
% the {\it leftist substitutions} introduced by Pfenning and Davies in the 
% context of the possibility modality \cite{pfenning01judgmental}.

Unlike cut admissibility, which we discuss in
Section~\ref{sec:lincut}, both of the focal substitution principles
are straightforward inductions over the structure of the derivation
containing the suspended proposition. As an aside, when we encode
%The Structural Focalization
%development \cite{simmons11structural} discusses how, when we encode
the focused sequent calculus for persistent logic in LF, a suspended
positive premise can be naturally encoded as a hypothetical right
focus. This encoding makes the ${\it id}^+$ rule an instance of the
hypothesis rule provided by LF and establishes
Theorem~\ref{thm:fsubst-pos} ``for free'' as an instance of LF
substitution. This is possible to do for negative focal substitution
as well, but it is counterintuitive and relies on a peculiar use of
LF's uniform function space \cite{simmons11structural}.

The two substitution principles can be phrased as admissible rules for
building derivations, like the ${\it weaken}$ rule above:
\[
\infer-[{\it subst}^+]
{\mildseq{\Gamma}{\underline{\Delta'}, \Delta}{\underline{U}}}
{\mildseq{\Gamma}{\Delta}{[A^+]}
 &
 \mildseq{\Gamma}{\underline{\Delta'}, \langle A^+ \rangle}{\underline{U}}}
\qquad
\infer-[{\it subst}^-]
{\mildseq{\Gamma}{\Delta', \underline{\Delta}}{U}}
{\mildseq{\Gamma}{\underline{\Delta}}{\langle A^- \rangle}
 &
 \mildseq{\Gamma}{\Delta', [A^-]}{U}}
\]

Note the way in which these admissible substitution principles
generalize the logic: ${\it subst}^+$ or ${\it subst}^-$ are the
only rules we have discussed that allow us to introduce non-atomic
suspended propositions, because only {\it atomic} suspended propositions are
introduced explicitly by rules $\eta^+$ and $\eta^-$.

\subsection{Identity expansion}
\label{sec:linindentity}

Suspended propositions appear in Figure~\ref{fig:kaustuv-focused} in
two places: in the identity rules, which we have just discussed and
connected with the focal substitution principles, and in the rules
marked $\eta^+$ and $\eta^-$, which are also the only mention of
atomic propositions in the presentation. It is here that we need to
make a critical shift of perspective from unfocused to focused
logic. In an unfocused logic, the rules nondeterministically break
down propositions, and the initial rule ${\it id}$ puts an end to this
process when an atomic proposition is reached. In a focused logic, the
focus and inversion phases must break down a proposition {\it all the
  way} until a shift is reached. The two $\eta$ rules are what put an
end to this when an atomic proposition is reached, and they work
hand-in-glove with the two ${\it id}$ rules that allow these
necessarily suspended propositions to successfully conclude a right or
left focus.

\input{figs/fig-lineta-1}
\input{figs/fig-lineta-2}

Just as the ${\it id}$ rule is a particular instance of the admissible
identity sequent $\seq{\Gamma}{A}{A}$ in unfocused linear logic, the
atomic suspension rules $\eta^+$ and $\eta^-$ are instances of an admissible
{\it identity expansion} rule in focused linear logic:
\[
\infer-[\eta^+]
{\mildseq{\Gamma}{\Delta, A^+}{U}}
{\mildseq{\Gamma}{\Delta, \langle A^+ \rangle}{U}}
\qquad
\infer-[\eta^-]
{\mildseq{\Gamma}{\Delta}{A^-}}
{\mildseq{\Gamma}{\Delta}{\langle A^- \rangle}}
\]
In other words, the admissible identity expansion rules allow us to
act as if the $\eta^+$ and $\eta^-$ rules apply to {\it arbitrary}
propositions, not just atomic propositions. The atomic propositions
must be handled by an explicit rule, but the general principle is 
admissible.

The two admissible identity expansion rules above can be rephrased
as an identity expansion theorem:

\bigskip
\begin{theorem}[Identity expansion]~\label{thm:linidentity}
\begin{itemize}
\item 
If $\mildseq{\Gamma}{\Delta, \langle A^+ \rangle}{U}$, 
then $\mildseq{\Gamma}{\Delta, A^+}{U}$.
\item
If $\mildseq{\Gamma}{\Delta}{\langle A^- \rangle}$, 
then $\mildseq{\Gamma}{\Delta}{A^-}$.
\end{itemize}
\end{theorem}

\begin{proof}
Mutual induction over the structure of the proposition $A^+$ or $A^-$,
with a critical use of focal substitution in each case.

Most of the cases of this proof are represented in
Figure~\ref{fig:lineta-1}. The remaining case (for the multiplicative
unit $\one$) is presented in Figure~\ref{fig:lineta-2} along with the
cases for the additive connectives $\zero$, $\oplus$, $\top$, and
$\with$, which are neglected elsewhere in this chapter. (Note that in
Figures~\ref{fig:lineta-1}~and~\ref{fig:lineta-2} we omit polarity
annotations from propositions as they are always clear from the
context.)
\end{proof}

The admissible identity expansion rules fit with an interpretation of
positive atomic propositions as stand-ins for arbitrary positive
propositions and of negative atomic propositions as stand-ins for
negative atomic propositions: if we substitute a proposition for
some atomic proposition, all the instances of atomic suspension
corresponding to that rule become admissible instances of identity
expansion. 

The usual identity principles are 
corollaries of identity expansion:
\[
\infer-[\eta^+]
{\mildseq{\Gamma}{A^+}{A^+}}
{\infer[{\it focus}_R]
 {\mildseq{\Gamma}{\langle A^+ \rangle}{A^+}}
 {\infer[{\it id}^+]
  {\mildseq{\Gamma}{\langle A^+ \rangle}{[A^+]}}
  {}}}
\qquad
\infer-[\eta^-]
{\mildseq{\Gamma}{A^-}{A^-}}
{\infer[{\it focus}_L]
 {\mildseq{\Gamma}{A^-}{\langle A^- \rangle}}
 {\infer[{\it id}^-]
  {\mildseq{\Gamma}{[A^-]}{\langle A^- \rangle}}
  {}}}
\]


\subsection{Cut admissibility}
\label{sec:lincut}

Cut admissibility, Theorem~\ref{thm:lincut} below, mostly follows the
well-worn contours of a structural cut admissibility argument
\cite{pfenning00structural}. A slight inelegance of the proof given
here is that some very similar cases must be considered more than once
in different parts of the proof. The right commutative cases -- cases
in which the last rule in the second given derivation is an invertible
rule that is not decomposing the principal cut formula $A^+$ -- must
be repeated in parts 1 and 4, for instance. (Pfenning's classification
of the cases of cut admissibility into principal, left commutative,
and right commutative cuts is discussed in Section~\ref{sec:ord-cut}.)
In addition to this duplication, the proof of part 4 is almost
identical in form to the proof of part 5. The proof of cut
admissibility in the next chapter will eliminate both forms of
duplication.

% The most notable exception is that we can
%  so we defer a full discussion of cut
% admissibility until the next chapter, where we will give a tidier
% proof by incorporating more of the machinery of the Structural
% Focalization development.\footnote{The main ``untidy'' aspect of
%   Theorem~\ref{thm:lincut} is that the lack of a fixed inversion
%   order means that the right commutative cases -- cases in which the
%   last rule in the second given derivation is an invertible rule that
%   is not decomposing the principal cut formula $A^+$ -- must
%   be repeated in parts 1 and 4. (Pfenning's classification of the
%   cases of cut admissibility into principal, left commutative,
%   and right commutative cuts is discussed in Section~\ref{sec:ord-cut}.)
%
%   In addition to this duplication, the proof of part 4 is almost
%   identical in form to the proof of part 5. The proof in the 
%   next chapter will avoid both forms of duplication.}
% %

The most important caveat about 
cut admissibility is that it is only applicable in the absence of any
non-atomic suspended propositions. If we did not make this
restriction, then in Theorem~\ref{thm:lincut}, part 1, we might encounter
a derivation of $\mildseq{\Gamma}{\langle A \tensor B \rangle}{[ A \tensor B ]}$
that concludes with ${\it id}^+$ being cut into the derivation
\[
\infer[{\otimes}_R]
{\mildseq{\Gamma}{\Delta',A \tensor B}{U}}
{\deduce{\mildseq{\Gamma}{\Delta', A, B}{U}}{\mathcal E}}
\]
in which case there is no clear way to proceed and prove 
$\mildseq{\Gamma}{\Delta', \langle A \tensor B \rangle}{U}$. 

\bigskip
\begin{theorem}[Cut admissibility]\label{thm:lincut}
For all $\Gamma$, $A^+$, $A^-$, $\Delta$, $\Delta'$, and $U$ that
do not contain any non-atomic suspended propositions:
\begin{enumerate}
\item If $\mildseq{\Gamma}{\Delta}{[A^+]}$
      and $\mildseq{\Gamma}{\Delta',A^+}{U}$
      (where $\Delta$ is stable), 
      then $\mildseq{\Gamma}{\Delta',\Delta}{U}$.
\item If $\mildseq{\Gamma}{\Delta}{A^-}$
      and $\mildseq{\Gamma}{\Delta', [A^-]}{U}$
      (where $\Delta$, $\Delta'$, and $U$ are stable),
      then $\mildseq{\Gamma}{\Delta',\Delta}{U}$. 
\item If $\mildseq{\Gamma}{\underline{\Delta}}{A^+}$
      and $\mildseq{\Gamma}{\Delta', A^+}{U}$,
      (where $\Delta'$ and $U$ are stable),
      then $\mildseq{\Gamma}{\Delta',\underline{\Delta}}{U}$. 
\item If $\mildseq{\Gamma}{\Delta}{A^-}$
      and $\mildseq{\Gamma}{\underline{\Delta'}, A^-}{\underline{U}}$,
      (where $\Delta$ is stable),
      then $\mildseq{\Gamma}{\underline{\Delta'},\Delta}{\underline{U}}$. 
\item If $\mildseq{\Gamma}{\cdot}{A^-}$
      and $\mildseq{\Gamma, A^-}{\underline{\Delta'}}{\underline{U}}$,
      then $\mildseq{\Gamma}{\underline{\Delta'}}{\underline{U}}$. 
\end{enumerate}
\end{theorem}
\bigskip

\noindent
Parts 1 and 2 are where most of the action happens, but there is a
sense in which the {\it necessary} cut admissibility property is
contained in structure of parts 3, 4, and 5 -- these are the cases
used to prove the completeness of focusing
(Theorem~\ref{thm:linfoccomplete}). The discrepancy between the
stability restrictions demanded for part 1 and part 2 is discussed
below; this peculiarity is justified by the fact that these two parts
need only be general enough to prove parts 3, 4, and 5.

\begin{proof}
  The proof is by induction: in each invocation of the induction
  hypothesis, either the principal cut formula $A^+$ or $A^-$ gets
  smaller or else it stays the same and the ``part size'' (1-5) gets
  smaller. When the principal cut formula and the part size remain the
  same, either the first given derivation gets smaller (part 3)
  or the second given derivation gets smaller (parts 1, 4 and 5).

  %As noted in \cite{simmons11structural}, 
  This termination argument is a refinement of the standard structural
  termination argument for cut admissibility in unfocused logics
  \cite{pfenning00structural} -- in part 3, we don't need to know that
  the second given derivation stays the same size, and in parts 1, 4,
  and 5 we don't need to know that the first given derivation stays
  the same size. This refined termination argument is the reason that
  we do not need to prove that admissible weakening preserves the
  structure of proofs.
 
  We schematically present one or two illustrative cases for each part
  of the proof.

  \subsubsection{Part 1 (positive principal cuts, right commutative cuts)}
  {\small \noindent($\Delta_1$, $\Delta_2$ stable are stable by assumption) \[
  \infer-[\it cut (1)]
  {\mildseq{\Gamma}{\Delta', \Delta_1, \Delta_2}{U}}
  {\infer[{\otimes}_R]
   {\mildseq{\Gamma}{\Delta_1, \Delta_2}{[A_1^+ \otimes A_2^+]}}
   {\deduce{\mildseq{\Gamma}{\Delta_1}{[A_1^+]}}{\mathcal D_1}
    &
    \deduce{\mildseq{\Gamma}{\Delta_2}{[A_2^+]}}{\mathcal D_2}}
  &
   \infer[{\otimes}_L]
   {\mildseq{\Gamma}{\Delta', A_1^+ \otimes A_2^+}{U}}
   {\deduce{\mildseq{\Gamma}{\Delta', A_1^+, A_2^+}{U}}{\mathcal E'}}}
  \qquad\qquad\qquad\qquad\qquad\qquad\qquad
  \]\vspace{-20pt}\[
  \qquad\qquad\quad\qquad\qquad\qquad\qquad
  \deduce{\mathstrut}{\Longrightarrow} \qquad
  \infer-[\it cut (1)]
  {\mildseq{\Gamma}{\Delta', \Delta_1, \Delta_2}{U}}
  {\deduce{\mildseq{\Gamma}{\Delta_1}{[A_1^+]}}{\mathcal D_1}
   & 
   \infer-[\it cut (1)]
   {\mildseq{\Gamma}{\Delta', A_1^+, \Delta_2}{U}}
   {\deduce{\mildseq{\Gamma}{\Delta_2}{[A_2^+]}}{\mathcal D_2}
    &
    \deduce{\mildseq{\Gamma}{\Delta', A_1^+, A_2^+}{U}}{\mathcal E'}}}
  \]}

  {\small \noindent($\Delta$ is stable by assumption) \[
  \infer-[\it cut (1)]
  {\mildseq{\Gamma}{\Delta', B_1^+ \otimes B_2^+, \Delta}{U}}
  {\deduce{\mildseq{\Gamma}{\Delta}{[A^+]}}{\mathcal D}
  &
   \infer[{\otimes}_L]
   {\mildseq{\Gamma}{\Delta', B_1^+ \otimes B_2^+, A^+}{U}}
   {\deduce{\mildseq{\Gamma}{\Delta', B_1^+, B_2^+, A^+}{U}}{\mathcal E'}}}
  \quad   
  \deduce{\mathstrut}{\Longrightarrow}
  \hspace{-10pt} 
  \infer[{\otimes}_L]
  {\mildseq{\Gamma}{\Delta', B_1^+ \otimes B_2^+, \Delta}{U}}
  {\infer-[\it cut (1)]
   {\mildseq{\Gamma}{\Delta', B_1^+, B_2^+, \Delta}{U}}
   {\deduce{\mildseq{\Gamma}{\Delta}{[A^+]}}{\mathcal D}
    &
    \deduce{\mildseq{\Gamma}{\Delta', B_1^+, B_2^+, A^+}{U}}{\mathcal E'}}}
  \]}

  \subsubsection{Part 2 (negative principal cuts)}

  {\small \noindent ($\Delta$, $\Delta'$, $\Delta'_A$, and $U$ are stable
   by assumption)
  \[
  \infer-[\it cut (2)]
  {\mildseq{\Gamma}{\Delta', \Delta'_A, \Delta}{U}}
  {\infer[{\lolli}_R]
   {\mildseq{\Gamma}{\Delta}{A_1^+ \lolli A_2^-}}
   {\deduce{\mildseq{\Gamma}{\Delta, A_1^+}{A_2^-}}{\mathcal D'}}
   &
   \infer[{\lolli}_L]
   {\mildseq{\Gamma}{\Delta', \Delta_A', [A_1^+ \lolli A_2^-]}{U}}
   {\deduce{\mildseq{\Gamma}{\Delta_A'}{[A_1^+]}}{\mathcal E_1}
    &
    \deduce{\mildseq{\Gamma}{\Delta', [A_2^-]}{U}}{\mathcal E_2}}}
  \qquad\qquad\qquad\qquad\qquad\qquad\qquad
  \]\vspace{-20pt}\[
  \qquad\qquad\qquad\qquad\qquad\qquad
  \deduce{\mathstrut}{\Longrightarrow}
  \hspace{-5pt}
  \infer-[\it cut(2)]
  {\mildseq{\Gamma}{\Delta', \Delta'_A, \Delta}{U}}
  {\infer-[\it cut(1)]
   {\mildseq{\Gamma}{\Delta'_A, \Delta}{A_2^-}}
   {\deduce{\mildseq{\Gamma}{\Delta_A'}{[A_1^+]}}{\mathcal E_1}
    &
    \deduce{\mildseq{\Gamma}{\Delta, A_1^+}{A_2^-}}{\mathcal D'}}
   &
   \deduce{\mildseq{\Gamma}{\Delta', [A_2^-]}{U}}{\mathcal E_2}}  
  \]}

  \subsubsection{Part 3 (left commutative cuts)}

  {\small \noindent ($\Delta'$ and $U$ are stable by assumption,
    $\Delta$ is stable by the side condition on rule ${\it focus}_R$)
  \[
  \infer-[\it cut(3)]
  {\mildseq{\Gamma}{\Delta', \Delta}{U}}
  {\infer[{\it focus}_R]
   {\mildseq{\Gamma}{\Delta}{A^+}}
   {\deduce{\mildseq{\Gamma}{\Delta}{[A^+]}}{\mathcal D'}}
   &
   \deduce{\mildseq{\Gamma}{\Delta', A^+}{U}}{\mathcal E}}
  \qquad
  \deduce{\mathstrut}{\Longrightarrow}
  \qquad
  \infer-[\it cut (1)]
  {\mildseq{\Gamma}{\Delta', \Delta}{U}}
  {\deduce{\mildseq{\Gamma}{\Delta}{[A^+]}}{\mathcal D'}
   &
   \deduce{\mildseq{\Gamma}{\Delta', A^+}{U}}{\mathcal E}}
  \]}

  {\small \noindent ($\Delta'$ and $U$ are stable by assumption)
  \[
  \infer-[\it cut (3)]
  {\mildseq{\Gamma}{\Delta', \Delta, B_1^+ \otimes B_2^+}{A^+}}
  {\infer[{\otimes}_L]
   {\mildseq{\Gamma}{\Delta, B_1^+ \otimes B_2^+}{A^+}}
   {\deduce{\mildseq{\Gamma}{\Delta, B_1^+, B_2^+}{A^+}}{\mathcal D'}}
   &
   \deduce{\mildseq{\Gamma}{\Delta', A^+}{U}}{\mathcal E}}
  \quad
  \deduce{\mathstrut}{\Longrightarrow}
  \hspace{-15pt}
  \infer[{\otimes}_L]
  {\mildseq{\Gamma}{\Delta', \Delta, B_1^+ \otimes B_2^+}{A^+}}
  {\infer-[\it cut(3)]
   {\mildseq{\Gamma}{\Delta', \Delta, B_1^+, B_2^+}{A^+}}
   {\deduce{\mildseq{\Gamma}{\Delta, B_1^+, B_2^+}{A^+}}{\mathcal D'}
    &
    \deduce{\mildseq{\Gamma}{\Delta', A^+}{U}}{\mathcal E}}}
  \]}

  \subsubsection{Part 4 (right commutative cuts)}

  {\small \noindent ($\Delta$ is stable by assumption, $\Delta'$ and
    $U$ are stable by the side condition on rule ${\it focus}_R$)
  \[
  \infer-[\it cut (4)]
  {\mildseq{\Gamma}{\Delta', \Delta}{U}}
  {\deduce{\mildseq{\Gamma}{\Delta}{A^-}}{\mathcal D}
   &
   \infer[{\it focus}_R]
   {\mildseq{\Gamma}{\Delta', A^-}{U}}
   {\deduce{\mildseq{\Gamma}{\Delta', [A^-]}{U}}{\mathcal E'}}}
  \qquad
  \deduce{\mathstrut}{\Longrightarrow}
  \qquad
  \infer-[\it cut (2)]
  {\mildseq{\Gamma}{\Delta', \Delta}{U}}
  {\deduce{\mildseq{\Gamma}{\Delta}{A^-}}{\mathcal D}
   &
   \deduce{\mildseq{\Gamma}{\Delta', [A^-]}{U}}{\mathcal E'}}
  \]}

  \subsubsection{Part 5 (persistent right commutative cuts)}

  {\small 
  \[
  \infer-[\it cut (5)]
  {\mildseq{\Gamma}{\cdot}{[{!}B^-]}}
  {\deduce{\mildseq{\Gamma}{\cdot}{A^-}}{\mathcal D}
   &
   \infer[{!}_R]
   {\mildseq{\Gamma, A^-}{\cdot}{[{!}B^-]}}
   {\deduce{\mildseq{\Gamma, A^-}{\cdot}{B^-}}{\mathcal E'}}}
  \qquad
  \deduce{\mathstrut}{\Longrightarrow}
  \infer[{!}_R]
  {\mildseq{\Gamma}{\cdot}{[{!}B^-]}}
  {\infer-[\it cut(5)]
   {\mildseq{\Gamma}{\cdot}{B^-}}
   {\deduce{\mildseq{\Gamma}{\cdot}{A^-}}{\mathcal D}
    &
    \deduce{\mildseq{\Gamma, A^-}{\cdot}{B^-}}{\mathcal E'}}}
  \]}

\noindent
All the other cases follow the same pattern.
\end{proof}

% Each of these cases handles
% a derivation with exactly one non-stable proposition; in part 3 this
% is the second given derivation $\mildseq{\Gamma}{\Delta', A^+}{U}$,
% and in parts 4 and 5 this is the first given derivation
% $\mildseq{\Gamma}{\Delta}{A^-}$ or $\mildseq{\Gamma}{\cdot}{A^-}$.

%  In
% all three parts, the action of the proof is to induct on the
% derivation where the principal cut formula is stable to find every
% place where that principal cut formula ($A^+$ or $A^-$) is put in
% focus: at each of these points, principal cut admissibility (part 1 or
% 2) is invoked. Parts 1 and 2 of cut admissibility are where most of the
% action happens.

As noted above, there is a notable asymmetry between part 1 of the
theorem, which does not require stability of $\Delta'$ and $U$ in the
second given derivation $\mildseq{\Gamma}{\Delta',A^+}{U}$, and part 2
of the theorem, which does require stability of $\Delta$ in the first
given derivation $\mildseq{\Gamma}{\Delta}{A^-}$. The theorem would
still hold for non-stable $\Delta$, but we do not need the more
general theorem, and the less general theorem is easier to prove -- it
allows us to avoid duplicating the left commutative cuts between parts
2 and 3. On the other hand, we cannot make the theorem more specific,
imposing extra stability conditions on part 1, without fixing the
order in which invertible rules are applied. Fixing the order in which
invertible rules are applied has some other advantages as well; this
is a point we will return to in Section~\ref{sec:confluent-v-fixed}.

\subsection{Correctness of focusing}
\label{sec:lincorrectness}

\input{figs/fig-lin-shift-ctx}

Now we will prove the correctness property for the focused, polarized
logic that we discussed in Section~\ref{sec:linpolar}: that there is
an unfocused derivation of $(A^+)^\circ$ or $(A^-)^\circ$ if and only
if there is a focused derivation of $A^+$ or $A^-$.  The proof
requires us to lift our erasure function to contexts and succedents,
which is done in Figure~\ref{fig:lin-shift-ctx}. Note that erasure is
only defined on focused sequents
$\mildseq{\Gamma}{\underline{\Delta}}{\underline{U}}$ when all
suspended propositions are atomic. We are justified in making this
restriction because non-atomic suspended propositions cannot arise in
the process of proving a proposition $A^+$ or $A^-$ in an empty
context, and we are required to make this restriction due to the
analogous restrictions on cut admissibility
(Theorem~\ref{thm:lincut}).

Theorems~\ref{thm:linfocsound}~and~\ref{thm:linfoccomplete} therefore
implicitly carry the same extra condition that we put on the cut
admissibility theorem: that $\underline{\Delta}$ and $\underline{U}$
must contain only atomic suspended propositions.

\bigskip
\begin{theorem}[Soundness of focusing]\label{thm:linfocsound}
If $\mildseq{\Gamma}{\underline{\Delta}}{\underline{U}}$, 
then $\seq{\Gamma^\circ}{\underline{\Delta}^\circ}{\underline{U}^\circ}$.
\end{theorem}

\begin{proof}
  By straightforward induction on the given derivation; in each case,
  the result either follows directly by invoking the induction
  hypothesis (in the case of rules like ${\uparrow}_R$) or by invoking
  the induction hypothesis and applying one rule from
  Figure~\ref{fig:linear} (in the case of rules like ${\otimes}_R$).
\end{proof}

\begin{theorem}[Completeness of focusing]\label{thm:linfoccomplete}
If $\seq{\Gamma^\circ}{\Delta^\circ}{U^\circ}$, where $\Delta$ and $U$ are
stable,
then $\mildseq{\Gamma}{\Delta}{U}$. 
\end{theorem}

\begin{proof}
  By induction on the first given derivation. Each rule in the
  unfocused system (Figure~\ref{fig:linear}) corresponds to one {\it
    unfocused admissibility lemma}, plus a some extra steps.

  These extra steps arise are due to the generality of erasure. If we
  know that ${!}A = (C^+)^\circ$ (as in the case for ${!}_R$ below),
  then by case analysis on the structure of $C^+$, $C^+$ must be
  either ${!}B^-$ (for some $B^-$) or ${\downarrow}C_1^-$ (for some
  $C_1^-$). In the latter case, by further case analysis on $C_1^-$ we
  can see that $C_1^-$ must equal ${\uparrow}C_2^+$ (for some
  $C_2^+$). But then $C_2^+$ can be either ${!}B_2^-$ or
  ${\downarrow}C_3^-$; in the latter case $C^+ =
  {\downarrow}{\uparrow}{\downarrow}C_3^-$, and this can go on
  arbitrarily long (but not forever, because $C^-$ is a finite
  term). So we say that, by induction on the structure of $C^+$, there
  exists an $A^-$ such that $C^+ =
  {\downarrow}{\uparrow}\ldots{\downarrow}{\uparrow}{!}A^-$ and $A =
  (A^-)^\circ$. Depending on the case, we then repeatedly apply either
  the ${\uparrow}{\downarrow}_R$ rule or the ${\downarrow}{\uparrow}_L$
  rule,
  both of which are derived below, to eliminate all the extra shifts.
  (Zero or more instances of a rule are indicated by a double-ruled
  inference rule.)

\vspace{-7pt}
  {\small \[
  \infer[{\downarrow}{\uparrow}_R]
  {\mildseq{\Gamma}{\Delta}{{\downarrow}{\uparrow}A^+}}
  {\mildseq{\Gamma}{\Delta}{A^+}}
  ~
  \deduce{\mathstrut}{=}
  ~
  \infer[{\it focus}_R]
  {\mildseq{\Gamma}{\Delta}{{\downarrow}{\uparrow}A^+}}
  {\infer[{\downarrow}_R]
   {\mildseq{\Gamma}{\Delta}{[{\downarrow}{\uparrow}A^+}]}
   {\infer[{\uparrow}_R]
    {\mildseq{\Gamma}{\Delta}{{\uparrow}A^+}}
    {\mildseq{\Gamma}{\Delta}{A^+}}}}
  \qquad
  \infer[{\uparrow}{\downarrow}_L]
  {\mildseq{\Gamma}{\Delta, {\uparrow}{\downarrow}A^-}{U}}
  {\mildseq{\Gamma}{\Delta, A^-}{U}}
  ~
  \deduce{\mathstrut}{=}
  ~
  \infer[{\it focus}_L]
  {\mildseq{\Gamma}{\Delta, {\uparrow}{\downarrow}A^-}{U}}
  {\infer[{\uparrow}_L]
   {\mildseq{\Gamma}{\Delta, [{\uparrow}{\downarrow}A^-]}{U}}
   {\infer[{\downarrow}_R]
    {\mildseq{\Gamma}{\Delta, {\downarrow}A^-}{U}}
    {\mildseq{\Gamma}{\Delta, A^-}{U}}}}
  \]}
%   The other two are admissible:
%   \[
%   \infer-[\dedownup_R]
%   {\mildseq{\Gamma}{\Delta}{A^+}}
%   {\mildseq{\Gamma}{\Delta}{{\downarrow}{\uparrow}A^+}}
%   \quad
%   \deduce{\mathstrut}{=}
%   \quad
%   \infer-[{\it cut}(3)]
%   {\mildseq{\Gamma}{\Delta}{A^+}}
%   {\mildseq{\Gamma}{\Delta}{{\downarrow}{\uparrow}A^+}
%    &
%    \infer[{\downarrow}_L]
%    {\mildseq{\Gamma}{{\downarrow}{\uparrow}A^+}{A^+}}
%    {\infer[{\it focus}_L]
%     {\mildseq{\Gamma}{{\uparrow}A^+}{A^+}}
%     {\infer[{\uparrow}_L]
%      {\mildseq{\Gamma}{[{\uparrow}A^+]}{A^+}}
%      {\infer-[\eta^+]
%       {\mildseq{\Gamma}{A^+}{A^+}}
%       {\infer[{\it focus}_R]
%        {\mildseq{\Gamma}{\langle A^+ \rangle}{A^+}}
%        {\infer[{\it id}^+]
%         {\mildseq{\Gamma}{\langle A^+ \rangle}{[A^+]}}
%         {}}}}}}}
%   \]\[
%   \infer-[\deupdown_L]
%   {\mildseq{\Gamma}{\Delta, A^-}{U}}
%   {\mildseq{\Gamma}{\Delta, {\uparrow}{\downarrow}A^-}{U}}
%   \quad
%   \deduce{\mathstrut}{=}
%   \quad
%   \infer-[{\it cut}(4)] 
%   {\mildseq{\Gamma}{\Delta, A^-}{U}}
%   {\infer[{\uparrow}_R]
%    {\mildseq{\Gamma}{\Delta, A^-}{{\uparrow}{\downarrow}A^-}}
%    {\infer[{\it focus}_R]
%     {\mildseq{\Gamma}{\Delta, A^-}{{\downarrow}A^-}}
%     {\infer[{\downarrow}_R]
%      {\mildseq{\Gamma}{\Delta, A^-}{[{\downarrow}A^-]}}
%      {\infer-[\eta^-]
%       {\mildseq{\Gamma}{\Delta, A^-}{A^-}}
%       {\infer[{\it focus}_R]
%        {\mildseq{\Gamma}{\Delta, A^-}{\langle A^- \rangle}}
%        {\infer[{\it id}^-]
%         {\mildseq{\Gamma}{\Delta, [ A^- ]}{\langle A^- \rangle}}
%         {}}}}}}
%    &
%    \mildseq{\Gamma}{\Delta, {\uparrow}{\downarrow}A^-}{U}}
%   \]

\vspace{-13pt}

\noindent
We will describe a few cases to illustrate how unfocused admissibility
lemmas work.

  Rule {\it copy}: We are given 
  $\seq{\Gamma^\circ, A}{\Delta^\circ, A}{U^\circ}$, which is
  used to derive $\seq{\Gamma^\circ, A}{\Delta^\circ}{U^\circ}$.
  We know $A = (A^-)^\circ$. By the induction hypothesis, we have
  $\mildseq{\Gamma, A^-}{\Delta, A^-}{U}$, and we conclude
  with the unfocused admissibility lemma ${\it copy}_u$:
  \[
  \infer-[{\it cut}(5)]
  {\mildseq{\Gamma, A^-}{\Delta}{U}}
  {\infer-[\eta^-]
   {\mildseq{\Gamma, A^-}{\cdot}{A^-}}
   {\infer[{\it copy}]
    {\mildseq{\Gamma, A^-}{\cdot}{\langle A^- \rangle}}
    {\infer[{\it id}^-]
     {\mildseq{\Gamma, A^-}{[A^-]}{\langle A^- \rangle}}
     {}}}
   &
   \mildseq{\Gamma, A^-}{\Delta, A^-}{U}}
  \]

  Rule ${!}_L$: We are given $\seq{\Gamma^\circ,
    A}{\Delta^\circ}{U^\circ}$, which is used to derive
  $\seq{\Gamma^\circ}{\Delta^\circ, {!}A}{U^\circ}$.  We know ${!}A =
  (C^-)^\circ$; by induction on the structure of $C^-$ there exists
  $A^-$ such that $C^- =
  {\uparrow}{\downarrow}\ldots{\downarrow}{\uparrow}{!}A^-$.  By the
  induction hypothesis, we have $\mildseq{\Gamma, A^-}{\Delta}{U}$,
  and we conclude by the unfocused admissibility lemma ${!}_{uL}$,
  which is derivable:
  \[
  \infer=[{\uparrow}{\downarrow}_L]
  {\mildseq{\Gamma}
   {\Delta, {\uparrow}{\downarrow}\ldots{\downarrow}{\uparrow}{!}A^-}{U}}
  {\infer[{\it focus}_L]
   {\mildseq{\Gamma}{\Delta, {\uparrow}{!}A^-}{U}}
   {\infer[{\uparrow}_L]
    {\mildseq{\Gamma}{\Delta, [{\uparrow}{!}A^-]}{U}}
    {\infer[{!}_L]
     {\mildseq{\Gamma}{\Delta, {!}A^-}{U}}
     {\mildseq{\Gamma, A^-}{\Delta}{U}}}}}
  \]

  Rule ${!}_R$: We are given
  $\seq{\Gamma^\circ}{\cdot}{A}$,
  which is used to derive
  $\seq{\Gamma^\circ}{\cdot}{{!}A}$. 
  We know ${!}A = (C^+)^\circ$; by induction on the structure of
  $C^+$ there exists $A^-$ such that 
  $C^+ = {\downarrow}{\uparrow}\ldots{\downarrow}{\uparrow}!A^-$.
  By the induction hypothesis, we have
  $\mildseq{\Gamma}{\cdot}{{\downarrow}A^-}$, and we conclude by the unfocused
  admissibility lemma ${!}_{uR}$:
  \[
  \infer=[{\downarrow}{\uparrow}_R]
  {\mildseq{\Gamma}{\cdot}
   {{\downarrow}{\uparrow}\ldots{\downarrow}{\uparrow}!A^-}}
  {\infer-[{\it cut}(5)]
   {\mildseq{\Gamma}{\cdot}{!A^-}}
   {\infer[{\uparrow}_R]
    {\mildseq{\Gamma}{\cdot}{{\uparrow}{\downarrow}A^-}}
    {\mildseq{\Gamma}{\cdot}{{\downarrow}A^-}}
    &
    \infer[{\it focus}_R]
    {\mildseq{\Gamma, {\uparrow}{\downarrow}A^-}{\cdot}{!A^-}}
    {\infer[{!}_R]
     {\mildseq{\Gamma, {\uparrow}{\downarrow}A^-}{\cdot}{[!A^-]}}
     {\infer-[\eta^-]
      {\mildseq{\Gamma, {\uparrow}{\downarrow}A^-}{\cdot}{A^-}}
      {\infer[{\it copy}]
       {\mildseq{\Gamma, {\uparrow}{\downarrow}A^-}{\cdot}{\langle A^-\rangle}}
       {\infer[{\uparrow}_L]
        {\mildseq{\Gamma, {\uparrow}{\downarrow}A^-}
         {[{\uparrow}{\downarrow}A^-]}{\langle A^-\rangle}}
        {\infer[{\downarrow}_L]
         {\mildseq{\Gamma, {\uparrow}{\downarrow}A^-}
          {{\downarrow}A^-}{\langle A^-\rangle}}
         {\infer[{\it focus}_L]
          {\mildseq{\Gamma, {\uparrow}{\downarrow}A^-}
           {A^-}{\langle A^-\rangle}}
          {\infer[{\it id}^-]
           {\mildseq{\Gamma, {\uparrow}{\downarrow}A^-}
           {[ A^- ]}{\langle A^-\rangle}}
           {}}}}}}}}}}
  \]

  Rule $\lolli_L$: We are given 
  $\seq{\Gamma^\circ}{\Delta_A^\circ}{A}$ and
  $\seq{\Gamma^\circ}{\Delta^\circ, B}{U^\circ}$, which are used 
  to derive $\seq{\Gamma^\circ}{\Delta_A^\circ, \Delta^\circ, A \lolli B}{U}$.
  We know $A \lolli B = (C^-)^\circ$; by induction on the structure of
  $C^-$ there exist $A^+$ and
  $B^-$ such that $A = (A^+)^\circ$, $B = (B^-)^\circ$, and 
  $C^- = 
   {\uparrow}{\downarrow}\ldots{\uparrow}{\downarrow}(A^+ \lolli B^-)$.
  By the induction hypothesis, we have
  $\mildseq{\Gamma}{\Delta_A}{A^+}$ and
  $\mildseq{\Gamma}{\Delta, B^-}{U}$, and we conclude
  by the unfocused admissibility lemma ${\lolli}_{uL}$:
  \[
  \infer=[{\uparrow}{\downarrow}_L]
  {\mildseq{\Gamma}
   {\Delta_A, \Delta, 
    {\uparrow}{\downarrow}\ldots{\uparrow}{\downarrow}(A^+ \lolli B^-)}{U}}
  {\infer-[{\it cut}(3)]
   {\mildseq{\Gamma}{\Delta_A, \Delta, A^+ \lolli B^-}{U}}
   {\infer-[{\it cut}(3)]
    {\mildseq{\Gamma}{\Delta_A, A^+ \lolli B^-}{{\downarrow}B^-}}  
    {\mildseq{\Gamma}{\Delta_A}{A^+}
     &
     \infer-[\eta^+]
     {\mildseq{\Gamma}{A^+, A^+ \lolli B^-}{{\downarrow}B^-}}
     {\infer[{\it focus}_R]
      {\mildseq{\Gamma}{\langle A^+ \rangle, A^+ \lolli B^-}{{\downarrow}B^-}}
      {\infer[{\downarrow}_R]
       {\mildseq{\Gamma}{\langle A^+ \rangle, A^+ \lolli B^-}
        {[{\downarrow}B^-}]}
       {\infer-[\eta^-]
        {\mildseq{\Gamma}{\langle A^+ \rangle, A^+ \lolli B^-}
         {B^-}}
        {\infer[{\it focus}_L]
         {\mildseq{\Gamma}{\langle A^+ \rangle, A^+ \lolli B^-}
          {\langle B^- \rangle}}
         {\infer[{\lolli}_L]
          {\mildseq{\Gamma}{\langle A^+ \rangle, [ A^+ \lolli B^- ]}
           {\langle B^- \rangle}}
          {\infer[{\it id}^+]
           {\mildseq{\Gamma}{\langle A^+ \rangle}
            {[A^+]}}
           {}
           &
           \infer[{\it id}^-]
           {\mildseq{\Gamma}{[B^-]}{\langle B^- \rangle}}
           {}}}}}}}}
    &
    \infer[B^-]
    {\mildseq{\Gamma}{\Delta, {\downarrow}B^-}{U}}
    {\mildseq{\Gamma}{\Delta, B^-}{U}}}} 
  \]

  Rule $\lolli_R$: We are given 
  $\seq{\Gamma^\circ}{\Delta^\circ, A}{B}$, which is used 
  to derive $\seq{\Gamma^\circ}{\Delta^\circ}{A \lolli B}$.
  We know $A \lolli B = (C^+)^\circ$; by induction on the structure of
  $C^+$ there exist $A^+$ and
  $B^-$ such that $A = (A^+)^\circ$, $B = (B^-)^\circ$, and 
  $C^+ = 
   {\downarrow}{\uparrow}\ldots{\uparrow}{\downarrow}
    (A^+ \lolli B^-)$.
  By the induction hypothesis, we have
  $\mildseq{\Gamma}{\Delta, {\uparrow}A^+}{{\downarrow}B^+}$, 
  and we conclude by the 
  unfocused admissibility lemma ${\lolli}_{uR}$:
  \[
  \infer=[{\downarrow}{\uparrow}_R]
  {\mildseq{\Gamma}{\Delta}{{\downarrow}{\uparrow}\ldots{\uparrow}{\downarrow}
    (A^+ \lolli B^-)}}
  {\infer-[{\it cut}(4)]
   {\mildseq{\Gamma}{\Delta}{{\downarrow}(A^+ \lolli B^-)}}
   {\infer[{\lolli}_R]
    {\mildseq{\Gamma}{\Delta}
     {{\downarrow}{\uparrow}A^+ \lolli {\uparrow}{\downarrow}B^-}}
    {\infer[{\downarrow}_L]
     {\mildseq{\Gamma}{\Delta, {\downarrow}{\uparrow}A^+}
      {{\uparrow}{\downarrow}B^-}}
     {\infer[{\uparrow}_R]
      {\mildseq{\Gamma}{\Delta, {\uparrow}A^+}
       {{\uparrow}{\downarrow}B^-}}
      {\mildseq{\Gamma}{\Delta, {\uparrow}A^+}
       {{\downarrow}B^-}}}}
    & 
    \infer[{\it focus}_R]
    {\mildseq{\Gamma}
     {{\downarrow}{\uparrow}A^+ \lolli {\uparrow}{\downarrow}B^-}
     {{\downarrow}(A^+ \lolli B^-)}}
    {\infer[{\downarrow}_R]
     {\mildseq{\Gamma}
      {{\downarrow}{\uparrow}A^+ \lolli {\uparrow}{\downarrow}B^-}
      {[{\downarrow}(A^+ \lolli B^-)]}}
     {\infer[{\lolli}_R]
      {\mildseq{\Gamma}
       {{\downarrow}{\uparrow}A^+ \lolli {\uparrow}{\downarrow}B^-}
       {A^+ \lolli B^-}}
      {\infer-[\eta^+]
       {\mildseq{\Gamma}
        {{\downarrow}{\uparrow}A^+ \lolli {\uparrow}{\downarrow}B^-, A^+}
        {B^-}}
       {\infer-[\eta^-]
        {\mildseq{\Gamma}
         {{\downarrow}{\uparrow}A^+ \lolli {\uparrow}{\downarrow}B^-, 
          \langle A^+ \rangle}
         {B^-}}
        {\infer[{\it focus}_L]
         {\mildseq{\Gamma}
          {{\downarrow}{\uparrow}A^+ \lolli {\uparrow}{\downarrow}B^-, 
           \langle A^+ \rangle}
          {\langle B^- \rangle}}
         {\infer[{\lolli}_L]
          {\mildseq{\Gamma}
           {[{\downarrow}{\uparrow}A^+ \lolli {\uparrow}{\downarrow}B^-], 
            \langle A^+ \rangle}
           {\langle B^- \rangle}}
          {\infer[{\downarrow}_R]
           {\mildseq{\Gamma}{\langle A^+ \rangle}
            {[ {\downarrow}{\uparrow}A^+ ]}}
           {\infer[{\uparrow}_R]
            {\mildseq{\Gamma}{\langle A^+ \rangle}{{\uparrow}A^+}}
            {\infer[{\it focus}_R]
             {\mildseq{\Gamma}{\langle A^+ \rangle}{A^+}}
             {\infer[{\it id}^+]
              {\mildseq{\Gamma}{\langle A^+ \rangle}{[ A^+ ]}}
              {}}}}
           &
           \infer[{\uparrow}_L]
           {\mildseq{\Gamma}{[ {\uparrow}{\downarrow}B^- ]}
            {\langle B^- \rangle}}
           {\infer[{\downarrow}_L]
            {\mildseq{\Gamma}{{\downarrow}B^-}{\langle B^- \rangle}}
            {\infer[{\it focus}_L]
             {\mildseq{\Gamma}{B^-}{\langle B^- \rangle}}
             {\infer[{\it id}^-]
              {\mildseq{\Gamma}{[B^-]}{\langle B^- \rangle}}
              {}}}}}}}}}}}}}
  \]

\noindent
All the other cases follow the same pattern.
\end{proof}

\subsection{Confluent versus fixed inversion}
\label{sec:confluent-v-fixed}

A salient feature of this presentation of focusing is that invertible,
non-focused rules need not be applied in any particular order.
Therefore, the last step in a proof of $\mildseq{\Gamma}{\Delta, A
  \tensor B, \one, {!}C}{D \lolli E}$ could be ${\otimes}_L$,
${\one}_L$ ${!}_L$, or ${\lolli}_R$. 
The style is exemplified by Liang and
Miller's LJF \cite{liang09focusing}, and the confluent presentation in
this chapter is closely faithful to Pfenning's course notes on linear
logic \cite{pfenning12chaining}.

Allowing for this inessential nondeterminism simplifies the
presentation a bit, but it also gets in the way of effective proof
search and canonical derivations if we do not address it in some way.
The different possibilities for addressing this nondeterminism within
an inversion phase echo the discussion of nondeterminism in LF from
the beginning of the chapter.  We can, as suggested in that
introduction, declare that all proofs which differ only by the order
of their invertible, non-focused rules be treated as equivalent. It is
possible to establish that all possible inversion orderings will lead
to the same set of stable sequents, which lets us know that all of
these reorderings do not fundamentally change the structure of the
rest of the proof.  This property already seems to be necessary to prove
{\it unfocused cut} as expressed by this admissible rule:
 \[
 \infer-[{\it cut}]
 {\mildseq{\Gamma}{\Delta', \Delta}{U}}
 {\mildseq{\Gamma}{\Delta}{A}
  &
  \mildseq{\Gamma}{\Delta', A}{U}}
\]
(where $A$ is $A^+$ or $A^-$ and ${\Delta}$, ${\Delta'}$, and $U$
contain no focus but may not be stable).  If $A$ is $A^+$, proving the
admissibility of this rule involves permuting invertible rules in the
second given derivation, $\mildseq{\Gamma}{\Delta', A^+}{U}$, until
$A^+$ is the only unstable part of the second sequent, at which point
part 3 of Theorem~\ref{thm:lincut} applies.  Similarly, if $A$ is
$A^-$, we must permute invertible rules in the first given derivation
until $A^-$ is the only unstable part of the first sequent, at which
point part 4 of Theorem~\ref{thm:lincut} applies. 

By proving and using this more general cut property, it would be
possible to prove a more general completeness theorem: if 
$\seq{\Gamma^\circ}{\Delta^\circ}{U^\circ}$, then $\mildseq{\Gamma}{\Delta}{U}$
(Theorem~\ref{thm:linfoccomplete} as stated also requires that $\Delta$
and $U$ be stable). The cases of this new 
theorem corresponding to the unfocused rules ${!}_R$,
${\lolli}_R$, and ${\otimes}_L$, which required the use of
doubly-shifted side derivations in our presentation, are trivial in this
modified presentation.  Unfortunately, the proof of unfocused cut,
while simple, is tedious and long. Gentzen's original proof of cut
admissibility \cite{gentzen35untersuchungen} and Pfenning's mechanization
\cite{pfenning00structural} all scale linearly with the number of
connectives and rules in the logic; the proofs of identity expansion, cut
admissibility, soundness of focusing, and completeness of focusing
presented in this chapter do too. There is no known proof of the
unfocused admissibly of the rule ${\it cut}$ above that scales
linearly in this way: all known proofs grow quadratically with the
number of connectives and rules in the logic.

%; one of them
%is Proposition~\ref{prop:confluence-lin}:
%
%\bigskip
%\begin{proposition}
%\label{prop:confluence-lin}
%Let $\Xi$ be a set of derivations of stable sequents. If 
%$\mildseq{\Gamma}{\Delta}{U}$, where $\Delta$ and $U$ contain no 
%focused propositions, can be derived from $\Xi$ using only non-focused
%rules, then for all non-focused rules that can be used to derive
%$\mildseq{\Gamma}{\Delta}{U}$, its premises can be derived
%from $\Xi$ using only non-focused rules.
%\end{proposition}
%\bigskip
Once we equate all proofs that differ only on the order
in which inference rules are applied within an inversion phase, 
we can pick some member of
each equivalence class
to serve as a canonical representative; this will suffice
to solve the problems with proof search, as we can search for 
the canonical representatives of focused proofs rather than searching
within the larger set of all focused proofs. The most common canonical
representatives force invertible rules to decompose propositions 
in a depth-first ordering. 

Then, reminiscent of the move from LF to Canonical LF, the logic
itself can be restricted so that only the canonical representatives
are admitted. The most convenient way of forcing a left-most,
depth-first ordering is to isolate the invertible propositions ($A^+$
on the left and $A^-$ on the right) in separate, ordered inversion
contexts, and then to only work on the left-most proposition in the
context. This is the way most focused logics are defined, including
those by Andreoli, Chaudhuri, and myself in the next chapter. This style of 
presenting a focusing logic can be called a {\it fixed} presentation,
as the inversion phase is fixed in a particular, though
fundamentally arbitrary, shape. 

The completeness of focusing for a fixed presentation of focusing is
implied by the completeness of focusing for a confluent presentation
of the same logic along with the appropriate confluence property for
that logic, whereas the reverse is not true. In this sense, the
confluent presentation allows us to prove a stronger theorem than the
fixed presentation does, though the fixed presentation will be
sufficient for our purposes here and in later chapters. We will not
prove confluence in this chapter, though doing so is a straightforward
exercise.

\subsection{Running example}
\label{sec:linlogtrans}

\input{figs/fig-focused-robot}

Figure~\ref{fig:focused-robot} gives the result of taking our robot
example, Figure~\ref{fig:unfocused-robot}, through the polarization
process and then running the result through
Theorem~\ref{thm:linfoccomplete}. There is indeed only one proof of
this focused proposition up to the reordering of invertible rules, and
only one proof period if we always decompose invertible propositions
in a left-most (i.e., depth-first) ordering as we do in
Figure~\ref{fig:focused-robot}.

We have therefore successfully used focusing to get a canonical
proof structure that correctly corresponds to our 
informal series of transitions:
\[
\begin{array}{ccccc}
\begin{array}{c}
\mbox{\it \$6 (1)}\medskip\\ 
\mbox{\it battery-less robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}
\end{array}
& \leadsto &
\begin{array}{c}
\mbox{\it battery  (1)}\medskip\\ 
\mbox{\it battery-less robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}
\end{array}
& \leadsto &
\begin{array}{c}
\mbox{\it robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}\medskip\\~\\
\end{array}
\end{array}
\]
But at what cost? Figure~\ref{fig:focused-robot} contains a
fair amount of bureaucracy compared to the original
Figure~\ref{fig:unfocused-robot}, even if does a better job of
matching, when read from bottom to top, the series of transitions. A
less cluttered way of looking at these proofs is in terms of what we,
following Chaudhuri, call {\it synthetic inference rules}
\cite{chaudhuri08focusing}.

\section{Synthetic inference rules}
\label{sec:linsynthetic}

Synthetic inference rules were introduced by Andreoli as the
derivation fragments associated with {\it
  bipoles}. A monopole is the outermost negative (or positive)
structure of a proposition, and a bipole is a monopole surrounded by
positive (or, respectively, negative) propositions
\cite{andreoli01focussing}. In a polarized setting, bipoles capture
the outermost structure of a proposition up to the second occurrence
of a shift or an exponential.

The first idea behind synthetic inference rules is that the most
important sequents in a polarized sequent calculus are stable sequents
where all suspended propositions are atomic.  This was reflected by
our proof of the completeness of focusing
(Theorem~\ref{thm:linfoccomplete}), which was restricted to stable
sequents.\footnote{If we had established the unfocused cut rule
  discussed in Section~\ref{sec:confluent-v-fixed} and had then proven
  the completeness of focusing (Theorem~\ref{thm:linfoccomplete}) for
  arbitrary inverting sequents, it would have enabled an
  interpretation that puts all unfocused sequents on similar footing,
  but that is not our goal here.}
The second idea is that the bottommost rule in the proof of 
a stable sequent must be one of the following:
\smallskip
\begin{itemize}
\item ${\it copy}$ on some proposition $A^-$ from $\Gamma$, 
\item ${\it focus}_L$ on some proposition $A^-$ in $\Delta$, or
\item ${\it focus}_R$ on the succedent $A^+$
\end{itemize}
\smallskip
%
Once we know which proposition we have focused on, the bipole
structure of that proposition (that is, the outermost structure of the
proposition up through the second occurrence of a shift of exponential)
completely (though not uniquely) dictates the structure of the proof
up to the next occurrences of stable sequents.

For example, consider the act of focusing on the proposition $a^+
\lolli {\uparrow}b^+$ in $\Gamma$ using the ${\it copy}$ rule, where
$a^+$ and $b^+$ are positive atomic propositions.  This must mean that
a suspended atomic proposition $a^+$ appears suspended in the context
$\Delta$, or else the proof could not be completed:
\[
\infer[{\it copy}]
{\mildseq{\Gamma, a^+ \lolli {\uparrow}b^+}{\Delta, \langle a^+ \rangle}{U}}
{\infer[{\lolli}_L]
 {\mildseq{\Gamma, a^+ \lolli {\uparrow}b^+}
   {\Delta, \langle a^+ \rangle, [a^+ \lolli {\uparrow}b^+]}{U}}
 {\infer[{\it id}^+]
  {\mildseq{\Gamma, a^+ \lolli {\uparrow}b^+}
   {\langle a^+ \rangle}{[ a^+ ]}}
  {}
  &
  \infer[{\uparrow}_L]
  {\mildseq{\Gamma, a^+ \lolli {\uparrow}b^+}{\Delta, [{\uparrow}b^+]}{U}}
  {\infer[\eta^+]
   {\mildseq{\Gamma, a^+ \lolli {\uparrow}b^+}{\Delta, b^+}{U}}
   {\mildseq{\Gamma, a^+ \lolli {\uparrow}b^+}
    {\Delta, \langle b^+ \rangle}{U}}}}}
\]
The non-stable sequents in the middle are not interesting parts 
of the structure of the proof, as they are fully determined by the
choice of focus, so we can collapse this series of transitions
into a single synthetic rule:
\[
\infer[{\sf CP}]%_{a^+ \lolli {\uparrow}b^+}]
{\mildseq{\Gamma, a^+ \lolli {\uparrow}b^+}{\Delta, \langle a^+ \rangle}{U}}
{\mildseq{\Gamma, a^+ \lolli {\uparrow}b^+}{\Delta, \langle b^+ \rangle}{U}}
\]
For the MELL fragment of linear logic, we can associate exactly one
rule with every positive proposition (corresponding to a right-focus 
on that proposition) and two rules with every negative proposition
(corresponding to left focus on a negative proposition in the persistent
context and left focus on that negative proposition in the positive
context). Here are three examples:
\[
\infer[{\sf LF}]%_{a^+ \lolli {\uparrow}b^+}]
{\mildseq{\Gamma}{\Delta, \langle a^+ \rangle, a^+ \lolli {\uparrow}b^+}{U}}
{\mildseq{\Gamma}{\Delta, \langle b^+ \rangle}{U}}
\]
\[
\infer[{\sf RF}%_{{\downarrow}({!}A^- \otimes b^+ \otimes {\downarrow}C^- \lolli 
%   {\uparrow}D^+)}
]
{\mildseq{\Gamma}{\Delta}
  {{\downarrow}({!}A^- \otimes b^+ \otimes {\downarrow}C^- \lolli 
   {\uparrow}D^+)}}
{\mildseq{\Gamma, A^-}{\Delta, \langle b^+ \rangle, C^-}{D^+}}
\quad
\infer[{\sf RF}']%_{a^+}]
{\mildseq{\Gamma}{\langle a^+ \rangle}{a^+}}
{}
\]
This doesn't mean that there are no choices to be made within focused
phases, just that, in MELL, those choices are limited to the way the
resources -- propositions in $\Delta$ -- are distributed among the
branches of the proof.  If we also consider additive connectives, we
can identify some number of synthetic rules for each right focus, left
focus, or copy. This may be zero, as there's no way to successfully
right focus on a proposition like $\zero \otimes
{\downarrow}{\uparrow}A^+$, and therefore zero synthetic inference rules
are associated with this proposition. It may be more than one: there are
three ways to successfully right focus on the proposition $a^+ \oplus
b^+ \oplus c^+$, and so three synthetic inference rules are associated
with this proposition:
\[
\infer
{\mildseq{\Gamma}{\susp{a^+}}{a^+ \oplus b^+ \oplus c^+}}
{}
\quad
\infer
{\mildseq{\Gamma}{\susp{b^+}}{a^+ \oplus b^+ \oplus c^+}}
{}
\quad
\infer
{\mildseq{\Gamma}{\susp{c^+}}{a^+ \oplus b^+ \oplus c^+}}
{}
\]

\begin{figure}
\small\[
\infer[{\sf RF}%_{{\downarrow}({!}A^- \otimes b^+ \otimes {\downarrow}C^- \lolli 
%   {\uparrow}D^+)}
]
{\mildseq{\cdot}{\cdot}
   {{\downarrow}({!}({\sf 6bucks} \lolli {\uparrow}{\sf battery}) \otimes
             {\sf 6bucks} \otimes 
             {\downarrow}({\sf battery} \lolli {\uparrow}{\sf robot}) \lolli 
             {\uparrow}{\sf robot})}}
{\infer[{\sf CP}%_{a^+ \lolli {\uparrow}b^+}
]
 {\mildseq{{\sf 6bucks} \lolli {\uparrow}{\sf battery}\quad}
    {\quad\langle {\sf 6bucks} \rangle, ~~
     {\sf battery} \lolli {\uparrow}{\sf robot}\quad}{\quad{\sf robot}}}
 {\infer[{\sf LF}%_{a^+\lolli{\uparrow}b^+}
]
  {\mildseq{{\sf 6bucks} \lolli {\uparrow}{\sf battery}\quad}
    {\quad{\sf battery} \lolli {\uparrow}{\sf robot}, ~~
     \langle {\sf battery} \rangle\quad}{\quad{\sf robot}}}
  {\infer[{\sf RF}'%_{a^+}
]
   {\mildseq{{\sf 6bucks} \lolli {\uparrow}{\sf battery}\quad}
       {\quad\langle {\sf robot} \rangle\quad}{\quad{\sf robot}}}
   {}}}}
\]
\caption{Our running example, presented with synthetic rules}
\label{fig:synthetic-robot}
\end{figure}

Focused proofs of stable sequents are, by definition, in a 1-to-1
correspondence with proofs using synthetic inference rules. If we look
at our running example as a derivation using the
example synthetic inference rules presented above
(as demonstrated in Figure~\ref{fig:synthetic-robot}), we see that the
system takes four steps. The middle two steps, furthermore, correspond
precisely to the two steps in our informal description of the
robot-battery-store system.



\section{Hacking the focusing system}
\label{sec:linhack}

Despite the novel treatment of suspended propositions in
Section~\ref{sec:foclinlog}, the presentation of linear logic given
there is equivalent to the presentation in Chaudhuri's
dissertation \cite{chaudhuri06focused}, in the sense that the logic gives
rise to the same synthetic inference rules. It is {\it not} a faithful
intuitionistic analogue to Andreoli's original presentation of focusing
\cite{andreoli92logic}, though the presentation in Pfenning's course notes is
\cite{pfenning12chaining}.\footnote{We will blur the lines, in this
  section, between Andreoli's original presentation of focused
  classical linear logic and Pfenning's adaptation to intuitionistic
  linear logic. In particular, we will mostly use the notation of
  Pfenning's presentation, but the observations are equally applicable
  in Andreoli's focused triadic system.}  Nor does it have the same
synthetic inference rules as the focused presentation used in the
framework of ordered logical specifications that we
presented in \cite{pfenning09substructural}.
% However, our current objective is not simply to study
% focused logic; we want to use the structure of proofs in focused
% logics as a logical framework for encoding systems we are interested
% in.

In this section, we will discuss four different presentations of 
focused sequent calculi that are closely connected to the logic we
have just presented. Each system differs significantly in its
treatment of positive atomic propositions, the exponential 
${!}A$, and the interaction between them.
\begin{itemize}
\item Andreoli's original system, which I name the {\it atom
    optimization}, complicates the interpretation of atomic
  propositions as stand-ins for arbitrary propositions.
\item A further change to the atom optimization, the {\it exponential
    optimization}, complicates the relationship between the focused
  logic and the unfocused logic.
\item The {\it adjoint logic} of Benton and Wadler
  \cite{benton96linear} introduces a new syntactic class of persistent
  propositions, restricting linear propositions to the linear 
  context and persistent propositions to the persistent context.
\item The introduction of {\it permeable atomic propositions}, a
  notion (which dates at least back to Girard's LU
  \cite{girard93unity}) that some propositions can be treated as {\it
    permeable} between the persistent and linear contexts and that
  permeable atomic propositions can be introduced to stand for this
  class of permeable propositions.
\end{itemize}

The reason we survey these different systems is that they all
provide a solution to a pervasive problem encountered when using 
focused sequent calculi as logical frameworks:
the need to allow for synthetic inference rules of the form
\[
\infer
{\mildseq{\Gamma, p}{\Delta, q}{C}}
{\mildseq{\Gamma, p}{\Delta, r}{C}}
\]
where $p$ is an atomic proposition in the persistent context that is
observed (but not consumed), $q$ is an atomic proposition that is
consumed in the transition, and $r$ is an atomic proposition that is
generated as the result of the transition. In the kinds of
specifications we will be dealing with, the ability to form these
synthetic inference rules is critical. In some uses, the persistent
resource acts as {\it permission} to consume $q$ and produce $r$. In
other uses, $p$ represents knowledge that we must currently possess in
order to enact a transition. As a concrete example, America's 2010
health care reform law introduced a requirement that restaurant menus
include calorie information. This means that, in the near future, we
can exchange six bucks for a soup and salad at Panera, but only if we
know how many calories are in the meal. The six bucks, soup, and salad
remain ephemeral resources like $q$ and $r$, but the calorie count is
persistent. A calorie count is scientific knowledge, which is a
resource that is not consumed by the transition.

My justification for presenting Chaudhuri's system as the canonical
focusing system for linear logic in Section~\ref{sec:foclinlog} is
because it most easily facilitates reasoning about the focused sequent
calculus {\it as a logic}. Internal soundness and completeness
properties are established by the cut admissibility and identity
expansion theorems
(Theorems~\ref{thm:lincut}~and~\ref{thm:linidentity}), and these
theorems are conceptually prior to the soundness and completeness of
the focused system relative to the unfocused system
(Theorems~\ref{thm:linfocsound}~and~\ref{thm:linfoccomplete}). The
various modifications we discuss in this section complicate the
treatment of focused logics as independently justifiable sequent
calculi for linear logic. I suggest in
Section~\ref{sec:permeable} that the last option, the incorporation of
permeable atomic propositions, is the most pleasing mechanism for
incorporating the structure we desire into a focused presentation of
linear logic.

All of the options discussed in this section are compatible with a
fifth option, discussed in Section~\ref{sec:pseudopositive}, of
avoiding positive propositions altogether and instead changing our
view of stable sequents. The proposition ${\downarrow}a^- \lolli
{\downarrow}b^- \lolli c^-$ is associated with this synthetic
inference rule:
\[
\infer {\mildseq{\Gamma}{\Delta, \Delta', {\downarrow}a^- \lolli
    {\downarrow}b^- \lolli c^-}{\susp{c^-}}}
{\mildseq{\Gamma}{\Delta}{\susp{a^-}} &
  \mildseq{\Gamma}{\Delta'}{\susp{b^-}}}
\]
If we can prove a general theorem that the sequent
$\mildseq{\Gamma}{\Delta}{\susp{a^-}}$ can only be proven if $\Delta =
a^-$ or if $\Delta = \cdot$ and $a^- \in \Gamma$, then $a^-$ is a {\it
  pseudo-positive} atomic proposition.  Proving the succedent
$\susp{a^-}$ where $a^-$ is pseudo-positive is functionally very
similar to proving ${[a^+]}$ in focus for a positive atomic
proposition. This gives us license to treat stable sequents that prove
a pseudo-positive proposition not as a stable sequent that appears in
synthetic inference rules but as an immediate subgoal that gets folded
into the synthetic inference rule. 
If $a^-$ is pseudo-positive, the persistent
proposition ${\downarrow}a^- \lolli {\downarrow}b^- \lolli c^-$ can be
associated with these two synthetic inference rules:
\[
\infer
{\mildseq{\Gamma}{\Delta, {\downarrow}a^- \lolli
    {\downarrow}b^- \lolli c^-, a^-}{\susp{c^-}}}
{\mildseq{\Gamma}{\Delta}{\susp{b^-}}}
\quad
\infer
{\mildseq{\Gamma, a^-}{\Delta, {\downarrow}a^- \lolli
    {\downarrow}b^- \lolli c^-}{\susp{c^-}}}
{\mildseq{\Gamma, a^-}{\Delta}{\susp{b^-}}}
\]
The machinery of lax logic introduced in Chapter~\ref{chapter-order}
and the fragment of this logic that forms a logical framework in
Chapter~\ref{chapter-framework} make it feasible, in practice, to
observe when negative atomic propositions are pseudo-positive.

\subsection{Atom optimization}
\label{sec:atomopt}

Andreoli's original focused system isn't polarized, so propositions
that are syntactically invalid in a polarized presentation, like
${!}(p^+ \otimes q^+)$ or ${!}p^+$, are valid in his system (we would
have to write ${!}{\uparrow}{(p^+ \otimes q^+)}$ and
${!}{\uparrow}p^+$). It's therefore possible, in an unpolarized
presentation, to use the ${\it copy}$ rule to copy a positive
proposition out of the context and into left focus, but the focus
immediately blurs, as in this (intuitionistic) proof
fragment:\footnote{We will use the sequent form
  $\andseq{\Gamma}{\Delta}{C}$ in this section for focused but
  unpolarized systems. Again, we frequently reference Pfenning's
  presentation of focused linear logic \cite{pfenning12chaining} as a
  faithful intuitionistic analogue of Andreoli's system.}
\[
\infer[{\it copy}]
{\andseq{p^+ \otimes q^+}{\cdot}{q^+ \otimes p^+}}
{\infer[{\it blur}_L]
 {\andseq{p^+ \otimes q^+}{[p^+ \otimes q^+]}{q^+ \otimes p^+}}
 {\infer[{\otimes}_L]
  {\andseq{p^+ \otimes q^+}{p^+ \otimes q^+}{q^+ \otimes p^+}}
  {\deduce
   {\andseq{p^+ \otimes q^+}{p^+, q^+}{q^+ \otimes p^+}}
   {\vdots}}}}
\]
Note that, in the polarized setting, the
effect of the ${\it blur}_L$ rule is accomplished by the
${\downarrow}_L$ rule.

Andreoli's system makes a single restriction to the ${\it copy}$ rule:
it cannot apply to a positive atomic proposition in the persistent
context. On its own, this restriction would make the system incomplete
with respect to unfocused linear logic -- there would be no focused
proof of ${!}p^+ \lolli p^+$ -- and so Andreoli-style focusing systems
restore completeness by creating a second initial sequent for positive
atomic propositions that allows a positive right focus on an atomic
proposition to succeed if the atomic proposition appears in the
persistent context:
\[
\infer[{\it id}_1^+]
{\andseq{\Gamma}{p^+}{[p^+]}}
{}
\qquad
\infer[{\it id}_2^+]
{\andseq{\Gamma, p^+}{\cdot}{[p^+]}}
{}
\]
With the second initial rule, we can once again prove ${!}p^+ \lolli p^+$,
and the system becomes complete with respect to unfocused linear
logic again.
\[
\infer[{\lolli}_R]
{\andseq{\cdot}{\cdot}{{!}p^+ \lolli p^+}}
{\infer[{!}_L]
 {\andseq{\cdot}{{!}p^+}{p^+}}
 {\infer[{\it focus}_R]
  {\andseq{p^+}{\cdot}{p^+}}
  {\infer[{\it id}^+_2]
   {\andseq{p^+}{\cdot}{[p^+]}}
   {}}}}
\]
This modified treatment of positive atoms will be called the 
{\it atom optimization}, as it reduces the number of focusing steps that 
need to be applied: it takes only one right focus to prove
${!}p^+ \lolli p^+$ in Andreoli's system, but it would take two focusing
steps to prove the same proposition in Chaudhuri's system (or to prove
${!}{\uparrow}p^+ \lolli {\uparrow}p^+$ in the focusing system we have
presented). 

There seem to be three ways of adapting the atom optimization to a polarized
setting. The first option would be to add an initial sequent that 
directly mimics the one in Andreoli's system, while adding an additional
requirement to the {\it copy} rule that $A^-$ is not a shifted positive
atomic proposition:
\[
\infer[{\it id}^+]
{\mildseq{\Gamma}{\susp{A^+}}{[A^+]}}
{}
\quad
\infer[{\it id}^+_2]
{\mildseq{\Gamma,{\uparrow}p^+}{\cdot}{[p^+]}}
{}
\quad
\infer[{\it copy}^*]
{\mildseq{\Gamma, A^-}{\Delta}{U}}
{A \neq {\uparrow}p^+
 &
 \mildseq{\Gamma, A^-}{\Delta, [A^-]}{U}}
\]
The second approach is to extend suspended propositions to
the persistent context, add a corresponding rule for right focus,
and modify the left rule for ${!}$ to notice
the presence of a positive atomic proposition:
\[
\infer[{!}_{L1}]
{\mildseq{\Gamma}{\Delta, {!}A^-}{U}}
{A^- \neq {\uparrow}p^+
 &
 \mildseq{\Gamma, A^-}{\Delta}{U}}
\quad
\infer[{!}_{L2}]
{\mildseq{\Gamma}{\Delta, {!}{\uparrow}p^+}{U}}
{\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{U}}
\]\[
\infer[{\it id}^+_1]
{\mildseq{\Gamma}{\langle A^+ \rangle}{[A^+]}}
{}
\quad
\infer[{\it id}^+_2]
{\mildseq{\Gamma, \langle A^+ \rangle}{\cdot}{[A^+]}}
{}
\]
The third approach is to introduce a new connective, $\pbang$, that
can only be applied to positive atomic propositions, just as ${!}$ can
only be applied to negative propositions. We can initially view this
option as equivalent to the previous one by defining ${\pbang}p^+$ as
a notational abbreviation for ${!}{\uparrow}p^+$ and styling rules
according to the second approach above:
\[
\infer[{\pbang}_R]
{\mildseq{\Gamma}{\cdot}{[{\pbang}p^+]}}
{\mildseq{\Gamma}{\cdot}{p^+}}
\quad
\infer[{\pbang}_L]
{\mildseq{\Gamma}{\Delta, {\pbang}p^+}{U}}
{\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{U}}
\quad
\infer[{\it id}^+_1]
{\mildseq{\Gamma}{\langle A^+ \rangle}{[A^+]}}
{}
\quad
\infer[{\it id}^+_2]
{\mildseq{\Gamma, \langle A^+ \rangle}{\cdot}{[A^+]}}
{}
\]

All three of these options are similar; we will go with the
last, as it allows us to preserve the original meaning of ${!}{\uparrow}p^+$
if that is our actual intent. 
Introducing the atom optimization as a
new connective also allows us to isolate the effects that this new
connective has on cut admissibility, identity expansion,
and the correctness of focusing; we will consider
each in turn.

\begin{figure}
\[
\infer[{\pbang}_L]
{\mildseq{\cdot}{{\pbang}p^+}{p^+ \otimes p^+}}
{\infer[{\it focus}_R]
 {\mildseq{\langle p^+ \rangle}{\cdot}{p^+ \otimes p^+}}
 {\infer[{\otimes}_R]
  {\mildseq{\langle p^+ \rangle}{\cdot}{[ p^+ \otimes p^+ ]}}
  {\infer[{\it id}_2^+]
   {\mildseq{\langle p^+ \rangle}{\cdot}{[p^+]}}
   {}
   &
   \infer[{\it id}_2^+]
   {\mildseq{\langle p^+ \rangle}{\cdot}{[p^+]}}
   {}}}}
~~
\mbox{vs.}
\!\!\!
\infer[{!}_L]
{\mildseq{\cdot}{{!}{\uparrow}{A^+}}{A^+ \otimes A^+}}
{\infer[\it copy]
 {\mildseq{{\uparrow}{A^+}}{\cdot}{A^+ \otimes A^+}}
 {\infer[{\uparrow}_L]
  {\mildseq{{\uparrow}{A^+}}{[{\uparrow}A^+]}{A^+ \otimes A^+}}
  {\infer-[\eta^+]
   {\mildseq{{\uparrow}{A^+}}{A^+}{A^+ \otimes A^+}}
   {\infer[{\it copy}]
    {\mildseq{{\uparrow}{A^+}}{\langle A^+ \rangle}{A^+ \otimes A^+}}
    {\infer[{\uparrow}_L]
     {\mildseq{{\uparrow}{A^+}}{\langle A^+ \rangle, [{\uparrow}A^+]}
        {A^+ \otimes A^+}}
     {\infer-[\eta^+]
      {\mildseq{{\uparrow}{A^+}}{\langle A^+ \rangle, A^+}
         {A^+ \otimes A^+}}
      {\infer[{\it focus}_R]
       {\mildseq{{\uparrow}{A^+}}{\langle A^+ \rangle, \langle A^+ \rangle}
          {A^+ \otimes A^+}}
       {\infer[{\otimes}_R]
        {\mildseq{{\uparrow}{A^+}}{\langle A^+ \rangle, \langle A^+ \rangle}
           {[A^+ \otimes A^+]}}
        {\infer[{\it id}_1^+]
         {\mildseq{{\uparrow}{A^+}}{\langle A^+ \rangle}
            {[A^+]}}
         {}
         &
         \infer[{\it id}_1^+]
         {\mildseq{{\uparrow}{A^+}}{\langle A^+ \rangle}
            {[A^+]}}
         {}}}}}}}}}}
\]
\caption{Substituting $A^+$ for $p^+$
in the presence of the atom optimization }
\label{fig:replacement-breaks}
\end{figure}

\paragraph{Identity expansion}

There is
one new case of identity expansion, which is unproblematic:
\[
\infer-[\eta^+]
{\mildseq{\Gamma}{\Delta, {\pbang}p^+}{U}}
{\deduce
 {\mildseq{\Gamma}{\Delta, \langle {\pbang}p^+ \rangle}{U}}
 {\mathcal D}}
\quad
\Longrightarrow
\infer[{\pbang}_L]
{\mildseq{\Gamma}{\Delta, {\pbang}p^+}{U}}
{\infer-[{\it subst}^+]
 {\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{U}}
 {\infer[{\pbang}_R]
  {\mildseq{\Gamma, \langle p^+ \rangle}{\cdot}{[{\pbang}p^+]}}
  {\infer[{\it focus}_R]
   {\mildseq{\Gamma, \langle p^+ \rangle}{\cdot}{p^+}}
   {\infer[{\it id}^+_2]
    {\mildseq{\Gamma, \langle p^+ \rangle}{\cdot}{[ p^+ ]}}
    {}}}
  &
  \infer-[{\it weaken}]
  {\mildseq{\Gamma, \langle p^+ \rangle}{\Delta, \langle {\pbang}p^+ \rangle}
     {U}}
  {\deduce
   {\mildseq{\Gamma}{\Delta, \langle {\pbang}p^+ \rangle}{U}}
   {\mathcal D}}}}
\]

Even though the identity expansion theorem is unproblematic, we can
illuminate one problem with the atom optimization by considering
the substitution of arbitrary propositions for atomic propositions.
%
Previously, when we substituted a positive
proposition for an atomic proposition, the proof's structure remained
fundamentally unchanged -- instances of the $\eta^+$ rule on $p^+$
turned into admissible instances of the general identity expansion
rule $\eta^+$ on $A^+$. Now, we have to explain what it even means to
substitute $A^+$ for $p^+$ in ${\pbang}p^+$, since ${\pbang}A^+$ is
not a syntactically valid proposition; the only obvious candidate
seems to be ${!}{\uparrow}A^+$. That substitution may require us to
change the structure of proofs in a significant way, as shown in
Figure~\ref{fig:replacement-breaks}. Immediately before entering into
any focusing phase where the ${\it id}_2^+$ rule is used $n$ times on
the hypothesis $\langle p^+ \rangle$, we need to left-focus on
${\uparrow}A^+$ $n$ times with the ${\it copy}$ rule to get $n$ copies
of $\langle A^+ \rangle$ into the linear context, each of which can be
used to replace one of the ${\it id}^+_2$ instances with an instance
of ${\it id}^+_1$.

\paragraph{Cut admissibility}
While we might be willing to sacrifice the 
straightforward interpretation of atomic
propositions as stand-ins for arbitrary propositions, another instance
of the same problematic pattern arises when we try to establish the
critical cut admissibility theorem for the
logic with ${\pbang}p^+$. 
Most of the new cases are unproblematic,
but trouble arises in part 1
when we cut a right-focused proof of ${\pbang}p^+$  
against a proof that is decomposing ${\pbang}p^+$ on the left:
\[
\infer-[{\it cut}(1)]
{\mildseq{\Gamma}{\Delta}{U}}
{\infer[{\pbang}_R]
 {\mildseq{\Gamma}{\cdot}{[{\pbang}p^+]}}
 {\mildseq{\Gamma}{\cdot}{p^+}}
 &
 \infer[{\pbang}_L]
 {\mildseq{\Gamma}{\Delta, {\pbang}p^+}{U}}
 {\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{U}}}
\]
We are left needing to prove that 
$\mildseq{\Gamma}{\cdot}{p^+}$ and 
$\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{U}$
proves $\mildseq{\Gamma}{\Delta}{U}$, which does not fit the structure
of any of our existing cut principles. It is similar to 
the statement of part 5 of Theorem~\ref{thm:lincut} 
(if $\mildseq{\Gamma}{\cdot}{A^-}$
and $\mildseq{\Gamma, A^-}{\underline{\Delta}}{\underline{U}}$, 
then $\mildseq{\Gamma}{\underline{\Delta}}{\underline{U}}$),
but the proof is not so straightforward.

\begin{figure}[t]
\[
\infer-[{\it cut}(6b)]
{\mildseq{\Gamma}{\cdot}{p^+ \otimes p^+}}
{\infer[{\it copy}]
 {\mildseq{\Gamma}{\cdot}{p^+}}
 {\infer[{\lolli}_L]
  {\mildseq{\Gamma}{q^+ \lolli {\uparrow}p^+}{p^+}}
  {\infer[{\it id}_2^+]
   {\mildseq{\Gamma}{\cdot}{[q^+]}}
   {}
   &
   \infer[{\uparrow}_L]
   {\mildseq{\Gamma}{[{\uparrow}p^+]}{p^+}}
   {\infer[\eta^+]
    {\mildseq{\Gamma}{p^+}{p^+}}
    {\infer[{\it focus}_R]
     {\mildseq{\Gamma}{\langle p^+ \rangle}{p^+}}
     {\infer[{\it id}_1^+]
      {\mildseq{\Gamma}{\langle p^+ \rangle}{[ p^+ ]}}
      {}}}}}}
 &
 \infer[{\it focus}_R]
 {\mildseq{\Gamma, \langle p^+ \rangle}{\cdot}
   {p^+ \otimes p^+}}
 {\infer[{\otimes}_R]
  {\mildseq{\Gamma, \langle p^+ \rangle}{\cdot}
    {[ p^+ \otimes p^+ ]}}
  {\infer[{\it id}_2^+]
   {\mildseq{\Gamma, \langle p^+ \rangle}{\cdot}
     {[ p^+ ]}}
   {}
   &
   \infer[{\it id}_2^+]
   {\mildseq{\Gamma, \langle p^+ \rangle}{\cdot}
     {[ p^+ ]}}
   {}}}}
\]\[\Longrightarrow\]\[
\infer-[{\it cut}(3)]
{\mildseq{\Gamma}{\cdot}{p^+ \otimes p^+}}
{\deduce
 {\mildseq{\Gamma}{\cdot}{p^+}}
 {\vdots}
 &
 \infer[\eta^+]
 {\mildseq{\Gamma}{p^+}{p^+ \otimes p^+}}
 {\infer-[{\it cut}(3)]
  {\mildseq{\Gamma}{\langle p^+ \rangle}{p^+ \otimes p^+}}
  {\deduce
   {\mildseq{\Gamma}{\cdot}{p^+}}
   {\vdots}
   &
   \infer[\eta^+]
   {\mildseq{\Gamma}{\langle p^+ \rangle, p^+}{p^+ \otimes p^+}}
   {\infer[{\it focus}_R]
    {\mildseq{\Gamma}{\langle p^+ \rangle, \langle p^+ \rangle}
      {p^+ \otimes p^+}}
    {\infer[{\otimes}_R]
     {\mildseq{\Gamma}{\langle p^+ \rangle, \langle p^+ \rangle}
       {[ p^+ \otimes p^+ ]}}
     {\infer[{\it id}_1^+]
      {\mildseq{\Gamma}{\langle p^+ \rangle}{[p^+]}}
      {}
      &
      \infer[{\it id}_1^+]
      {\mildseq{\Gamma}{\langle p^+ \rangle}{[p^+]}}
      {}}}}}}}
\]\[
(\mbox{where}~\Gamma = q^+ \lolli {\uparrow}p^+, \langle q^+ \rangle)
\]
\caption{A problematic cut that arises from the introduction of 
the ${\pbang}p^+$ connective}
\label{fig:bad-cut}
\end{figure}

To see why this cut is more complicated to prove than part 5 of 
Theorem~\ref{thm:lincut}, consider what it will take to reduce
the cut in the top half of Figure~\ref{fig:bad-cut}. We cannot immediately 
call the induction hypothesis on the sub-derivation in the right branch, as 
there is no way to prove $p^+ \otimes p^+$ in focus when 
$\langle p^+ \rangle$ does not appear (twice) in the linear context. 
We need to get two suspended $\langle p^+ \rangle$ antecedents
in the linear context; then we can replace all the instances of
${\it id}_2^+$ with instances of ${\it id}_1^+$ that use freshly-minted
$\langle p^+ \rangle$ antecedents. This can be achieved with 
repeated application of part 3 of Theorem~\ref{thm:lincut}, as shown
in the bottom half of Figure~\ref{fig:bad-cut}.

The minimal extension to cut admissibility (Theorem~\ref{thm:lincut})
that justifies the atom optimization appears to be the following,
where $\langle p^+ \rangle^n$ denotes $n$ copies of the suspended
positive proposition $\langle p^+ \rangle$.

\bigskip
\begin{theorem}[Extra cases of cut admissibility (Theorem~\ref{thm:lincut})]
~
\begin{enumerate}
\item[6a.] 
  If $\mildseq{\Gamma}{\cdot}{p^+}$ 
  and $\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{[ B^+ ]}$, 
  then there exists $n$ such that
      $\mildseq{\Gamma}
          {\Delta, \langle p^+ \rangle^n}
          {[ B^+ ]}$.
\item[6b.] 
  If $\mildseq{\Gamma}{\cdot}{p^+}$
  and $\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{U}$,
  then $\mildseq{\Gamma}{\Delta}{U}$. 
\item[6c.]
  If $\mildseq{\Gamma}{\cdot}{p^+}$
  and $\mildseq{\Gamma, \langle p^+ \rangle}{\Delta, [B^-]}{U}$,
  then there exists $n$ such that
      $\mildseq{\Gamma}
          {\Delta, \langle p^+ \rangle^n, [B^-]}
          {U}$.
\end{enumerate}
\end{theorem}

\begin{proof}
Induction on the second given derivation; whenever ${\it focus}_R$,
${\it focus}_L$ or ${\it copy}$ are the last rule in part $6b$, we need
to make $n$ calls to part 3 of the cut admissibility lemma, each one
followed by a use of the $\eta^+$ rule, where
$n$ is determined by the inductive call to part $6a$ (for ${\it focus}_R$) 
or $6c$ (for ${\it focus}_L$ and ${\it copy}$). 

The calls to part 3 are justified by the existing induction metric: the 
principal cut formula $p^+$ stays the same and the part number gets smaller.
\end{proof}


\paragraph{Correctness of focusing}

The obvious way of extending erasure for our extended logic is
to let $({\pbang}p^+)^\circ = {!}p^+$ and  
to let $(\Gamma, \langle p^+ \rangle)^\circ = (\Gamma)^\circ, p^+$. 
Under this interpretation, the soundness of ${\pbang}_L$ and ${\pbang}_R$
has the same structure as the soundness of ${!}_L$ and ${!}_R$, and the
soundness of ${\it id}_2^+$ in the focused system is
established with ${\it copy}$ and ${\it id}$ in the unfocused system:
\[
\infer[{\it copy}]
{\seq{\Gamma^\circ, p^+}{\cdot}{p^+}}
{\infer[{\it id}]
 {\seq{\Gamma^\circ, p^+}{p^+}{p^+}}
 {}}
\]

The extension to the proof of completeness requires two additional cases
to deal with ${\pbang}$,
both of which are derivable\ldots
\[
\infer-[{\pbang}_{uR}]
{\mildseq{\Gamma}{\cdot}
  {{\downarrow}{\uparrow}\ldots{\downarrow}{\uparrow}{\pbang}p^+}}
{\mildseq{\Gamma}{\cdot}{p^+}}
\qquad
\infer-[{\pbang}_{uL}]
{\mildseq{\Gamma}
  {\Delta, {\uparrow}{\downarrow}\ldots{\downarrow}{\uparrow}{\pbang}p^+}
  {U}}
{\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{U}}
\]
\ldots as well as a case dealing with the
situation where we apply ${\it copy}$ to the erasure
of a persistent suspended proposition. This case reduces to
a case of ordinary focal substitution:
\[
\infer-[\langle {\it copy} \rangle_{u}]
{\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{U}}
{\mildseq{\Gamma, \langle p^+ \rangle}{\Delta, \langle p^+ \rangle}{U}}
\quad
\deduce
{\mathstrut}
{=\mathstrut}
\quad
\infer-[{\it subst}^+]
{\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{U}}
{\infer[{\it id}^+_2]
 {\mildseq{\Gamma, \langle p^+ \rangle}{\cdot}{[p^+]}}
 {}
 &
 {\mildseq{\Gamma, \langle p^+ \rangle}{\Delta, \langle p^+ \rangle}{U}}
 }
\]

For such a seemingly simple change, the atom optimization adds a
surprising amount of complexity to the cut admissibility theorem for
focused linear logic. What's more, the three extra cases of cut that
we had to introduce were all for the purpose of handling a single 
problematic case
in the proof of part 1 where both derivations were decomposing the
principal cut formula ${\pbang}p^+$.

\subsection{Exponential optimization}
\label{sec:bangopt}

The choice of adding ${\pbang}p^+$ as a special new connective instead
of defining it as ${!}{\uparrow}p^+$ paves the way for us to modify
its meaning further. For instance, there turns out to be no internal
reason for the ${\pbang}_R$ rule to lose focus in its premise, even
though it is critical that ${!}_R$ lose focus on its
premise; if we fail to do so propositions like ${!}(p^+ \otimes
  q^+) \lolli {!}(q^+ \otimes p^+)$ will have no proof. We can revise
${\pbang}_R$ accordingly.
\[
\infer[{\pbang}_R]
{\mildseq{\Gamma}{\cdot}{[{\pbang}p^+]}}
{\mildseq{\Gamma}{\cdot}{[p^+]}}
\quad
\infer[{\pbang}_L]
{\mildseq{\Gamma}{\Delta, {\pbang}p^+}{U}}
{\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{U}}
\quad
\infer[{\it id}^+_1]
{\mildseq{\Gamma}{\langle A^+ \rangle}{[A^+]}}
{}
\quad
\infer[{\it id}^+_2]
{\mildseq{\Gamma, \langle A^+ \rangle}{\cdot}{[A^+]}}
{}
\]
This further optimization
can be called the {\it exponential optimization}, as it, like the atom 
optimization, potentially reduces the number of focusing phases
in a proof. Identity expansion is trivial to modify, and cut 
admissibility is significantly simpler. 

The problematic case of cut is easy to handle 
in this modified system: 
we can conclude by case analysis
that the first given derivation 
must prove $p^+$ in focus using the ${\it id}_2^+$ rule. This, in turn,
means that $\langle p^+ \rangle$ must already appear in $\Gamma$,
so $\Gamma = \Gamma', \langle p^+ \rangle$, and 
the cut reduces to an admissible instance of contraction.
\[
\infer-[{\it cut}(1)]
{\mildseq{\Gamma', \langle p^+ \rangle}{\Delta}{U}}
{\infer[{\pbang}_R]
 {\mildseq{\Gamma', \langle p^+ \rangle}{\cdot}{[{\pbang}p^+]}}
 {\infer[{\it id}_2^+]
  {\mildseq{\Gamma', \langle p^+ \rangle}{\cdot}{[p^+]}}
  {}}
 & 
 \infer[{\pbang}_L]
 {\mildseq{\Gamma', \langle p^+ \rangle}{\Delta, {\pbang}p^+}{U}}
 {\mildseq{\Gamma', \langle p^+ \rangle, \langle p^+ \rangle}{\Delta}{U}}}
\quad
\deduce
{\mathstrut}
{\Longrightarrow}
\quad
\infer-[{\it contract}]
{\mildseq{\Gamma', \langle p^+ \rangle}{\Delta}{U}}
{\mildseq{\Gamma', \langle p^+ \rangle, \langle p^+ \rangle}{\Delta}{U}}
\]
Thus, we no longer need the complicated extra parts $6a$ - $6c$ of cut 
admissibility in order to prove cut admissibility for a focused system
with the exponential optimization. 

Because cut and identity hold, we can think of a 
focused logic with the exponential optimization as being 
internally sensible. The problem is that this logic is no longer
{\it externally} sensible relative to normal linear logic, because
we cannot erase ${\pbang}p^+$ into regular
linear logic in a sensible way. 
Specifically, if we continue to define $({\pbang}p^+)^\circ$ as
${!}p^+$, then ${\pbang}q^+ \lolli {!}(q^+
\lolli {\uparrow}p^+) \lolli {\uparrow}{\pbang}p^+$ has no proof in
focused linear logic, whereas its erasure, ${!}q^+ \lolli {!}(q^+
\lolli p^+) \lolli {!}p^+$, does have an unfocused proof. In other
words, the completeness of focusing (Theorem~\ref{thm:linfoccomplete})
no longer holds under the exponential optimization!

Our focused logic with the exponential optimization has some resemblance to
tensor logic \cite{mellies10resource}, as well the polarized logic
that Girard presented in a note, ``On the sex of angels,'' which first
introduced the ${\uparrow}A^+$ and ${\downarrow}A^-$ notation to the
discussion of polarity \cite{girard91sex}. Both of these presentations
incorporate a general focus-preserving ${\pbang}A^+$ connective -- a
positive formula with a positive subformula -- in lieu of the
focus-interrupting ${!}A^-$ connective.  Both presentations also have
the prominent caveat that ${!}$ in the unfocused logic necessarily
corresponds to ${\pbang}{\downarrow}$ in the focused logic: it is {\it
  not} possible to derive ${\pbang}(A \otimes B) \vdash {\pbang}(B
\otimes A)$ in these systems, and no apology is made for this fact,
because ${\pbang}{\downarrow}{\uparrow}(A \otimes B) \vdash
{\pbang}{\downarrow}{\uparrow}(B \otimes A)$ holds as expected.  We
want avoid this route because it gives the shifts too much power: they
influence the {\it existence} of proofs, not just the structure of
proofs.\footnote{Both the note of Girard and the paper of Melli{\`e}s
  and Tabareau see the shifts as a form of negation; therefore, 
  writing from an intuitionistic perspective, they
  are unconcerned that $A^+$ has a different meaning of
  ${\downarrow}{\uparrow}A^+$ in their constructive logic. There
  are many propositions where $\neg\neg A$ is provable even though 
  $A$ is not! This view of shifts as negations seems rather
  foreign to the erasure-based understanding of shifts we have been
  discussing, though Zeilberger has attempted to reconcile these
  viewpoints \cite{zeilberger08unity}.} This interpretation of shifts
therefore threatens our critical ability to intuitively understand and
explain linear logic connectives as resources.

There is an easily identifiable class of sequents that obey {\it
  separation}, which is the property that positive atomic propositions
can be separated into two classes $p^+_l$ and $p^+_p$. The {\it
  linear} positive propositions $p^+_l$ are never suspended in the
persistent context and never appear as ${\pbang}p^+_l$, whereas the
{\it persistent} positive propositions $p^+_p$ are never suspended in
the linear context and always appear as ${\pbang}p^+_p$ inside of
other propositions. For sequents and formulas obeying separation, we
can use the obvious erasure operation and obtain a proof of the
completeness of focusing; this notion of separation was the basis of
our completeness result in \cite{pfenning09substructural}.  However,
separation is a meta-logical property, something that we observe about
a fragment of the logic and not an inherent property of the logic
itself. There are many propositions $A^+$ and $A^-$ that we cannot
prove in focused linear logic with the exponential optimization even though
$(A^+)^\circ$ and $(A^-)^\circ$ are provable in linear logic, and that
makes the exponential optimization unsatisfactory.

The remaining two approaches, adjoint logic and the introduction of
permeable atomic propositions, can both be seen as attempts to turn
separation into a logical property instead of a meta-logical property.

\subsection{Adjoint logic}
\label{sec:moreprim}

We introduced ${\pbang}p^+$ as a connective defined as
${!}{\downarrow}p^+$ -- that is, the regular ${!}A^-$ connective plus
a little something extra, the shift. After our experience with
modifying the rules of ${\pbang}$, we can motivate adjoint logic by
trying to view ${\pbang}$ as a more primitive connective -- that is,
we will try to view ${!}$ as ${\pbang}$ plus a little something extra.

It is frequently observed that the exponential ${!}A$ of linear logic
appears to have two or more parts; the general idea is that ${\pbang}$
represents just one of those pieces. Accounts of linear logic that
follow the judgmental methodology of Martin-L{\"o}f
\cite{lof96meanings}, such as the analysis by Chang et al.
\cite{chang03judgmental}, emphasize that the regular hypothetical
sequent $\seq{\Gamma}{\Delta}{A}$ of linear logic is establishing the
judgment that $A$ is {\it true}: we can write
$\seq{\Gamma}{\Delta}{\isconc{A}}$ to emphasize this. The judgment of
{\it validity},
represented by the judgment $\isvalid{A}$, is defined as
truth using no ephemeral resources, and ${!}A$ is understood as the
internalization of judgmental validity:
\[
\infer[{\it valid}]
{\pseq{\Gamma}{\isvalid{A}}}
{\seq{\Gamma}{\cdot}{\isconc{A}}}
\quad
\infer[{!}'_R]
{\seq{\Gamma}{\Delta}{\isconc{{!}A}}}
{\Delta = \cdot ~~~ & \pseq{\Gamma}{\isvalid{A}}}
\]
The ${\it valid}$ rule is invertible, so if we ever need to prove
$\pseq{\Gamma}{\isvalid{A}}$, we may asynchronously transition to proving
$\seq{\Gamma}{\cdot}{\isconc{A}}$. This observation is used to explain
why we don't normally consider validity on the right in linear
logic. Our more familiar
rule for ${!}_R$ is derivable using these two rules:
\[
\infer[{!}'_R]
{\Gamma; \Delta \vdash {!}\isconc{A} \mathstrut}
{\Delta = \cdot
 &
 \infer[{\it valid}]
 {\Gamma \vdash \isvalid{A} \mathstrut}
 {\Gamma; \cdot \vdash \isconc{A}} \mathstrut}
\]

Note that the ${!}'_R$ rule is not invertible,
because it forces the linear context to be empty, which means ${!}$
must be positive. The ${\it valid}$
rule, on the other hand, is invertible and has an
asynchronous or negative character,
because it represents the invertible step of deciding to prove that
$A$ is {\it valid} (true without recourse to any ephemeral resources)
by proving that it is {\it true}
(in a context with no ephemeral resources). This combination of
positive and negative actions explains why ${!}A^-$ is a positive
proposition with a negative subformula, and similarly explains why we
must break focus when we reach ${!}A$ on the right and why we must
stop decomposing the proposition when we reach ${!}A$ on the left.
The salient feature of the exponential optimization's rules for ${\pbang}p^+$, of
course, is that they do {\it not} break focus on the right and that
they continue to decompose the proposition on the left (into a
suspended proposition $\langle p^+ \rangle$ in the persistent
context). This is the reason for arguing that ${\pbang}$ captures only
the first, purely positive, component of the ${!}$ connective.

If the $\pbang$ connective is the first part of the $!$ connective,
can we characterize the rest of the connective? Giving a reasonable
answer necessarily requires a more general account of the $\pbang$
connective -- a {\it unfocused} logic where it is generally applicable
rather than restricted to positive atomic propositions. In other
words, to account for the behavior of $\pbang$, we must give a more
primitive logic into which linear logic can be faithfully encoded.

\input{figs/fig-fragment-adjoint}

A candidate for a more primitive logic, and one that has tacitly
formed the basis of much of my previous work on logic programming and
logical specification in substructural logic
\cite{pfenning09substructural,simmons11weak,simmons11logical}, 
is {\it adjoint logic}.
Adjoint logic was first characterized by Benton and Wadler as a
natural deduction system \cite{benton96linear} and was substantially
generalized by Reed in a sequent calculus setting
\cite{reed09judgmental}. The logic generalizes both linear logic and
Fairtlough and Mendler's lax logic \cite{fairtlough97propositional}
as sub-languages of a common logic, whose propositions come
in two syntactically distinct categories that are connected by the
adjoint operators $F$ and $G$:
\begin{align*}
\mbox{\it Persistent propositions} & &
X, Y, Z & ::= G A \mid x \mid X \supset Y \mid X \times Y\\
\mbox{\it Linear propositions} & & 
A, B, C & ::= F X \mid a \mid A \lolli B \mid A \otimes B
\end{align*}
In adjoint logic, persistent propositions $X$ appear in the
persistent context $\Gamma$ and as the succedents of sequents
$\pseq{\Gamma}{X}$, whereas linear propositions $A$ appear in
the linear context $\Delta$ and as the succedents of sequents
$\seq{\Gamma}{\Delta}{A}$. Going back to our previous discussion,
this means that persistent propositions are only ever judged
to be valid, and that linear propositions are only ever judged
to be true. A fragment of the logic is shown in
Figure~\ref{fig:fragment-adjoint}.  Note the similarity between the
$G_L$ rule and our unfocused ${\it copy}$ rule, as well as the
similarity between $F_R$ and $G_R$ in
Figure~\ref{fig:fragment-adjoint} and the rules ${!}_R$ and ${\it
  valid}$ in the previous discussion.  Linear logic is recovered as a
fragment of adjoint logic by removing all of the persistent
propositions except for $GA$; the usual ${!}A$ is then definable as
$FGA$.\footnote{Lax logic, on the other hand, is recovered by removing
  all of the linear propositions except for $FX$; the distinguishing
  connective of lax logic, $\ocircle X$, is then definable as $GFX$.}

One drawback of this approach is simply the logistics of giving 
a fully focused presentation
of adjoint logic. We end up with a proliferation of propositions,
because the syntactic distinction between $X$ and $A$ is orthogonal to
the syntactic distinction between positive and negative
propositions. A polarized presentation of adjoint logic would have
four syntactic categories: $X^+$, $X^-$, $A^+$, and $A^-$, with one
pair of shifts mediating between $X^+$ and $X^-$ and another pair of
shifts mediating between $A^+$ and $A^-$.\footnote{To make matters
  worse, in Levy's Call-By-Push-Value language, the programming
  language formalism that corresponds to polarized logic, ${\uparrow}$
  and ${\downarrow}$ are characterized as adjoints as well ($F$ and
  $U$, respectively), so a fully polarized adjoint logic has {\it
    three} distinct pairs of unary connectives that can be
  characterized as adjoints!}
Given a focused presentation of adjoint logic, however, 
the separation criteria discussed above can be
in terms of the two forms of positive atomic proposition $a$ and $x$. 
Positive atomic propositions that are always associated with
${\pbang}$ can be encoded as persistent positive atomic
propositions $x^+$, whereas positive atomic propositions that are never
associated with ${\pbang}$ can be encoded as linear positive atomic
propositions $a^+$. The proposition ${\pbang}p^+$ can then be
translated as $F x^+$, where $x^+$ is the translation of $p^+$ as a
persistent positive atomic proposition.

Adjoint logic gives one answer to why, in Andreoli-style
presentations of linear logic, we can't easily substitute positive
propositions for positive atomic propositions when those positive
atomic propositions appear suspended in the persistent linear context:
because these propositions are actually stand-ins for {\it persistent}
propositions, not for linear propositions, and we are working in a
fragment of the logic that has no interesting persistent propositions
other than atomic propositions $x$ and the negative inclusion $G A$
back into linear propositions.  This effectively captures the
structure of the separation requirement (as defined at the end of
Section~\ref{sec:bangopt} above) in a logical way, but it makes the
structure of persistent atomic propositions rather barren and
degenerate, and it places an extra logic, adjoint logic, between the
focused system and our original presentation of intuitionistic linear
logic.

% Adjoint logic, because it requires a syntactic differentiation of
% persistent and linear atomic propositions, still does not allow us to
% pleasantly embed the structure of focused linear logic with the atom
% and bang optimizations when separation is not enforced.  The flaw is
% due to the atom optimization, which allows us to successfully right
% focus on an positive atomic proposition $p^+$ if the proposition
% appears suspended in the linear context {\it or} in the persistent
% context. This is not simple to do in adjoint logic, which forces us to
% syntactically specify the context where we expect to find any given
% proposition. A more complicated translation in to adjoint logic that
% associated each atomic proposition $p^+$ with a persistent proposition
% $x_p^+$ and a linear proposition $a_p^+$ would suffice; a subgoal
% $p^+$ could be represented as $a_p^+ \oplus F x_p^+$, for instance.
% Such a translation would be unsatisfactory both because it further
% complicates the previously simple interpretation of atomic
% propositions and because the translation of an atomic proposition is
% non-uniform -- it depends on knowing whether
% an atomic proposition will ultimately appear on the left or the right
% side of the sequent.

% I conjecture that a variant of adjoint logic which does {\it
%   not} syntactically differentiate between persistent and linear
% propositions might be a better target for faithfully embedding focused
% linear logic with and without the atom and bang optimizations; but
% this is outside the scope of this thesis.

\subsection{Permeability}
\label{sec:permeable}

Let us review the problems with our previous attempts to motivate a
satisfactory treatment of positive propositions in the persistent
context.  Andreoli's atom optimization interferes with the structure
of cut admissibility. The exponential optimization lacks a good
interpretation in unfocused linear logic. The adjoint formulation of
linear logic introduces persistent positive propositions as members of
a syntactic class $X$ of persistent propositions, a syntactic class
that usually lies hidden in between the two right-synchronous and
right-asynchronous (that is, positive and negative) halves of the
${!}$ connective. This approach works but requires a lot of extra
machinery.

Our final attempt to logically motivate a notion of a persistent
positive proposition will be based on an analysis of {\it
  permeability}.  Permeability in classical presentations of linear
logic dates back to Girard's LU \cite{girard93unity}.  In this
section, we will motivate permeable atomic propositions in
intuitionistic linear logic by first considering a new identity
expansion principle that only applies to permeable propositions, a
syntactic refinement of the positive propositions.\footnote{Permeable
  {\it negative} propositions are relevant to classical linear logic,
  but the asymmetry of intuitionistic linear logic means that, for
  now, it is reasonable to consider permeability exclusively as a
  property of positive propositions. We will consider a certain kind
  of right-permeable propositions in Chapter~\ref{chapter-order}.}

The admissible identity expansion rules, like the admissible identity
rule present in most unfocused sequent calculus systems, help us
write down compact proofs. If $F(n) = p_1^+ \otimes \ldots \otimes
p_n^+$, then the number of steps in the smallest proof of
$\mildseq{\Gamma}{F(n)}{F(n)}$ is in $\Omega(n)$. However, by using
the admissible identity expansion rule $\eta^+$, we can represent the
proof in a compact way:
\[
{\infer-[\eta^+]
{\mildseq{\Gamma}{F(n)}{F(n)}}
{{\infer[{\it focus}_R]
  {\mildseq{\Gamma}{\susp{F(n)}}{F(n)}}
  {\infer[{\it id}^+]
   {\mildseq{\Gamma}{\susp{F(n)}}{[ F(n) ]}}
   {}}}}}
\]
%The use of identity expansion lets write down potentially large proofs 
%quite concisely.

\subsubsection{Permeability as a property of identity expansion}

The pattern we want to capture with our new version of identity
expansion is the situation where we are trying to prove a sequent like
$\mildseq{\Gamma}{\Delta}{\one}$ or $\mildseq{\Gamma}{\Delta}{{!}A^-}$
and we know, by the syntactic structure of $\Delta$, that inversion
will empty the linear context. One instance of this pattern is the 
sequent
$\mildseq{\Gamma}{G(n)}{{!}{\uparrow}G(n)}$ where $G(n) = {!}p_1^+ \otimes
\ldots \otimes {!}p_n^+$. Our goal will be to 
prove such a sequent succinctly 
by suspending the proposition $G(n)$
directly in the persistent context just as we did with the proof
involving $F(n)$ above. To use these suspended
propositions, we introduce a hypothesis rule for positive propositions
suspended in the persistent context.
\[
\infer[{\it id}^+_p]
{\mildseq{\Gamma, \langle A^+ \rangle}{\cdot}{[A^+]}}
{}
\]
This rule is, of course,
exactly the ${\it id}^+_2$ rule from our discussion of Andreoli's
system.
There is also a focal substitution principle, Theorem~\ref{thm:fsubst-posper}. 
This theorem was
true in Andreoli's system, but we did not need or discuss it.

\bigskip
\begin{theorem}[Focal substitution (positive, persistent)]
\label{thm:fsubst-posper}~\\
If $\mildseq{\Gamma}{\cdot}{[A^+]}$ 
and $\mildseq{\Gamma, \langle A^+ \rangle}{\underline{\Delta'}}
      {\underline{U}}$, 
then $\mildseq{\Gamma}{\underline{\Delta'}}{\underline{U}}$.
\end{theorem}

\begin{proof}
  Once again, this is a straightforward induction over the second
  given derivation, as in a proof of regular substitution in a natural
  deduction system. If the second derivation is the axiom ${\it
    id}_p^+$ applied to the suspended proposition $\langle A^+
  \rangle$ we are substituting for, then the result follows
  immediately using the first given derivation.
\end{proof}

Given this focal substitution principle, we can consider the class of
{\it permeable} positive  propositions. A permeable 
proposition is one where, when we use the admissible $\eta^+$ rule to
suspend it in the linear context, we might just as well suspend it in
the persistent context, as it decomposes entirely into persistent
pieces. In other words, we want a class of propositions $A^+_p$ such
that $\mildseq{\Gamma, \langle A^+_p \rangle}{\Delta}{U}$ implies
$\mildseq{\Gamma}{\Delta, A^+_p}{U}$; this is the permeable identity
expansion property. It is possible to precisely characterize the MELL
propositions that are permeable as a syntactic refinement of positive
propositions:
\[
A^+_p ::= {!}A^- \mid \one \mid A^+_p \otimes B^+_p
\]
In full first-order linear logic, $\zero$, $A^+_p \oplus B^+_p$, and
$\exists x. A^+_p$ would be included as well; essentially only $p^+$
and ${\downarrow}A^-$ are excluded from this fragment. 
%
% A more semantic
% critera for the fragment we are interested in is that it is the set of
% positive propositions such that, whenever
% $\mildseq{\Gamma}{\Delta}{[A^+]}$ is derivable, we know that $\Delta =
% \cdot$.  These amount to the same thing, however: a focus that results
% in proving $p^+$ will only succeed when the linear context is
% non-empty, and a focus that results in proving ${\downarrow}A^-$ can
% succeed in a non-empty linear context if $\Gamma$ contains the
% proposition $p^+ \lolli A^-$.

\bigskip
\begin{theorem}[Permeable identity expansion]\label{thm:permident}
If $\mildseq{\Gamma, \langle A^+_p \rangle}{\Delta}{U}$, 
then $\mildseq{\Gamma}{\Delta, A^+_p}{U}$.
\end{theorem}

\begin{proof}
Induction over the structure of the proposition $A_p^+$ or $A^-$.
The cases of
this proof are represented in Figure~\ref{fig:lineta-3}.
\end{proof}

\input{figs/fig-lineta-3}

As admissible rules, Theorems \ref{thm:fsubst-posper} and \ref{thm:permident}
are written ${\it subst}^+_p$ and ${\eta}^+_p$:
\[
\infer-[{\it subst}^+_p]
{\mildseq{\Gamma}{\underline{\Delta}}{\underline{U}}}
{\mildseq{\Gamma}{\cdot}{[A^+]}
 &
 \mildseq{\Gamma, \langle A^+ \rangle}{\underline{\Delta}}{\underline{U}}}
\qquad
\infer-[\eta^+_p]
{\mildseq{\Gamma}{\Delta, A^+_p}{U}}
{\mildseq{\Gamma, \langle A^+_p \rangle}{\Delta}{U}}
\]
We can use this persistent identity expansion property to
give a compressed proof of our motivating example:
\[
{\infer-[\eta^+_p]
{\mildseq{\Gamma}{G(n)}{{!}G(n)}}
{{\infer[{\it focus}_R]
  {\mildseq{\Gamma, \susp{G(n)}}{\cdot}{{!}G(n)}}
  {\infer[{!}_R]
   {\mildseq{\Gamma, \susp{G(n)}}{\cdot}{[{!}G(n)]}}
   {\infer[{\it id}^+]
   {\mildseq{\Gamma}{\susp{G(n)}}{[ G(n) ]}}
   {}}}}}}
\]

% \[
% \infer-[\eta^+_p]
% {\mildseq{\Gamma}
%   {{!}A 
%    \otimes {!}B 
%    \otimes {!}C 
%    \otimes {!}D}
%   {\one}}
% {\infer[{\it focus}_R]
%  {\mildseq
%    {\Gamma, 
%     \langle {!}A 
%     \otimes {!}B 
%     \otimes {!}C 
%     \otimes {!}D \rangle}
%    {\cdot}
%    {\one}}
%  {\infer[{\one}_R]
%   {\mildseq
%     {\Gamma,
%      \langle {!}A 
%      \otimes {!}B 
%      \otimes {!}C 
%      \otimes {!}D \rangle}
%     {\cdot}
%     {[\one]}}
%   {}}}
% \]

\subsubsection{Permeable atomic propositions}
\label{sec:permable-atomic}

It would have been possible, in the discussion of focused linear logic
in Section~\ref{sec:foclinlog}, to present identity expansion as
conceptually prior to atomic propositions. In such a retelling, the
$\eta^+$ and $\eta^-$ rules can be motivated as the necessary base
cases of identity expansion when we have propositional variables that
stand for unknown positive and negative propositions,
respectively. Conversely, we can now present a new class of {\it
  permeable} atomic propositions $p^+_p$ that stand in for arbitrary
permeable propositions $A^+_p$. These add a new base case to permeable
identity expansion (Theorem~\ref{thm:permident}) that can only be
satisfied with an explicit $\eta^+_p$ rule:
\[
\infer[\eta^+_p]
{\mildseq{\Gamma}{\Delta, p^+_p}{U}}
{\mildseq{\Gamma, \langle p^+_p \rangle}{\Delta}{U}}
\]
Because the permeable propositions are a syntactic refinement of the
positive propositions, $p^+_p$ must be a valid positive atomic proposition
as well. This is the revised grammar for 
intuitionistic MELL with permeable atomic propositions:
\begin{align*}
A^+ & ::= p^+ \mid p^+_p \mid {\downarrow}A^- \mid {!}A^- \mid \one \mid A^+ \otimes B^+\\
A^+_p & ::= p^+_p \mid {!}A^- \mid \one \mid A^+_p \otimes B^+_p\\ 
A^- & ::= p^- \mid {\uparrow}A^+ \mid A^+ \lolli B^-
\end{align*}

This addition to the logic requires some additions to 
positive identity expansion, cut admissibility, and completeness, but
none of the changes are too severe; we consider each in turn.

\paragraph{Identity expansion}
The new addition to the language of positive propositions requires us to 
extend identity expansion with one additional case:
\[
\infer-[\eta^+_p]
{\mildseq{\Gamma}{\Delta, p^+_p}{U}}
{\mildseq{\Gamma}{\Delta, \langle p^+_p \rangle}{U}}
\quad
\deduce{\mathstrut}{\mathstrut \Longrightarrow}
\infer[\eta^+_p]
{\mildseq{\Gamma}{\Delta, p^+_p}{U}}
{\infer-[{\it subst}^+_p]
 {\mildseq{\Gamma, \langle p^+_p \rangle}{\Delta}{U}}
 {\infer[{\it id}^+_p] 
  {\mildseq{\Gamma, \langle p^+_p \rangle}{\cdot}{[p^+_p]}}
  {}
  &
  \infer-[{\it weaken}]
  {\mildseq{\Gamma, \langle p^+_p \rangle}{\Delta, \langle p^+_p \rangle}{U}}
  {\mildseq{\Gamma}{\Delta, \langle p^+_p \rangle}{U}}}}
\]

\paragraph{Cut admissibility}
We must clarify the restriction on cut admissibility for our extended
logic. In Theorem~\ref{thm:lincut}, we required that sequents contain
only suspensions of atomic propositions, and in our generalization of
cut admissibility, we need to further require that all suspensions in
the persistent context $\Gamma$ be permeable and atomic and that all
suspensions in the linear context $\Delta$ be non-permeable and
atomic.  Under this restriction, the proof proceeds much as it did for
the system with the exponential optimization.

\paragraph{Correctness of focusing} There are two ways we can
understand the soundness and completeness of focusing for linear logic
extended with permeable atomic propositions. One option is to add a
notion of permeable atomic propositions to our core linear logic from
Figure~\ref{fig:linear}, in which case soundness and completeness are
straightforward. Alternatively, we can use our intuition that a
permeable proposition $A$ is interprovable with ${!}A$ and let
$(p^+_p)^\circ = {!}p^+_p$. 

The erasure of permeable propositions $p^+_p$ in the focused
logic to ${!}p^+_p$ in the unfocused logic reveals that
permeable propositions, which we motivated entirely from a discussion
of identity expansion, are effectively a logical treatment of
separation. Rather than ${\pbang}$, a separate proposition that we
apply only to positive propositions, permeability is a
property intrinsic to a given atomic proposition, much like the
proposition's positivity or negativity.

\section{Revisiting our notation}
\label{sec:linnote}

Andreoli, in his 2001 paper introducing the idea of synthetic
inference rules \cite{andreoli01focussing}, observed that the atom
optimization can lead to an exponential explosion in the number of
synthetic rules associated with a proposition.  For instance, if $a^+
\otimes b^+ \lolli {\uparrow}c^+$ appears in $\Gamma$, the atom
optimization means that the following are all synthetic inference
rules for that proposition:
\[
\infer
{\mildseq{\Gamma}{\Delta, \langle a^+ \rangle, \langle b^+ \rangle}{U}}
{\mildseq{\Gamma}{\Delta, \langle c^+ \rangle}{U}}
\quad
\infer
{\mildseq{\Gamma, \langle a^+ \rangle}{\Delta, \langle b^+ \rangle}{U}}
{\mildseq{\Gamma, \langle a^+ \rangle}{\Delta, \langle c^+ \rangle}{U}}
\]\[
\infer
{\mildseq{\Gamma, \langle b^+ \rangle}{\Delta, \langle a^+ \rangle}{U}}
{\mildseq{\Gamma, \langle b^+ \rangle}{\Delta, \langle c^+ \rangle}{U}}
\quad
\infer
{\mildseq{\Gamma, \langle a^+ \rangle, \langle b^+ \rangle}{\Delta}{U}}
{\mildseq{\Gamma, \langle a^+ \rangle, \langle b^+ \rangle}
   {\Delta, \langle c^+ \rangle}{U}}
\]
Andreoli suggests coping with this problem by restricting the form of
propositions so that positive atoms never appear in the persistent
context. From our perspective, this is a rather unusual
recommendation, since it just returns us to linear logic without the
atom optimization! The focused system in Section~\ref{sec:foclinlog},
which we have argued is a more fundamental presentation
(following Chaudhuri), effectively avoid this problem.

However, it's not necessary to view Andreoli's proliferation of rules
as a problem with the logic; rather, it is possible to view it merely
as a problem of notation. It is already the case that, in writing
sequent calculus rules, we tacitly use of a fairly large number of
notational conventions, at least relative to Gentzen's original
formulation where all contexts were treated as sequences of
propositions \cite{gentzen35untersuchungen}.  For instance, the
bottom-up reading of the ${\one}_R$ rule's conclusion,
$\mildseq{\Gamma}{\cdot}{[\one]}$, indicates the presence of an
additional premise checking that the linear context is empty, and the
conclusion $\mildseq{\Gamma}{\Delta_1, \Delta_2}{[A \otimes B]}$ of
the ${\tensor}_R$ rule indicates the condition that the context can be
split into two parts. In other words, both the conclusion of the
${\one}_R$ rule and ${\otimes}_R$ rule, as we normally write them, can
be seen as having special {\it matching constructs} that constrain the
shape of the context $\Delta$.\footnote{More than anything else we have
  discussed so far, this is a view of inference rules that emphasizes
  {\it bottom-up} proof search and proof construction. A view of
  linear logic that is informed by the inverse method, or top-down
  proof construction, is bound to look very different (see, for
  example, \cite{chaudhuri06focused}).}

\input{figs/fig-linear-alt}

I propose to deal with the apparent proliferation of synthetic rules
in a system with the atom optimization by adding a new matching
construct for the conclusion of rules. We can say that $\Gamma;
\Delta$ matches $\Gamma; \Delta' / \langle p^+ \rangle$ either when
$\langle p^+ \rangle \in \Gamma$ and $\Delta = \Delta'$ or when
$\Delta = (\Delta', \langle p^+ \rangle)$. We can also iterate this
construction, so that $\Gamma; \Delta$ matches $\Gamma; \Delta_n /
\langle p^+_1 \rangle, \ldots, \langle p^+_n \rangle$ if $\Gamma;
\Delta$ matches $\Gamma; \Delta_1 / \langle p^+_1 \rangle$, $\Gamma;
\Delta_1$ matches $\Gamma; \Delta_2 / \langle p^+_2 \rangle$, \ldots
and $\Gamma; \Delta_{n-1}$ matches $\Gamma; \Delta_n / \langle p^+_n
\rangle$.  Armed with this notation, we can create a concise synthetic
connective that is equivalent to the four of the rules discussed
previously:
\[
\infer
{\mildseq{\Gamma}{\Delta/\langle a^+ \rangle, \langle b^+ \rangle}{U}}
{\mildseq{\Gamma}{\Delta, \langle c^+ \rangle}{U}}
\]

This modified notation need not be reserved for synthetic connectives;
we can also use it to combine the two positive identity rules ${\it
  id}^+_1$ and ${\it id}^+_2$ (in the exponential-optimized system) or,
equivalently, ${\it id}^+$ and ${\it id}^+_p$ (in the system
incorporating permeability).  Furthermore, by giving $\Gamma; \Delta /
A^-$ the obviously analogous meaning, we can fuse the ${\it
  focus}_L$ rule and the ${\it copy}$ rule into a single rule that
is unconcerned with whether the proposition in question came from the
persistent or linear contexts:
\[
\infer[{\it id}^+]
{\mildseq{\Gamma}{\cdot/\langle A^+ \rangle}{[A^+]}}
{}
\quad
\infer[{\it focus}^*_L]
{\mildseq{\Gamma}{\Delta/A^-}{U}}
{\mildseq{\Gamma}{\Delta, [A^-]}{U}}
\]

Going yet one more step, we could use this notation to revise
the original definition of linear logic in Figure~\ref{fig:linear}.
The {\it copy} rule in that presentation sticks out as the only 
rule that doesn't deal directly with a connective, but we can eliminate
it by using the $\Gamma; \Delta/A$ matching construct. The resulting
presentation, shown in Figure~\ref{fig:linear-alt}, is equivalent
to the presentation in Figure~\ref{fig:linear}.

\bigskip
\begin{theorem}
$\seq{\Gamma}{\Delta}{C}$ if and only if $\altseq{\Gamma}{\Delta}{C}$.
\end{theorem}

\begin{proof}
The reverse direction is a straightforward induction: each rule in 
Figure~\ref{fig:linear-alt} can be translated as the related rule
in Figure~\ref{fig:linear} along with (potentially) an instance of 
the ${\it copy}$ rule.

The forward direction requires a lemma that the {\it copy} rule is
admissible according to the rules of Figure~\ref{fig:linear-alt}; this
lemma can be established by straightforward induction. Having
established the lemma, the forward direction is a straightforward
induction on derivations, applying the admissible rule whenever the 
{\it copy} rule is encountered.
\end{proof}




