
\chapter{Linear logic}

Logic as it has been traditionally understood and studied -- both in
its classical and intuitionistic varieties -- treats the truth of a
proposition as a {\it persistent resource}. That is, if we have
evidence for the truth of a proposition, we can ignore that evidence
if it is not needed and reuse the evidence as many times as we need
to. Throughout this thesis, ``logic as it has been traditionally
understood as studied'' will be referred to as {\it persistent} logic
to emphasize this treatment of evidence. 

Linear logic, which was studied and
popularized by Girard \cite{girard87linear},
treats evidence as an {\it ephemeral} resource; the use of an
ephemeral resource consumes it, at which point it is unavailable for
further use.  Linear logic, like persistent logic, comes in classical
and intuitionistic flavors. We will favor intuitionistic linear logic
in part because the propositions of intuitionistic linear logic
(written $A$, $B$, $C$, \ldots) have a more natural correspondence
with our physical intuitions about consumable resources. Linear
conjunction $A \tensor B$ represents the resource built from the
resources $A$ and $B$; if you have both a bowl of soup {\it and} a
sandwich, that resource can be represented by the proposition ${\sf
  soup} \otimes {\sf sandwich}$. Linear implication $A \lolli B$
represents a resource that can interact with another resource $A$ to
produce a resource $B$. One robot with batteries not included could be
represented as the linear resource $({\sf battery} \lolli {\sf
  robot})$, and the linear resource $({\sf 6bucks} \lolli {\sf soup}
\tensor {\sf sandwich})$ represents the ability to use \$6 to obtain
lunch -- but only once!\footnote{Conjunction will always bind more
  tightly than implication, so this is equivalent to the proposition
  ${\sf 6bucks} \lolli ({\sf soup} \tensor {\sf sandwich})$.} Linear
logic also has a modal connective ${!}A$ representing a persistent
resource that can be
used to generate any number of $A$ resources, including zero. The
Panera ``You Pick Two'' menu might be represented as
\[ {!}({\sf 6bucks} \lolli {\sf soup} \tensor {\sf sandwich}) \otimes
{!}({\sf 6bucks} \lolli {\sf soup} \tensor {\sf salad}) \otimes
{!}({\sf 6bucks} \lolli {\sf sandwich} \tensor {\sf salad}),\] as the
menu gives you the opportunity to exchange six dollars for two
distinct members of the set $\{ {\sf soup}, {\sf salad}, {\sf
  sandwich} \}$ any number of times.

\input{figs/fig-linear.tex}

Figure~\ref{fig:linear} presents a standard sequent calculus for
linear logic, in particular the so-called {\it multiplicative,
  exponential} fragment of intuitionistic linear logic (or {\it
  MELL}). It corresponds most closely to Barber's dual intuitionistic
linear logic \cite{barber96dual}, but also to Andreoli's dyadic system
\cite{andreoli92logic} and Chang et al.'s judgmental analysis of
intuitionistic linear logic \cite{chang03judgmental}.

\subsection*{Transitions in linear logic}

The propositions of intuitionistic linear logic, and linear implication
in particular, capture a notion of state change: we can {\it
  transition} from a state where we have both a ${\sf battery}$ and
the battery-less robot (represented, as before, by the linear
implication ${\sf battery} \lolli {\sf robot}$) to a state where we
have the battery-endowed (and therefore presumably functional) robot
(represented by the proposition ${\sf robot}$). In other words, the
proposition
%
\[{\sf battery} \otimes ({\sf battery} \lolli {\sf robot}) \lolli
{\sf robot}\] 
%
is provable in linear logic. These transitions can be chained
together as well: if we start out with ${\sf
  6bucks}$ instead of ${\sf battery}$ but we also have the
persistent ability to turn ${\sf 6bucks}$ into a ${\sf battery}$ --
just like we turned \$6 into a bowl of soup and a salad at Panera --
then we can ultimately get our working robot as well.
Written as a series of transitions, the picture looks like this:
\[
\begin{array}{ccccc}
\begin{array}{c}
\mbox{\it \$6 (1)}\medskip\\ 
\mbox{\it battery-free robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}
\end{array}
& \leadsto &
\begin{array}{c}
\mbox{\it battery  (1)}\medskip\\ 
\mbox{\it battery-free robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}
\end{array}
& \leadsto &
\begin{array}{c}
\mbox{\it robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}\medskip\\~\\
\end{array}
\end{array}
\]
In linear logic, these transitions correspond to the provability
of the proposition
\[{!}({\sf 6bucks} \lolli {\sf battery}) \otimes {\sf 6bucks} \otimes
({\sf battery} \lolli {\sf robot}) \lolli {\sf robot}.\] 
A derivation of this proposition is given in
Figure~\ref{fig:unfocused-robot}.\footnote{In Chapter XXX, I will
  argue that this view isn't quite precise enough, and that the most
  natural representation of state change from the state $A$ to the
  state $B$ isn't really captured by derivations of the proposition $A
  \lolli B$ or by derivations of the hypothetical judgment
  $\seq{\cdot}{A}{B}$.  However, this view remains a simple and useful
  one; Cervesato and Scedrov cover it thoroughly in the context of
  intuitionistic linear logic \cite{cervesato09relating}.}  

\input{figs/fig-unfocused-robot}

It is precisely because linear logic contains this natural notion of
state and state transition that a rich line of work, dating back to
Chirmar's 1995 Ph.D. thesis, has sought to use linear logic as a {\it
  logical framework} for describing stateful systems
\cite{chirimar95proof,cervesato02linear,
  cervesato02concurrent,pfenning04substructural,miller09formalizing,
  pfenning09substructural,cervesato09relating}.  

\subsection*{Logical frameworks}

Generally speaking, logical frameworks use the {\it structure} of
proofs in a logic (like linear logic) to describe the structures we're
really interested in (like the process of obtaining a robot).  There
are two related reasons why linear logic as described in
Figure~\ref{fig:linear} is not immediately useful as a logical
framework. First, the structure of the proof in
Figure~\ref{fig:unfocused-robot} doesn't really match the intuitive
two-step transition that we sketched out above. Second, there are {\it
  lots} of derivations of our example proposition according to the
rules in Figure~\ref{fig:linear}, even though there's only one
``real'' series of transitions that get us to a working robot. The use
of ${!}L$, for instance, could be permuted up past the ${\otimes}L$
and then past the ${\lolli}L$ into the left branch of the proof. These
differences represent inessential nondeterminism in proof construction
or in proof search -- they just get in the way of the structure that
we are trying to capture. 

This is a general problem in the construction of logical frameworks,
and we'll discuss two solutions in the context of LF, a logical
framework based on dependent type theory that has proved to be a
suitable means of encoding a wide variety of deductive systems, such
as logics and programming languages \cite{harper93framework}.  The
first solution is to define an appropriate equivalence class of
proofs, and the second solution is to define an appropriate fragment
of canonical proofs.

Using an appropriate equivalence class of proofs can be an effective
way of defining away the problem of inessential nondeterminism.  In
linear logic as presented above, if the permutability of rules like
${!}_L$ and ${\otimes}_L$ is problematic, we can instead reason about
{\it equivalence classes} of derivations where proofs that differ only
in the ordering of ${!}_L$ and ${\otimes}_L$ rules are treated as
equivalent (that is, as members of the same equivalence class):
\[
\infer[{!}_L]
{\seq{\Gamma}{\Delta, {!}A, B \otimes C}{D}}
{\infer[{\otimes}_L]
 {\seq{\Gamma,A}{\Delta, B \otimes C}{D}}
 {\deduce{\seq{\Gamma,A}{\Delta, B, C}{D}}{\mathcal D}}}
\quad
\deduce{\mathstrut}{\mathstrut{\equiv}}
\quad
\infer[{\otimes}_L]
{\seq{\Gamma}{\Delta, {!}A, B \otimes C}{D}}
{\infer[{!}_L]
 {\seq{\Gamma}{\Delta, {!}A, B, C}{D}}
 {\deduce{\seq{\Gamma,A}{\Delta, B, C}{D}}{\mathcal D}}}
\]

In LF, lambda calculus terms (which correspond to derivations by the
Curry-Howard) are considered modulo the least equivalence class that
includes
\begin{itemize}
\item $\alpha$-equivalence ($\lambda x.N \equiv \lambda y.N[y/x]$ if 
$y \not\in {\it FV}(N)$), 
\item $\beta$-equivalence 
($(\lambda x.\,M)N \equiv M[N/x]$ if $x \not\in {\it FV}(N)$), and 
\item $\eta$-equivalence ($N \equiv \lambda x.N\,x$).
\end{itemize}
The weak normalization property for LF establishes that given any
typed LF term, we can find an equivalent term that is $\beta$-normal
(no $\beta$-redexes of the form $(\lambda x.M) N$ exist) and
$\eta$-long (replacing $N$ with $\lambda x.N\,x$ anywhere would
introduce a $\beta$-redex or make the term ill-typed).  Furthermore,
in any given equivalence class of typed LF terms, all the
$\beta$-normal and $\eta$-long terms are $\alpha$-equivalent.
Therefore, because $\alpha$-equivalence is decidable, the equivalence
of typed LF terms is also decidable. 

The uniqueness of $\beta$-normal and $\eta$-long terms within an
equivalence class of lambda calculus terms (modulo
$\alpha$-equivalence, which we will henceforth take for granted) makes
these terms useful as canonical representatives of equivalence
classes. In Harper, Honsell, and Plotkin's original formulation
of LF, a deductive system was said to be {\it adequately encoded} as
an LF type family in the case that there is a compositional bijection
between the formal objects in the deductive system and these
$\beta$-normal, $\eta$-long representatives of equivalence classes
\cite{harper93framework}.

More modern presentations of LF, such as Harper and Licata's
\cite{harper07mechanizing}, follow the approach developed by Watkins
et al.~\cite{watkins02concurrent} and define the logical framework so
that it only contains these $\beta$-normal, $\eta$-long {\it canonical
  forms} of LF. This presentations of LF is called Canonical LF to
distinguish it from the original presentation of LF in which the
canonical forms are just a subset of the possible terms. A central
component in this approach is {\it hereditary substitution}.
Hereditary substitution also establishes a normalization property for
LF; using hereditary substitution we can easily take a regular LF
term and transform it into a Canonical LF term.\footnote{This process
  is the same as the way we use cut admissibility to prove cut
  elimination.} An oft-overlooked point, which we will return to in
Section~\ref{sec:warning}, is that the normalization theorem we prove
this way is a strictly weaker theorem than so-called weak
normalization.

One analogue to the canonical forms of LF will be the {\it focused
  derivations} of linear logic that are presented in the next
section. In Section~\ref{sec:foclinlog} below, we will present 
focused linear logic and see that there is exactly 
one focused derivation that derives the proposition
\[{!}({\sf 6bucks} \lolli {\sf battery}) \otimes {\sf 6bucks} \otimes
({\sf battery} \lolli {\sf robot}) \lolli {\sf robot}.\] 
%
We will furthermore see that the structure of this derivation matches
the intuitive transition interpretation of the proposition, a point
that is reinforced by the discussion of {\it synthetic inference
  rules} in Section~\ref{sec:linsynthetic}. In
Section~\ref{sec:linhack}, we argue that our focused system, while it
may be the most natural one for linear logic, does not precisely meet
the demands we will place upon it. This, in turn, motivates a
discussion of notation (Section~\ref{sec:linnote}) 
which we will continue in the next chapter.

\section{Focused linear logic}
\label{sec:foclinlog}

Andreoli's original motivation for introducing focusing was not to
describe a logical framework, it was to describe a paradigm of logic
programming based on proof search in classical linear logic
\cite{andreoli92logic}. The existence of multiple proofs that differ
in inessential ways is particularly problematic for proof search, as
inessential differences between derivations correspond to unnecessary
choice points that a proof search procedure will need to backtrack
over. The presentation of focusing for intuitionistic linear logic in
this section most closely resembles Chaudhuri's focused intuitionistic
linear logic \cite{chaudhuri06focused} and my presentation of
polarized intuitionistic persistent logic
\cite{simmons11structural}. The major exception is the treatment of
asynchronous rules as confluent rather than fixed and arbitrary
(discussed in Section~\ref{sec:confluent-v-fixed}).

\subsection{Polarization}
\label{sec:linpolar}

The first step in describing a focused sequent calculus is to classify
connectives into two groups.  Some connectives, such as linear
implication $A \lolli B$, are called {\it asynchronous} because their
right rules can always be applied eagerly, without backtracking,
during bottom-up proof search. Other connectives, such as disjunction
$A \tensor B$, are called {\it synchronous} because their right rules
cannot be applied eagerly. For instance, if we are trying to prove
$\seq{\Gamma}{A \tensor B}{B \tensor A}$, the ${\tensor}R$ rule cannot
be applied eagerly; we first have to decompose $A \tensor B$ on the
left using the ${\tensor}L$ rule.\footnote{Andreoli dealt with a
  one-sided classical sequent calculus; in intuitionistic logics, it
  is common to call asynchronous connectives {\it right}-asynchronous
  and {\it left}-synchronous. Similarly, it is common to call
  synchronous connectives {\it right}-synchronous and {\it
    left}-asynchronous.

  Synchronicity, a property of connectives, is closely connected to
  (and sometimes conflated with) a property of rules called {\it
    invertibility}; a rule is invertible if the conclusion of the rule
  implies the premises. So ${\lolli}R$ is invertible
  ($\seq{\Gamma}{\Delta}{A \lolli B}$ implies $\seq{\Gamma}{\Delta,
    A}{B}$) but ${\lolli}L$ is not ($\seq{\Gamma}{\Delta, A \lolli
    B}{C}$ does not imply that $\Delta = \Delta_1, \Delta_2$ such that
  $\seq{\Gamma}{\Delta_1}{A}$ and $\seq{\Gamma}{\Delta_2, B}{C}$).
  Rules that can be applied eagerly need to be invertible, so
  asynchronous connectives have invertible right rules and synchronous
  connectives have invertible left rules. Therefore, another synonym
  for asynchronous/negative is {\it right-invertible}, and another
  synonym for synchronous/positive is {\it left-invertible}.}  
The nontrivial result of focusing is that it is possible to separate a
proof into phases: inversion phases in which all asynchronous rules
are applied exhaustively, and focused phases where synchronous rules
are applied repeatedly and exhaustively to a single proposition (the
proposition {\it in focus}). 

We call the asynchronous connectives {\it negative} ($\lolli$, $\top$
and $\with$ in full propositional linear logic) and call the
synchronous connectives {\it positive} ($\zero$, $\oplus$, $\one$, and
$\otimes$ in full propositional linear logic). Each atomic proposition
must be assigned to be either positive or negative, though this
assignment can be arbitrary. At this point, there is an important
choice to make. One way forward is to treat positive and negative
propositions as a syntactic refinements of all propositions, in which
case we end up focusing a standard intuitionistic linear logic. The
other way forward is to treat positive and negative propositions as
distinct syntactic classes $A^+$ and $A^-$ with explicit inclusions
between them. In this second case, we end up focusing a {\it
  polarized} linear logic.  These inclusions are traditionally called
{\it shifts}. The positive proposition ${\downarrow}A^-$, pronounced
``downshift $A$,'' has a subterm that is a negative proposition; the
negative proposition ${\uparrow}A^+$, pronounced ``upshift $A$,'' has
a subterm that is a positive proposition.

The choice doesn't make a large difference for our purposes.
Polarized logics are interesting, and polarized linear logic is a bit
more expressive than regular linear logic, as heavily-shifted
propositions like ${\uparrow}{\downarrow}{\uparrow}{\downarrow}A^-$
can be expressed. This extra expressiveness won't help us in
the design of logical frameworks, but the use of shifts is helpful
when explaining identity expansion in Section~\ref{sec:linindentity}, 
so we will focus a polarized linear logic with shifts.

\input{figs/fig-lin-shift}

The relationship between unpolarized and polarized linear logic is
given by two erasure functions $(A^+)^\circ$ and $(A^-)^\circ$ that
wipe away all the shifts; this function is defined in
Figure~\ref{fig:lin-shift}. While shifts turn out to have a profound
impact on the structure of focused proofs, they are intended to have
no impact on provability. Therefore, the strongest statement of the
correctness of focusing is based on erasure: there is an unfocused
derivation of $(A^+)^\circ$ if and only if there is a focused
derivation of $A^+$.\footnote{I chose $A^+$ mostly to be brief; the
  condition that $(A^-)^\circ$ is derivable iff $A^-$ is could, of
  course, be added.}  However, most proofs of the correctness of
focusing prove a weaker property. Every proposition in linear logic
has an obvious polarized analogue with a minimal number of shifts;
this analogue is formalized as the two functions $A^\oplus$ and
$A^\ominus$ in Figure~\ref{fig:lin-shift}. Note that both of these
functions are partial inverses of erasure: $(A^\oplus)^\circ =
(A^\ominus)^\circ = A$. Almost all proofs of the correctness of
focusing work on the basis of these partial inverses, which we call
{\it polarization strategies}, establishing that there is an unfocused
proof of $A$ if and only if there is a focused proof of
$A^\oplus$.\footnote{The two exceptions are Zeilberger completeness
  proof in classical persistent logic \cite{zeilberger08unity} and my
  proof in intuitionistic persistent logic \cite{simmons11structural}.}
The weaker formulation is sufficient for our current purposes, so we
will discuss the weaker property of polarization-strategy-based
correctness, not erasure-based correctness.

\subsection{Sequent calculus}

Usually, focused logics are described as
having multiple judgments:
\begin{itemize}
\item $\mildrfoc{\Gamma}{\Delta}{A^+}$ (the {\it right focus} sequent, where
the proposition $A^+$ is in focus),
\item $\mildinv{\Gamma}{\Delta}{C}$ (the {\it inversion} sequent), and
\item $\mildlfoc{\Gamma}{\Delta}{A^-}{C}$ (the {\it left focus} sequent,
where the proposition $A^-$ is in focus).
\end{itemize}
Another reasonable presentation of linear logic uses only one sequent
$\mildseq{\Gamma}{\Delta}{U}$, but generalizes what is to allowed to
to appear in the linear context $\Delta$ or in the succeedant, which
we write $U$. We will use this interpretation to understand the logic
described in Figure~\ref{fig:kaustuv-focused}.

\input{figs/fig-kaustuv-focused.tex}

By adding a side condition to the three rules ${\it focus}_R$, ${\it
  focus}_L$, and ${\it copy}$ that neither the context $\Delta$ nor
the succeedant $U$ can contain an in-focus proposition $[A^+]$ or
$[A^-]$, derivations can maintain the invariant that there is always
at most one proposition in focus, effectively restoring the situation
in which there are three distinct judgments.  This restriction alone
gives us what Pfenning calls a {\it chaining} logic
\cite{pfenning12chaining} and which Laurent calls a {\it weakly
  focused} logic \cite{laurent04proof}.\footnote{This is not what I
  called a weakly focused logic \cite{simmons09weak}. That weakly
  focused system had an additional restriction that invertible rules
  could not be applied when any other proposition was in focus; this
  corresponded to what Laurent called a strongly $+$-focused logic.}
We obtain a fully focused logic by further restricting these three
rules so that they only apply when the sequent below the line is {\it
  stable}.  A sequent $\mildseq{\Gamma}{\Delta}{U}$ is stable if the
context $\Delta$ contains only negative propositions $A^-$ and
suspended positive propositions $\langle A^+ \rangle$ and the
succeedant $U$ is either a positive proposition $A^+$ or a suspended
negative proposition $\langle A^- \rangle$. 

We will now turn our attention to the meaning of these suspended
propositions and the four rules that interact with them: ${\it id}^+$,
${\it id}^-$, $\eta^+$, and $\eta^-$.

\subsection{Suspended propositions}

In unfocused sequent calculi, such as the one for linear logic in
Figure~\ref{fig:linear}, initial sequents are restricted to atomic
propositions. All sequent calculi, focused or unfocused, have the
subformula property: every rule breaks down a proposition, either on
the left or the right. Since the logical interpretation of atomic
propositions is that they are stand-ins for unknown propositions, we
are unable to break them down any further. We are therefore only able
to derive an atomic conclusion or use an atomic premise with the {\it
  init} rule that concludes $\seq{\Gamma}{p}{p}$ and has no premises.
This {\it init} rule is the only instance of the admissible identity
theorem $\seq{\Gamma}{A}{A}$ that must be explicitly included as a
proof rule. If we substitute in a proposition for an atomic
propositions, the structure of the proof stays exactly the same,
except that instances of initial sequents become admissible instances
of the general identity theorem.

To my knowledge, all published proof systems for focused logic have
attempted to replicate this initial rule {\it init}. This is a design
error, and it is one that has historically made it enormously (and
unnecessarily) difficult to prove the identity theorem for focused
systems. Our presentation uses {\it suspensions}: suspended positive
propositions $\langle A^+ \rangle$ only appear in the linear context
$\Delta$, and suspended negative propositions $\langle A^- \rangle$
only appear as succeedants. They treated as stable (we never break
down a suspended proposition) and are only used to immediately
prove a proposition in focus with one of the identity rules
${\it id}^+$ or ${\it id}^-$.

Suspended positive propositions act much like regular variables in a
natural deduction system. The positive identity rule ${\it id}^+$
allows us to prove any positive proposition given that positive
proposition appears suspended in the context.  There is a
corresponding substitution principle for focal substitutions that has
a natural-deduction-like flavor: we can substitute a derivation
right-focused on $A^+$ for a suspended positive proposition $\langle
A^+ \rangle$ in a context.

\bigskip
\begin{theorem}[Focal substitution (positive)]\label{thm:fsubst-pos}~\\
For stable $\Delta$,
if $\mildseq{\Gamma}{\Delta}{[A^+]}$ 
and $\mildseq{\Gamma}{\Delta', \langle A^+ \rangle}{U}$, 
then $\mildseq{\Gamma}{\Delta', \Delta}{U}$.
\end{theorem}

\begin{proof}
  Straightforward induction over the second given derivation, as in a
  proof of regular substitution in a natural deduction system. If the
  second derivation is the axiom ${\it id}^+$, the result follows
  immediately using the first given derivation.
\end{proof}

\noindent
Note that, in the statement of Theorem~\ref{thm:fsubst-pos}, the
second premise $\mildseq{\Gamma}{\Delta', \langle A^+ \rangle}{U}$ may
be a right-focused sequent $\mildseq{\Gamma}{\Delta', \langle A^+
  \rangle}{[B^+]}$, a left-focused sequent $\mildseq{\Gamma}{\Delta'',
  [B^-], \langle A^+ \rangle}{U}$, or an inverting sequent. 

Suspended negative propositions are a bit weirder. While a derivation
of $\mildseq{\Gamma}{\Delta', \langle A^+ \rangle}{U}$ is missing a
premise that can be satisfied by a derivation of
$\mildseq{\Gamma}{\Delta}{[A^+]}$, a derivation of 
$\mildseq{\Gamma}{\Delta}{\langle A^- \rangle}$ is missing a 
{\it continuation} that can be satisfied by a derivation of
$\mildseq{\Gamma}{\Delta', [A^-]}{U}$. The focal substitution principle,
however, still takes the basic form of a substitution principle.

\bigskip
\begin{theorem}[Focal substitution (negative)]\label{thm:fsubst-neg}~\\
For stable $\Delta'$ and $U$, 
if $\mildseq{\Gamma}{\Delta}{\langle A^- \rangle}$
and $\mildseq{\Gamma}{\Delta', [A^-]}{U}$, 
then $\mildseq{\Gamma}{\Delta', \Delta}{U}$. 
\end{theorem}

\begin{proof}
  Straightforward induction over the {\it first} given derivation; if
  the first derivation is the axiom ${\it id}^-$, the result follows
  immediately using the second given derivation.
\end{proof}

\noindent
As a regular substitution principle that is inductive over the structure
of the first given proposition, focal substitution is reminiscent of 
the {\it leftist substitutions} introduced by Pfenning and Davies in the 
context of the possibility modality \cite{pfenning01judgmental}.

Unlike cut admissibility, which we discuss in Section~\ref{sec:lincut}, both
of the focal substitution principles are straightforward inductions
over the structure of the derivation containing the suspended
proposition. In the development of structural focalization, I discuss
how, in a focused presentation of persistent intuitionistic logic that
is encoded in LF, a suspended positive premise can be encoded as a
hypothetical right focus. This encoding makes the ${\it id}^+$ rule an
instance of the hypothesis rule provided by LF and establishes
Theorem~\ref{thm:fsubst-pos} ``for free'' as an instance of LF
substitution. This is possible to do for negative focal substitution
as well, but it is somewhat counter-intuitive
\cite{simmons11structural}.

The two substitution
principles can be phrased as admissible rules for building derivations,
which we indicate using a dashed line:
\[
\infer-[{\it subst}^+]
{\mildseq{\Gamma}{\Delta', \Delta}{U}}
{\mildseq{\Gamma}{\Delta}{[A^+]}
 &
 \mildseq{\Gamma}{\Delta', \langle A^+ \rangle}{U}}
\qquad
\infer-[{\it subst}^-]
{\mildseq{\Gamma}{\Delta', \Delta}{U}}
{\mildseq{\Gamma}{\Delta}{\langle A^- \rangle}
 &
 \mildseq{\Gamma}{\Delta', [A^-]}{U}}
\]

\subsection{Identity expansions}
\label{sec:linindentity}

Suspended propositions appear in Figure~\ref{fig:kaustuv-focused} in
two places: first in the identity rules that we have just discussed
and connected with the focal substitution principles, and second in
the rules marked $\eta^+$ and $\eta^-$, which are also the only
mention of atomic propositions in the presentation. It is here that we
need to make an absolutely critical shift of perspective from
unfocused to focused logic. In an unfocused logic, the rules
nondeterministically break down propositions, and the initial rule {\it
  init} puts an end to this process when an atomic proposition is
reached. In a focused logic, the focus and inversion phases {\it must}
break down a proposition all the way until a shift is reached. The two
$\eta$ rules are what put an end to this when an atomic proposition is
reached, and they correspond to the two ${\it id}$ rules that allow
these necessarily suspended propositions to successfully conclude a
right or left focus.

\input{figs/fig-lineta-1}
\input{figs/fig-lineta-2}

Just as the {\it init} rule is a particular instance of the admissible
identity sequent $\seq{\Gamma}{A}{A}$ in unfocused linear logic, the
atomic suspension rules $\eta^+$ and $\eta^-$ are instances of an admissible
identity expansion rule in focused linear logic:
\[
\infer-[\eta^+]
{\mildseq{\Gamma}{\Delta, A^+}{U}}
{\mildseq{\Gamma}{\Delta, \langle A^+ \rangle}{U}}
\qquad
\infer-[\eta^-]
{\mildseq{\Gamma}{\Delta}{A^-}}
{\mildseq{\Gamma}{\Delta}{\langle A^- \rangle}}
\]
This admissible rule must be established by mutual recursion; the
proof is structurally inductive on the structure of the proposition,
and uses focal substitution in a critical way. Most of the cases of
this proof are represented in Figure~\ref{fig:lineta-1}. (Note that in
this figure we omit polarity annotations from propositions as they are
always clear from the context.) The remaining case (for the
multiplicative unit $\one$) is presented in Figure~\ref{fig:lineta-2}
along with the cases for the additive connectives $\zero$, $\oplus$,
$\top$, and $\with$, which are neglected elsewhere in this chapter.

The admissible identity expansion rules fit with an interpretation of
positive atomic propositions as stand-ins for arbitrary positive
propositions and of negative atomic propositions as stand-ins for
negative atomic propositions: if we substitute a proposition in for
some atomic proposition, all the instances of atomic suspension
corresponding to that rule become admissible instances of identity
expansion. Furthermore, the usual identity principles are simple
corollaries of identity expansion:
\[
\infer-[\eta^+]
{\mildseq{\Gamma}{A^+}{A^+}}
{\infer[{\it focus}_R]
 {\mildseq{\Gamma}{\langle A^+ \rangle}{A^+}}
 {\infer[{\it id}^+]
  {\mildseq{\Gamma}{\langle A^+ \rangle}{[A^+]}}
  {}}}
\qquad
\infer-[\eta^-]
{\mildseq{\Gamma}{A^-}{A^-}}
{\infer[{\it focus}_L]
 {\mildseq{\Gamma}{A^-}{\langle A^- \rangle}}
 {\infer[{\it id}^-]
  {\mildseq{\Gamma}{[A^-]}{\langle A^- \rangle}}
  {}}}
\]

\subsection{Cut admissibility}
\label{sec:lincut}

Theorem~\ref{thm:lincut} mostly follows the well-worn contours of a
structural cut admissibility argument \cite{pfenning00structural}, so
we defer a full discussion of cut admissibility until the next
chapter, where the use of structural focalization will allow us to
give a tidier proof of the theorem.\footnote{The main ``untidy''
  aspect of Theorem~\ref{thm:lincut} is that the lack of a forced
  inversion order means that the right commutative cases dealing with
  invertible rules must be repeated in parts 1 and 3, and likewise for
  the left commutative cases dealing with invertible rules and parts 2
  and 4. The repetition of {\it all} cases between parts 3 and 5 will
  also be addressed in the next chapter.}
%
The only important caveat to emphasize about Theorem~\ref{thm:lincut}
is that cut admissibility is only applicable in the absence of any
non-atomic suspended propositions. If we did not make this
restriction, then in Theorem~\ref{thm:lincut}, part 1, we might encounter
a derivation of $\mildseq{\Gamma}{\langle A \tensor B \rangle}{[ A \tensor B ]}$
being cut into the derivation
\[
\infer[{\otimes}_R]
{\mildseq{\Gamma}{\Delta',A \tensor B}{U}}
{\deduce{\mildseq{\Gamma}{\Delta', A, B}{U}}{\mathcal E}}
\]
in which case there is no clear way to proceed and prove 
$\mildseq{\Gamma}{\Delta', \langle A \tensor B \rangle}{U}$. 

\bigskip
\begin{theorem}[Cut admissibility]\label{thm:lincut}
For all $\Gamma$, $A^+$, $A^-$, $\Delta$, $\Delta'$, and $U$ that
do not contain any non-atomic suspended propositions:
\begin{enumerate}
\item If $\mildseq{\Gamma}{\Delta}{[A^+]}$
      and $\mildseq{\Gamma}{\Delta',A^+}{U}$
      (where $\Delta$ is stable), 
      then $\mildseq{\Gamma}{\Delta',\Delta}{U}$.
\item If $\mildseq{\Gamma}{\Delta}{A^-}$
      and $\mildseq{\Gamma}{\Delta', [A^-]}{U}$
      (where $\Delta'$ and $U$ are stable),
      then $\mildseq{\Gamma}{\Delta',\Delta}{U}$. 
\item If $\mildseq{\Gamma}{\Delta}{A^-}$
      and $\mildseq{\Gamma}{\Delta', A^-}{U}$
      (where $\Delta$ is stable), 
      then $\mildseq{\Gamma}{\Delta',\Delta}{U}$. 
\item If $\mildseq{\Gamma}{\Delta}{A^+}$
      and $\mildseq{\Gamma}{\Delta', A^+}{U}$
      (where $\Delta'$ and $U$ are stable),
      then $\mildseq{\Gamma}{\Delta',\Delta}{U}$. 
\item If $\mildseq{\Gamma}{\cdot}{A^-}$
      and $\mildseq{\Gamma, A^-}{\Delta}{U}$,
      then $\mildseq{\Gamma}{\Delta}{U}$. 
\end{enumerate}
\end{theorem}

\begin{proof}
By lexicographic induction. In each invocation of the induction
hypothesis, either the principal cut formula $A^+$ or $A^-$ gets 
smaller, or else it stays the same and the number of the part
gets smaller (as when we invoke part 2 when proving part 5). 

Within parts 3 and 5, the first two metrics stay the same while the
second given derivation gets smaller, and within part 4, the first two
metrics stay the same while the first given derivation gets smaller.
\end{proof}

\subsection{Correctness of focusing}

\input{figs/fig-lin-shift-ctx}

Now we will make precise the correctness for a focused, polarized
logic that was discussed in Section~\ref{sec:linpolar}: that there is
a unfocused proof of $A$ if and only if there is a focused proof of
$A^\oplus$. The proof require lifting of our erasure function and our
natural polarization strategy to contexts and succeedants, which is
done in Figure~\ref{fig:lin-shift-ctx}. Soundness is established on
the basis of erasure as in \cite{simmons11structural}, but as
discussed Section~\ref{sec:linpolar}, we will give the slightly
simpler polarization-strategy-based proof of completeness.

\bigskip
\begin{theorem}[Soundness of focusing]
If $\mildseq{\Gamma}{\Delta}{U}$, where $\Delta$ and $U$ contain only
atomic suspensions, then $\seq{\Gamma^\circ}{\Delta^\circ}{U^\circ}$.
\end{theorem}

\begin{proof}
  By straightforward induction on the given derivation; in each case,
  the result either follows directly by invoking the induction
  hypothesis or by invoking the induction hypothesis and applying one
  rule from Figure~\ref{fig:linear}.
\end{proof}

\begin{theorem}[Completeness of focusing]\label{thm:linfoccomplete}
If $\seq{\Gamma}{\Delta}{C}$,
then $\mildseq{\Gamma^\ominus}{\Delta^\ominus}{C^\oplus}$. 
\end{theorem}

\begin{proof}
  By induction on the first given derivation. Each rule in 
  Figure~\ref{fig:linear} corresponds to one {\it unfocused admissibility 
  lemma} that we must prove in the focused system; we give three examples
  of these unfocused admissibility lemmas here.

  Rule {\it copy}: Given
  $\mildseq{\Gamma^\ominus, A^\ominus}{\Delta^\ominus, A^\ominus}{C^\oplus}$, we
  have to prove 
  $\mildseq{\Gamma^\ominus, A^\ominus}{\Delta^\ominus}{C^\oplus}$:
  {\small \[
  \infer-[{\it cut}(3)]
  {\mildseq{\Gamma^\ominus, A^\ominus}{\Delta^\ominus}{C^\oplus
}}
  {\infer-[\eta^-]
   {\mildseq{\Gamma^\ominus, A^\ominus}{\cdot}{A^\ominus}}
   {\infer[{\it id}^-]
    {\mildseq{\Gamma^\ominus, A^\ominus}{\cdot}{\langle A^\ominus \rangle}}
    {}}
   &
   \mildseq{\Gamma^\bullet, A^\ominus}{\Delta^\bullet, A^\ominus}{C^\oplus}}
  \]}

  Rule $\lolli_L$: Given
  $\mildseq{\Gamma^\ominus}{\Delta^\ominus}{A^\oplus}$ and
  $\mildseq{\Gamma^\ominus}{\Delta_A^\ominus, B^\ominus}{C^\oplus}$, we must prove
  $\mildseq{\Gamma^\ominus}{\Delta^\ominus, \Delta_A^\ominus, (A \lolli B)^\ominus}
    {C^\oplus}$ which is 
  the same as
  $\mildseq{\Gamma^\ominus}
    {\Delta^\ominus, \Delta_A^\ominus, A^\oplus \lolli B^\ominus}
    {C^\oplus}$.
  {\small \[
  \infer-[{\it cut}(4)] 
  {\mildseq{\Gamma^\ominus}
    {\Delta^\ominus, \Delta_A^\ominus, A^\oplus \lolli B^\ominus}
    {C^\oplus}}
  {\infer-[{\it cut}(4)]
   {\mildseq{\Gamma^\ominus}{\Delta^\ominus, A^\oplus \lolli B^\ominus}
     {{\downarrow}B^\ominus}}
   {{\mildseq{\Gamma^\ominus}{\Delta^\ominus}{A^\oplus}}
    &
    \infer-[{\eta}^+]
    {\mildseq{\Gamma^\ominus}{A^\oplus \lolli B^\ominus, A^\oplus}
     {{\downarrow}B^\ominus}}
    {\infer[{\it focus}_R]
     {\mildseq{\Gamma^\ominus}{A^\oplus \lolli B^\ominus, \langle A^\oplus \rangle}
       {{\downarrow}B^\ominus}}
     {\infer[{\downarrow}_R]
      {\mildseq{\Gamma^\ominus}
        {A^\oplus \lolli B^\ominus, \langle A^\oplus \rangle}
        {[{\downarrow}B^\ominus}]}
      {\infer-[\eta^-]
       {\mildseq{\Gamma^\ominus}
         {A^\oplus \lolli B^\ominus, \langle A^\oplus \rangle}
         {B^\ominus}}
       {\infer[{\it focus}_L]
        {\mildseq{\Gamma^\ominus}
         {A^\oplus \lolli B^\ominus, \langle A^\oplus \rangle}
         {\langle B^\ominus \rangle}}
        {\infer[{\lolli}_L]
         {\mildseq{\Gamma^\ominus}
         {[A^\oplus \lolli B^\ominus], \langle A^\oplus \rangle}
         {\langle B^\ominus \rangle}}
         {\infer[{\it id}^+]
          {\mildseq{\Gamma^\ominus}{\langle A^\oplus \rangle}{[A^\oplus]}}
          {}
          &
          \infer[{\it id}^-]
          {\mildseq{\Gamma^\ominus}{[B^\ominus]}{\langle B^\ominus \rangle}}
          {}}}}}}}}
   &
   {\infer[{\downarrow}_L]
    {\mildseq{\Gamma^\ominus}{\Delta_A^\ominus, {\downarrow}B^\ominus}
      {C^\oplus}}
    {\mildseq{\Gamma^\ominus}{\Delta_A^\ominus, B^\ominus}
      {C^\oplus}}}}
  \]}

  Rule $\lolli_R$: Given 
  $\mildseq{\Gamma^\ominus}{\Delta^\ominus, A^\ominus}{B^\oplus}$, we must
  prove 
  $\mildseq{\Gamma^\ominus}{\Delta^\ominus}{(A \lolli B)^\oplus}$ which is the
  same as 
   $\mildseq{\Gamma^\ominus}{\Delta^\ominus}
    {{\downarrow}(A^\oplus \lolli B^\ominus)}$. The core of the 
  proof is this derivation:
  {\small \[
  \infer-[{\it cut}(3)]
  {\mildseq{\Gamma^\ominus}{\Delta^\ominus}
    {{\downarrow}(A^\oplus \lolli B^\ominus)}}
  {\infer[{\lolli}_R]
   {\mildseq{\Gamma^\ominus}{\Delta^\ominus}
     {{\downarrow}{\uparrow}A^\oplus \lolli {\uparrow}{\downarrow}B^\ominus}}  
   {\infer[{\downarrow}_L]
    {\mildseq{\Gamma^\ominus}{\Delta^\ominus, {\downarrow}{\uparrow}A^\oplus}
      {{\uparrow}{\downarrow}B^\ominus}}
    {\infer[{\uparrow}_R]
     {\mildseq{\Gamma^\ominus}{\Delta^\ominus, {\uparrow}A^\oplus}
      {{\uparrow}{\downarrow}B^\ominus}}
     {\mildseq{\Gamma^\ominus}{\Delta^\ominus, {\uparrow}A^\oplus}
      {{\downarrow}B^\ominus}}}}
   &
   \infer[{\it focus}_R]
   {\mildseq{\Gamma}{{\downarrow}{\uparrow}A^\oplus \multimap B^\ominus}
      {{\downarrow}(A^\oplus \lolli B^\ominus)}}
   {\infer[{\downarrow}_R]
    {\mildseq{\Gamma^\ominus}
      {{\downarrow}{\uparrow}A^\oplus \multimap {\uparrow}{\downarrow}B^\ominus}
      {[ {\downarrow}(A^\oplus \lolli B^\ominus)} ]}
    {\infer[{\lolli}_R]
     {\mildseq{\Gamma^\ominus}
       {{\downarrow}{\uparrow}A^\oplus \multimap {\uparrow}{\downarrow}B^\ominus}
       {A^\oplus \lolli B^\ominus}}
     {\infer-[\eta^+]
      {\mildseq{\Gamma^\ominus}
        {{\downarrow}{\uparrow}A^\oplus 
           \multimap {\uparrow}{\downarrow}B^\ominus, 
         A^\oplus}
        {B^\ominus}}
      {\infer-[\eta^-]
       {\mildseq{\Gamma^\ominus}
         {{\downarrow}{\uparrow}A^\oplus
            \multimap {\uparrow}{\downarrow}B^\ominus, 
          \langle A^\oplus \rangle}
         {B^\ominus}}
       {\infer[{\it focus}_L]
        {\mildseq{\Gamma^\ominus}
          {{\downarrow}{\uparrow}A^\oplus 
             \multimap {\uparrow}{\downarrow}B^\ominus, 
           \langle A^\oplus \rangle}
          {\langle B^\ominus \rangle}}
        {\infer[{\lolli}_L]
         {\mildseq{\Gamma^\ominus}
           {[ {\downarrow}{\uparrow}A^\oplus 
              \multimap {\uparrow}{\downarrow}B^\ominus ], 
            \langle A^\oplus \rangle}
           {\langle B^\ominus \rangle}}
         {\infer[{\downarrow}_R]
          {\mildseq{\Gamma^\ominus}{\langle A^\oplus \rangle}
           {[{\downarrow}{\uparrow}A^\oplus]}}
          {\infer[{\uparrow}_R]
           {\mildseq{\Gamma^\ominus}{\langle A^\oplus \rangle}
              {{\uparrow}A^\oplus}}
           {\infer[{\it focus}_R]
            {\mildseq{\Gamma^\ominus}{\langle A^\oplus \rangle}{A^\oplus}} 
            {\infer[{\it id}^+]
             {\mildseq{\Gamma^\ominus}{\langle A^\oplus \rangle}{[A^\oplus]}}
             {}}}}
          &
          \infer[{\uparrow}_L]
          {\mildseq{\Gamma^\ominus}{[{\uparrow}{\downarrow}B^\ominus]}
             {\langle B^\ominus \rangle}}
          {\infer[{\downarrow}_L]
           {\mildseq{\Gamma^\ominus}{{\downarrow}B^\ominus}
             {\langle B^\ominus \rangle}}
           {\infer[{\it focus}_L]
            {\mildseq{\Gamma^\ominus}{B^\ominus}{\langle B^\ominus \rangle}}
            {\infer[{\it id}^-]
             {\mildseq{\Gamma^\ominus}{[B^\ominus]}{\langle B^\ominus \rangle}}
             {}}}}}}}}}}}}
  \]}

  \noindent
  To conclude, we show that $\mildseq{\Gamma^\ominus}{\Delta^\ominus,
    {\uparrow}A^\oplus} {{\downarrow}B^\ominus}$ is derivable from
  $\mildseq{\Gamma^\ominus}{\Delta^\ominus, A^\ominus}{B^\oplus}$ in
  two steps.  First, by case analysis on $A$, we show that
  $\mildseq{\Gamma^\ominus}{\Delta^\ominus, {\uparrow}A^\oplus}
  {{\downarrow}B^\ominus}$ is derivable from
  $\mildseq{\Gamma^\ominus}{\Delta^\ominus,
    A^\ominus}{{\downarrow}B^\ominus}$.  Either $A$ is a
  right-synchronous connective, in which case $A^\ominus =
  {\uparrow}A^\oplus$ and the result is immediate, or else it is a
  right-asynchronous connective, in which case $A^\oplus =
  {\downarrow}A^\ominus$ and we can proceed as follows:
  \[
  \infer[{\it focus}_L]
  {\mildseq{\Gamma^\ominus}{\Delta^\ominus, {\uparrow}{\downarrow}A^\ominus}
     {{\downarrow}B^\ominus}} 
  {\infer[{\uparrow}_L]
   {\mildseq{\Gamma^\ominus}{\Delta^\ominus, [{\uparrow}{\downarrow}A^\ominus]}
     {{\downarrow}B^\ominus}}
   {\infer[{\downarrow_L}]
    {\mildseq{\Gamma^\ominus}{\Delta^\ominus, {\downarrow}A^\ominus}
     {{\downarrow}B^\ominus}}
    {\mildseq{\Gamma^\ominus}{\Delta^\ominus, A^\ominus}
     {{\downarrow}B^\ominus}}}}
  \]
  Second, by case analysis on $B$, we show that
  $\mildseq{\Gamma^\ominus}{\Delta^\ominus,
    A^\ominus}{{\downarrow}B^\ominus}$ is derivable from
  $\mildseq{\Gamma^\ominus}{\Delta^\ominus, A^\ominus}{B^\oplus}$.
  Either $B$ is a right-asynchronous connective, in which case
  $B^\oplus = {\downarrow}B^\ominus$ and the result is immediate, or
  else $B$ is a right-synchronous connective, in which case 
  $B^\ominus = {\uparrow}B^\oplus$ and we can proceed as follows:
  \[
  \infer[{\it focus}_R]
  {\mildseq{\Gamma^\ominus}{\Delta^\ominus, A^\ominus}{{\downarrow}{\uparrow}B^\oplus}} 
  {\infer[{\downarrow}_R]
   {\mildseq{\Gamma^\ominus}{\Delta^\ominus, A^\ominus}
      {[{\downarrow}{\uparrow}B^\oplus]}}
   {\infer[{\uparrow}_R]
    {\mildseq{\Gamma^\ominus}{\Delta^\ominus, A^\ominus}{{\uparrow}B^\oplus}}
    {\mildseq{\Gamma^\ominus}{\Delta^\ominus, A^\ominus}{B^\oplus}}}}
  \]

  \noindent
  This completes the case for rule ${\lolli}_R$.
\end{proof}

\subsection{Confluent versus fixed inversion}
\label{sec:confluent-v-fixed}

A salient feature of this presentation of focusing is that invertible,
non-focused rules need not be applied in any particular order.
Therefore, the last step in a proof of $\mildseq{\Gamma}{\Delta, A
  \tensor B, \one, {!}C}{D \lolli E}$ could be ${\otimes}_L$,
${\one}_L$ ${!}_L$, or ${\lolli}_R$. Allowing for this inessential
nondeterminism simplifies the presentation a bit, but it also gets in
the way of effective proof search if we do not address it in some way.
Addressing this nondeterminism within an inversion phase echos the
discussion of LF from the beginning of the chapter. 

We can say, as suggested in that introduction, declare that all
proofs which differ only by the order of their invertible, non-focused
rules be treated as equivalent. It is possible to establish that all
possible inversion orderings will lead to the same set of stable
sequents, which lets us know that all of these reorderings do not
fundamentally change the structure of the rest of the proof, and it
should furthermore be possible to show that this equivalence relation
on derivations is decidable. This is fundamentally a confluence
property, and so we can call this style of focusing a {\it confluent}
presentation; it is exemplified by Liang and Miller's LJF
\cite{liang09focusing}, and the confluent presentation in this chapter
is closely faithful to Pfenning's course notes on linear logic
\cite{pfenning12chaining}.  There are several ways to state this
confluence property.

%; one of them
%is Proposition~\ref{prop:confluence-lin}:
%
%\bigskip
%\begin{proposition}
%\label{prop:confluence-lin}
%Let $\Xi$ be a set of derivations of stable sequents. If 
%$\mildseq{\Gamma}{\Delta}{U}$, where $\Delta$ and $U$ contain no 
%focused propositions, can be derived from $\Xi$ using only non-focused
%rules, then for all non-focused rules that can be used to derive
%$\mildseq{\Gamma}{\Delta}{U}$, its premises can be derived
%from $\Xi$ using only non-focused rules.
%\end{proposition}
%\bigskip

Given this decidable equivalence on proofs, we can pick some member of
each to serve as a canonical representative; this will suffice
to solve the problems with proof search, as we can search for 
the canonical representatives of focused proofs rather than the 
much larger set of all focused proofs. The most common canonical
representatives force all invertible rules to be applied so that
propositions are decomposed in a depth-first ordering. 

Then, reminiscent of the move from LF to Canonical LF, the logic
itself can be restricted so that only the canonical representatives
are admitted. The most convenient way of forcing a left-most,
depth-first ordering is to isolate the invertible propositions ($A^+$
on the left and $A^-$ on the right) in separate, ordered inversion
contexts, and then to only work on the left-most proposition in the
context. This is the way most focused logics are defined, including
those by Andreoli, Chaudhuri, and myself (both in the Structural
Focalization development and in the next chapter). This style of 
presenting a focusing logic can be called a {\it fixed} presentation,
as the shape of the inversion phase is fixed in a particular, though
fundamentally arbitrary, shape. 

The completeness of focusing for a fixed presentation of focusing is
implied by the completeness of focusing for a confluent presentation
of the same logic along with the appropriate confluence property for
that logic, whereas the reverse is not true. Therefore, the confluent
presentation, in a certain sense, proving a stronger theorem than the
fixed presentation, though the fixed presentation will be sufficient
for our purposes.

\subsection{Running example}

\input{figs/fig-focused-robot}

Figure~\ref{fig:focused-robot} gives the result of taking our 
robot example, Figure~\ref{fig:unfocused-robot}, through the 
polarization and focalization process described by 
Theorem~\ref{thm:linfoccomplete}. There is indeed only one 
proof of this focused proposition up to the reordering of 
invertible rules, and only one proof period if we always
decompose invertible propositions in a left-most, depth-first
ordering (as we do in Figure~\ref{fig:focused-robot}). 

We have therefore successfully used focusing to get a canonical
proof structure that correctly corresponds to our 
informal series of transitions:
\[
\begin{array}{ccccc}
\begin{array}{c}
\mbox{\it \$6 (1)}\medskip\\ 
\mbox{\it battery-free robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}
\end{array}
& \leadsto &
\begin{array}{c}
\mbox{\it battery  (1)}\medskip\\ 
\mbox{\it battery-free robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}
\end{array}
& \leadsto &
\begin{array}{c}
\mbox{\it robot (1)} \medskip\\ 
\mbox{\it turn \$6 into a battery}\\
\mbox{\it (all you want)}\medskip\\~\\
\end{array}
\end{array}
\]
But at what cost? Figure~\ref{fig:focused-robot} definitely contains a
fair amount of bureaucracy compared to the original
Figure~\ref{fig:unfocused-robot}, even if does a better job of
matching, when read from bottom to top, the series of transitions. A
less cluttered way of looking at these proofs is in terms of what
Andreoli called {\it bipoles} \cite{andreoli01focussing} and what we,
following Chaudhuri, call {\it synthetic inference rules}
\cite{chaudhuri08focusing}.

\section{Synthetic inference rules}
\label{sec:linsynthetic}

Synthetic inference rules were introduced by Andreoli as {\it bipoles}
for the purpose of giving a more abstract way of looking at focused
logics and focused derivations \cite{andreoli01focussing}. The first
idea behind synthetic inference rules is that the stable
sequents are the ones that are most clearly endowed with meaning in a
focused sequent calculus; this was reflected by the polarization
strategy used in the completeness theorem for linear logic
(Theorem~\ref{thm:linfoccomplete}), which only produced
stable sequents. The second idea is that, once we know 
whether the last rule in the proof of a stable sequent is
is 
\begin{itemize}
\item ${\it copy}$ on some proposition $A^-$ from $\Gamma$, 
\item ${\it focus}_L$ on some proposition $A^-$ in $\Delta$, or
\item ${\it focus}_R$ on the succeedant $A^+$
\end{itemize} (it must be one of these
three), then the structure of the proof is completely determined
up to the next occurrence of a stable sequent. 

For example, consider the act of focusing on the proposition
$a^+ \lolli {\uparrow}b^+$ in $\Gamma$ using the ${\it copy}$ rule,
where $a^+$ and $b^+$ are positive atomic propositions. 
This must mean that a suspended atomic proposition $a^+$ appears
in the context $\Delta$, or else the proof could not be completed:
\[
\infer[{\it copy}]
{\mildseq{\Gamma, a^+ \lolli {\uparrow}b^+}{\Delta, \langle a^+ \rangle}{U}}
{\infer[{\lolli}_L]
 {\mildseq{\Gamma, a^+ \lolli {\uparrow}b^+}
   {\Delta, \langle a^+ \rangle, [a^+ \lolli {\uparrow}b^+]}{U}}
 {\infer[{\it id}^+]
  {\mildseq{\Gamma, a^+ \lolli {\uparrow}b^+}
   {\langle a^+ \rangle}{[ a^+ ]}}
  {}
  &
  \infer[{\uparrow}_L]
  {\mildseq{\Gamma, a^+ \lolli {\uparrow}b^+}{\Delta, [{\uparrow}b^+]}{U}}
  {\infer[\eta^+]
   {\mildseq{\Gamma, a^+ \lolli {\uparrow}b^+}{\Delta, b^+}{U}}
   {\mildseq{\Gamma, a^+ \lolli {\uparrow}b^+}
    {\Delta, \langle b^+ \rangle}{U}}}}}
\]
The non-stable sequents in the middle are not interesting parts 
of the structure of the proof, as they are fully determined by the
choice of focus, so we can collapse this series of transitions
into a single synthetic rule:
\[
\infer[{\sf CP}_{a^+ \lolli {\uparrow}b^+}]
{\mildseq{\Gamma, a^+ \lolli {\uparrow}b^+}{\Delta, \langle a^+ \rangle}{U}}
{\mildseq{\Gamma, a^+ \lolli {\uparrow}b^+}{\Delta, \langle b^+ \rangle}{U}}
\]
Similar rules can be given for focusing phases associated with the
${\it focus}_L$ and ${\it focus}_R$ rule:
\[
\infer[{\sf LP}_{a^+ \lolli {\uparrow}b^+}]
{\mildseq{\Gamma}{\Delta, \langle a^+ \rangle, a^+ \lolli {\uparrow}b^+}{U}}
{\mildseq{\Gamma}{\Delta, \langle b^+ \rangle}{U}}
\]
\[
\infer[{\sf RF}_{{\downarrow}({!}A^- \otimes b^+ \otimes {\downarrow}C^- \lolli 
   {\uparrow}D^+)}]
{\mildseq{\Gamma}{\Delta}
  {{\downarrow}({!}A^- \otimes b^+ \otimes {\downarrow}C^- \lolli 
   {\uparrow}D^+)}}
{\mildseq{\Gamma, A^-}{\Delta, \langle b^+ \rangle, C^-}{D^+}}
\quad
\infer[{\sf RF}_{a^+}]
{\mildseq{\Gamma}{\langle a^+ \rangle}{a^+}}
{}
\]

\begin{figure}
{\small\[
\infer[{\sf RF}_{{\downarrow}({!}A^- \otimes b^+ \otimes {\downarrow}C^- \lolli 
   {\uparrow}D^+)}]
{\mildseq{\cdot}{\cdot}
   {{\downarrow}({!}({\sf 6bucks} \lolli {\uparrow}{\sf battery}) \otimes
             {\sf 6bucks} \otimes 
             {\downarrow}({\sf battery} \lolli {\uparrow}{\sf robot}) \lolli 
             {\uparrow}{\sf robot})}}
{\infer[{\sf CP}_{a^+ \lolli {\uparrow}b^+}]
 {\mildseq{{\sf 6bucks} \lolli {\uparrow}{\sf battery}\quad}
    {\quad\langle {\sf 6bucks} \rangle, ~~
     {\sf battery} \lolli {\uparrow}{\sf robot}\quad}{\quad{\sf robot}}}
 {\infer[{\sf LF}_{a^+\lolli{\uparrow}b^+}]
  {\mildseq{{\sf 6bucks} \lolli {\uparrow}{\sf battery}\quad}
    {\quad{\sf battery} \lolli {\uparrow}{\sf robot}, ~~
     \langle {\sf battery} \rangle\quad}{\quad{\sf robot}}}
  {\infer[{\sf RF}_{a^+}]
   {\mildseq{{\sf 6bucks} \lolli {\uparrow}{\sf battery}\quad}
       {\quad\langle {\sf robot} \rangle\quad}{\quad{\sf robot}}}
   {}}}}
\]}
\caption{Our running example, presented with synthetic rules.}
\label{fig:synthetic-robot}
\end{figure}

Focused proofs of stable sequents are, by definition, in a 1-to-1
correspondence with proofs using synthetic inference rules. If we look
at our running example as a derivation using synthetic inference rules
(Figure~\ref{fig:synthetic-robot}), we see that the system takes four
steps. The last three steps, furthermore, correspond precisely to the
three steps in our informal description of the robot-battery-store
system.

In important side note: the fact that the structure of propositions is {\it
  entirely} determined is an artifact of our restriction to MALL. If
we also consider additive connectives, then we would be in a similar
situation except that we would be able to identify some number of
synthetic rules for each right focus, left focus, or copy (possibly
zero; there's no way to successfully right focus on a proposition
like $\zero \otimes {\uparrow}{\downarrow}A^+$).


\section{Hacking the focusing system}
\label{sec:linhack}

Despite the novel treatment of suspended propositions in
Section~\ref{sec:foclinlog}, the presentation of linear logic given
there is essentially the same as the presentation in Chaudhuri's
thesis \cite{chaudhuri06focused}, in the sense that the logic gives
rise to the same synthetic inference rules. It is {\it not} a faithful
intuitionistic analogue to Andreoli's original presentation of focusing
\cite{andreoli92logic}, as Pfenning's course notes is
\cite{pfenning12chaining}.\footnote{We will blur the lines, in this
  section, between Andreoli's original presentation of focused
  classical linear logic and Pfenning's adaptation to intuitionistic
  linear logic. In particular We will mostly use the notation of
  Pfenning's presentation, but the observations are equally applicable
  in Andreoli's focused triadic system.}  Nor does it have the same
synthetic inference rules as the focused presentation used in the
language of ordered logical specifications presented by Pfenning and I
\cite{pfenning09substructural}.

The difference, in each case, lies in the treatment of positive atomic
propositions, which can be a surprisingly rich source of fungible
behavior in focused systems. My justification for presenting
Chaudhuri's system as the canonical focusing system for linear logic
in Section~\ref{sec:foclinlog} is because the treatment of suspended
positive propositions fits beautifully with the idea that atomic
propositions are just placeholders for unspecified
propositions. However, our current goal is not to study focused
logics, it is to use the structure of proofs in focused logics as a
logical framework for encoding systems we are interested in. 

In this section, we will discuss three ideas that modify the structure
of focused proofs (though not the underlying fact that the focused
proofs are complete) and that. Two of them, Andreoli's atom
optimization and the bang optimization, seem to complicate the elegant
interpretation of positive atoms as stand-ins for positive atomic
propositions; the implications of this are discussed in
Section~\ref{sec:moreprim}. The third, concurrent equality, is a
notion of proof equivalence that operates on synthetic inference
rules. It represents an intermediate point between focusing and
multifocusing \cite{chaudhuri08canonical}, which defines an even more
coarse-grained notion of equivalence than focusing does.  All three of
these modifications will play a role in the focused ordered lax logic
described in Chapter~3.

\subsection{Atom optimization}

Andreoli's original focused system isn't polarized, so propositions
that are syntactically invalid in a polarized presentation, like
${!}(p^+ \otimes q^+)$ or ${!}p^+$ are valid in his system (we would
have to write ${!}{\uparrow}{(p^+ \otimes q^+)}$ and
${!}{\uparrow}p^+$). It's therefore possible to use the ${\it copy}$
rule to copy a positive proposition out of the context and into focus,
but the focus immediately blurs, as in the this proof
fragment:\footnote{We will use the sequent form
  $\andseq{\Gamma}{\Delta}{C}$ in this section for focused but
  unpolarized systems.}
\[
\infer[{\it copy}]
{\andseq{p^+ \otimes q^+}{\cdot}{q^+ \otimes p^+}}
{\infer[{\it blur}_L]
 {\andseq{p^+ \otimes q^+}{[p^+ \otimes q^+]}{q^+ \otimes p^+}}
 {\infer[{\otimes}_L]
  {\andseq{p^+ \otimes q^+}{p^+ \otimes q^+}{q^+ \otimes p^+}}
  {\deduce
   {\andseq{p^+ \otimes q^+}{p^+, q^+}{q^+ \otimes p^+}}
   {\vdots}}}}
\]
Note that, in the polarized setting we have used up to this point, the
effect of the ${\it blur}_L$ rule has been accomplished by the
${\downarrow}_L$ rule.

However, Andreoli's system makes a single restriction on the copy
rule: it cannot apply to a positive atomic proposition in the
persistent context. On its own, this restriction would make the system
incomplete -- there would be no focused proof of ${!}p^+ \lolli p^+$
-- and so Andreoli-style focusing systems restore completeness by
creating a second initial sequent for positive atomic propositions that
allows a positive right focus on an atomic proposition to succeed if 
the atomic proposition appears in the persistent context:
\[
\infer[{\it init}^+]
{\andseq{\Gamma}{p^+}{[p^+]}}
{}
\qquad
\infer[{\it init}^+_p]
{\andseq{\Gamma, p^+}{\cdot}{[p^+]}}
{}
\]
With the second initial rule, we can once again prove ${!}p^+ \lolli p^+$,
and more generally, the system becomes complete again.
\[
\infer[{\lolli}_R]
{\andseq{\cdot}{\cdot}{{!}p^+ \lolli p^+}}
{\infer[{!}_L]
 {\andseq{\cdot}{{!}p^+}{p^+}}
 {\infer[{\it focus}_R]
  {\andseq{p^+}{\cdot}{p^+}}
  {\infer[{\it init}^+_2]
   {\andseq{p^+}{\cdot}{[p^+]}}
   {}}}}
\]
This modified treatment of positive atoms can be called the 
{\it atom optimization} as it reduces the number of focusing steps that 
need to be applied: it takes only one right focus to prove
${!}p^+ \lolli p^+$ in Andreoli's system, but it would take two focusing
steps to prove the same proposition in Chaudhuri's system (or to prove
${!}{\uparrow}p^+ \lolli {\uparrow}p^+$ in the focusing system we have
presented). 

There seem to be three ways of adapting the atom optimization to a polarized
setting. The first option would be to add an initial sequent that 
directly mimics the one in an Andreoli style system, while adding an additional
requirement to the {\it copy} rule that $A^-$ is not a shifted positive
atomic proposition:
\[
\infer[{\it init}^+]
{\mildseq{\Gamma,{\uparrow}p^+}{\cdot}{[p^+]}}
{}
\quad
\infer[{\it copy}^*]
{\mildseq{\Gamma, A^-}{\Delta}{U}}
{A \neq {\uparrow}p^+
 &
 \mildseq{\Gamma, A^-}{\Delta, [A^-]}{U}}
\]
The second approach is to extend suspended propositions to
the persistent context, add a corresponding rule for right focus,
and modify the left rule for ${!}$ to notice
the presence of a positive atomic proposition:
\[
\infer[{!}_{L1}]
{\mildseq{\Gamma}{\Delta, {!}A^-}{U}}
{A^- \neq {\uparrow}p^+
 &
 \mildseq{\Gamma, A^-}{\Delta}{U}}
\quad
\infer[{!}_{L2}]
{\mildseq{\Gamma}{\Delta, {!}{\uparrow}p^+}{U}}
{\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{U}}
\quad
\infer[{\it id}^+_p]
{\mildseq{\Gamma, \langle A^+ \rangle}{\cdot}{[A^+]}}
{}
\]
The third approach is to introduce a third connective, $\pbang$, that
can only be applied to positive atomic propositions, just as ${!}$ can
only be applied to negative propositions. We can initially view this
option as equivalent to the previous one by defining ${\pbang}p^+$ as
a sort of synthetic connective equivalent to ${!}{\uparrow}p^+$ in the
second approach above:
\[
\infer[{\pbang}_R]
{\mildseq{\Gamma}{\cdot}{[{\pbang}p^+]}}
{\mildseq{\Gamma}{\cdot}{p^+}}
\quad
\infer[{\pbang}_L]
{\mildseq{\Gamma}{\Delta, {\pbang}p^+}{U}}
{\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{U}}
\quad
\infer[{\it id}^+_p]
{\mildseq{\Gamma, \langle A^+ \rangle}{\cdot}{[A^+]}}
{}
\]

All three of these options are essentially similar; we will go with the
last, as it allows us to preserve the original meaning of ${!}{\uparrow}p^+$
if that is our actual intent. 

This addition to the language creates one new case of identity
expansion:
\[
\infer-[\eta^+]
{\mildseq{\Gamma}{\Delta, {\pbang}p^+}{U}}
{\deduce
 {\mildseq{\Gamma}{\Delta, \langle {\pbang}p^+ \rangle}{U}}
 {\mathcal D}}
\quad
\Longrightarrow
\infer[{\pbang}_L]
{\mildseq{\Gamma}{\Delta, {\pbang}p^+}{U}}
{\infer-[{\it subst}^+]
 {\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{U}}
 {\infer[{\pbang}_R]
  {\mildseq{\Gamma, \langle p^+ \rangle}{\cdot}{[{\pbang}p^+}]}
  {\infer[{\it focus}_R]
   {\mildseq{\Gamma, \langle p^+ \rangle}{\cdot}{p^+}}
   {\infer[{\it id}^+_p]
    {\mildseq{\Gamma, \langle p^+ \rangle}{\cdot}{[ p^+ ]}}
    {}}}
  &
  \infer-[{\it weaken}]
  {\mildseq{\Gamma, \langle p^+ \rangle}{\Delta, \langle {\pbang}p^+ \rangle}
     {U}}
  {\deduce
   {\mildseq{\Gamma}{\Delta, \langle {\pbang}p^+ \rangle}{U}}
   {\mathcal D}}}}
\]

While we could also prove another focal substitution principle,
namely that $\mildseq{\Gamma}{\cdot}{[A^+]}$ and 
$\mildseq{\Gamma, \langle A^+ \rangle}{\Delta}{U}$ imply
$\mildseq{\Gamma}{\Delta}{U}$, it's notable that such a principle 
isn't necessary in order to establish the identity property in this
system. 

Our change does force us to add a new part to the cut admissibility
theorem that is closely related to the new focal substitution
principle, proving that $\mildseq{\Gamma}{\cdot}{p^+}$ and
$\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{U}$ imply
$\mildseq{\Gamma}{\Delta}{U}$ by induction on the second given
derivation. This is used to satisfy the principal cut for
${\pbang}p^+$. All other necessary modifications to the cut
admissibility theorem are straightforward.

\subsection{The unsatisfactory nature of the atom optimization}

All three alternatives for characterizing the atom optimization are
similarly unsatisfactory from the point of view that positive atomic
propositions are stand-ins for arbitrary positive propositions. None
of the rules involving positive atomic propositions -- ${\it init}^+$
in the first proposed alternative, ${!}_{L2}$ in the second proposed
alternative, or ${\pbang}_L$ in the third alternative -- can be
gracefully generalized (as $\eta^+$ and $\eta^-$ can be) to account
for the substitution of a positive atomic proposition with a positive
proposition. 

The key issue, which is illustrated by each of these three
alternatives, is that linear logic does not allow positive
propositions to decomposed once they enter the persistent context.
When we substitute a positive atomic proposition $A^+$ for a positive
atomic proposition $p^+$, all instances of $\langle p^+
\rangle$ in the persistent context must become instances of
${\uparrow}A^+$, and in the third proposal, all instances of
${\pbang}p^+$ must become instances of ${!}{\uparrow}A^+$. The structure
of proofs must change in an unusual way, as well: before a focusing
phase begins, it is necessary to insert, below that focusing phase,
a series of focusing phases that copy a shifted positive proposition
into the linear context and decompose it, one for each occurrence of
${\it id}^+_p$ for a substituted proposition in the focusing phase.
One instance of this phenomena 
is illustrated in Figure~\ref{fig:replacement-breaks}.

\begin{figure}
\[
\infer[{\pbang}_L]
{\mildseq{\cdot}{{\pbang}p^+}{p^+ \otimes p^+}}
{\infer[{\it focus}_R]
 {\mildseq{\langle p^+ \rangle}{\cdot}{p^+ \otimes p^+}}
 {\infer[{\otimes}_R]
  {\mildseq{\langle p^+ \rangle}{\cdot}{[ p^+ \otimes p^+ ]}}
  {\infer[{\it id}^+_p]
   {\mildseq{\langle p^+ \rangle}{\cdot}{[p^+]}}
   {}
   &
   \infer[{\it id}^+_p]
   {\mildseq{\langle p^+ \rangle}{\cdot}{[p^+]}}
   {}}}}
~~
\mbox{vs.}
\!\!\!
\infer[{!}_L]
{\mildseq{\cdot}{{!}{\uparrow}{A^+}}{A^+ \otimes A^+}}
{\infer[\it copy]
 {\mildseq{{\uparrow}{A^+}}{\cdot}{A^+ \otimes A^+}}
 {\infer[{\uparrow}_L]
  {\mildseq{{\uparrow}{A^+}}{[{\uparrow}A^+]}{A^+ \otimes A^+}}
  {\infer-[\eta^+]
   {\mildseq{{\uparrow}{A^+}}{A^+}{A^+ \otimes A^+}}
   {\infer[{\it copy}]
    {\mildseq{{\uparrow}{A^+}}{\langle A^+ \rangle}{A^+ \otimes A^+}}
    {\infer[{\uparrow}_L]
     {\mildseq{{\uparrow}{A^+}}{\langle A^+ \rangle, [{\uparrow}A^+]}
        {A^+ \otimes A^+}}
     {\infer-[\eta^+]
      {\mildseq{{\uparrow}{A^+}}{\langle A^+ \rangle, A^+}
         {A^+ \otimes A^+}}
      {\infer[{\it focus}_R]
       {\mildseq{{\uparrow}{A^+}}{\langle A^+ \rangle, \langle A^+ \rangle}
          {A^+ \otimes A^+}}
       {\infer[{\otimes}_R]
        {\mildseq{{\uparrow}{A^+}}{\langle A^+ \rangle, \langle A^+ \rangle}
           {[A^+ \otimes A^+]}}
        {\infer[{\it id}^+]
         {\mildseq{{\uparrow}{A^+}}{\langle A^+ \rangle}
            {[A^+]}}
         {}
         &
         \infer[{\it id}^+]
         {\mildseq{{\uparrow}{A^+}}{\langle A^+ \rangle}
            {[A^+]}}
         {}}}}}}}}}}
\]
\caption{In systems with the atom optimization, substituting a 
positive proposition $A^+$ for a position atomic proposition $p^+$ 
is more complicated than just replacing atomic instances of $\eta^+$
with admissible non-atomic instances of $\eta^+$.}
\label{fig:replacement-breaks}
\end{figure}

To emphasize, the polarized formulation is not at fault here:
Andreoli's focusing system exhibits the same issues. It is probably
for this reason that there is very little discussion of the meaning of
atomic propositions within focused logics; the only example we are
aware of is Miller \cite{miller08proof}. The atom optimization, and
the bang optimization considered below, are both {\it useful} for the
construction of expressive synthetic connectives even if they are not,
in a certain sense, canonical for presentations of linear logic.

\subsection{Bang optimization}
\label{sec:bangopt}

The choice of adding ${\pbang}p^+$ as a special new connective instead
of defining it as ${!}{\uparrow}p^+$ paves the way for us to modify its
meaning further. For instance, there turns out to be no particular
need for the ${\pbang}_R$ rule to lose focus in its premise, even though
it is critical that ${!}_R$ lose focus in its premise; if we fail
to do so propositions like ${!}(A \otimes B) \lolli {!}(B \otimes A)$
will have no proof. 
\[
\infer[{\pbang}_R]
{\mildseq{\Gamma}{\cdot}{[{\pbang}p^+]}}
{\mildseq{\Gamma}{\cdot}{[p^+]}}
\quad
\infer[{\pbang}_L]
{\mildseq{\Gamma}{\Delta, {\pbang}p^+}{U}}
{\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{U}}
\quad
\infer[{\it id}^+_p]
{\mildseq{\Gamma, \langle A^+ \rangle}{\cdot}{[A^+]}}
{}
\]
This further optimization
will be called the {\it bang optimization}, as it, like the atom 
optimization, potentially reduces the number of focusing phases.

This additional change actually simplifies both the identity
expansion theorem, which no longer needs the ${\it focus}_R$ rule
in the left branch of the derivation. The cut admissibility
theorem is simplified as well: the extra case establishing that
$\mildseq{\Gamma}{\cdot}{p^+}$ and 
$\mildseq{\Gamma, \langle p^+ \rangle}{\Delta}{U}$ imply
$\mildseq{\Gamma}{\Delta}{U}$ is unnecessary, as the principal cut on
${\pbang}p^+$ no longer needs to invoke the induction hypothesis.

If we think of ${\pbang}p^+$ as being naturally defined as 
${!}{\uparrow}p^+$, then the atom optimization modifies the left rule
and the bang optimization modifies the right rule. It is not, however,
possible to think about the bang optimization independently from the
atom optimization: identity expansion would fail on a system that 
only provided a modified right rule for this connective. 

\subsection{A more primitive logic?}
\label{sec:moreprim}

We introduced ${\pbang}p^+$ as a connective defined as
${!}{\downarrow}p^+$ -- that is, the regular ${!}A^-$ connective plus
a little something extra, the shift. Having modified the rules of
${\pbang}$ significantly, we will now take a crack at viewing
${\pbang}$ as a more primitive connective -- that is, we will view
${!}$ as ${\pbang}$ plus a little something extra.

It is frequently observed that the exponential ${!}A$ of linear logic
appears to have two or more parts; the general idea is that ${\pbang}$
represents just one of those pieces. Accounts of linear logic that
follow the judgmental methodology of Martin-L{\"o}f
\cite{lof96meanings}, such as the analysis by Chang et al.'s
\cite{chang03judgmental}, emphasize that the regular hypothetical
sequent $\seq{\Gamma}{\Delta}{A}$ of linear logic is establishing the
judgment that $A$ is ephemerally true: we can write
$\seq{\Gamma}{\Delta}{\iseph{A}}$ to emphasize this. Persistent truth,
represented by the judgment $\ispers{A}$, is defined as ephemeral
truth using no ephemeral resources, and ${!}A$ is understood as the
internalization of persistent truth:
\[
\infer[{\it pers}]
{\pseq{\Gamma}{\ispers{A}}}
{\seq{\Gamma}{\cdot}{\iseph{A}}}
\quad
\infer[{!}'_R]
{\seq{\Gamma}{\Delta}{\iseph{{!}A}}}
{\Delta = \cdot ~~~ & \pseq{\Gamma}{\ispers{A}}}
\]
The ${\it pers}$ rule is invertible, so if we ever need to prove
$\pseq{\Gamma}{\ispers{A}}$, we asynchronously transition to proving
$\seq{\Gamma}{\cdot}{\iseph{A}}$. This observation is used to explain
why we don't normally consider persistent truth on the right in linear
logic (we consider it on the left, of course, as all the propositions
of $\Gamma$ are judged as being persistently true). Our more familiar
rule for ${!}_R$ is derivable using these two rules:
\[
\infer[{!}'_R]
{\Gamma; \Delta \vdash {!}\iseph{A} \mathstrut}
{\Delta = \cdot
 &
 \infer[{\it pers}]
 {\Gamma \vdash \ispers{A} \mathstrut}
 {\Gamma; \cdot \vdash \iseph{A}} \mathstrut}
\]

Note that the ${!}'_R$ rule is naturally synchronous (positive),
because it forces the linear context to be empty. The ${\it pers}$
rule, on the other hand, is invertible and so naturally negative,
because it represents the invertible step of deciding to prove that
$A$ is {\it persistently} true by proving that it is {\it ephemerally}
true (in a context with no ephemeral resources). This combination of
positive and negative actions explains why ${!}A^-$ is a positive
proposition with a negative subformula, and similarly explains why we
must break focus when we reach ${!}A$ on the right and why we must
stop decomposing the proposition when we reach ${!}A$ on the left.
The salient feature of our modified rules for ${\pbang}p^+$, of
course, is that they do {\it not} break focus on the right and that
they continue to decompose the proposition on the left (into a
suspended proposition $\langle p^+ \rangle$ in the persistent
context). This is the reason for arguing that ${\pbang}$ captures only
the first, purely positive, component of the ${!}$ connective.

If the $\pbang$ connective is the first part of the $!$ connective,
can we characterize the rest of the connective? Giving a reasonable
answer necessarily requires a more general account of the $\pbang$
connective -- a logic where it is generally applicable rather than
restricted to positive atomic propositions. In other words, to account
for the behavior of $\pbang$, we must give a more primitive logic into
which focused linear logic, with or without the atom and bang
optimizations, may be faithfully encoded. 

\input{figs/fig-fragment-adjoint}

A candidate for a more primitive logic, and one that has tacitly
formed the basis of much of my previous work on logic programming in
substructural logic
\cite{pfenning09substructural,simmons09weak,simmons11logical}, is {\it
  adjoint logic}.  Adjoint logic was first characterized by Benton and
Wadler as a natural deduction system \cite{benton96linear} and was
substantially generalized by Reed in a sequent calculus setting
\cite{reed09judgmental}. The logic generalizes both linear logic and
lax logic as sub-languages of a common logic, whose propositions come
in two syntactically distinct categories that are connected by the
adjoint operators $F$ and $G$:\footnote{Note this syntactic
  distinction is very different than the syntactic distinction between
  positive and negative propositions. A polarized presentation of
  adjoint logic would have four syntactic categories: $X^+$, $X^-$,
  $A^+$, and $A^-$, with one pair of shifts mediating between $X^+$
  and $X^-$ and another pair of shifts mediating between $A^+$ and
  $A^-$. To make matters worse, in Levy's Call-By-Push-Value language,
  the programming language formalism that corresponds to polarized
  logic, ${\uparrow}$ and ${\downarrow}$ are characterized as adjoints
  as well ($F$ and $U$, respectively), so a fully polarized adjoint
  logic has {\it three} distinct pairs of unary connectives that can
  be characterized as adjoints!}
\begin{align*}
\mbox{\it Persistent propositions} & &
X, Y, Z & ::= G A \mid x \mid X \supset Y \mid X \times Y\\
\mbox{\it Linear propositions} & & 
A, B, C & ::= F X \mid a \mid A \lolli B \mid A \otimes B
\end{align*}
In adjoint logic, persistent propositions $X$ are contained in the
persistent context $\Gamma$ and as the succeedants of sequents
$\pseq{\Gamma}{X}$, whereas linear propositions $A$ are contained in
the linear context $\Delta$ and as the succeedants of sequents
$\seq{\Gamma}{\Delta}{A}$. A fragment of the logic is shown in
Figure~\ref{fig:fragment-adjoint}.  Note the similarity between the
$G_L$ rule and our unfocused ${\it copy}$ rule, as well as the
similarity between $F_R$ and $G_R$ in
Figure~\ref{fig:fragment-adjoint} and the rules ${!}_R$ and ${\it
  pers}$ in the previous discussion.  Linear logic is recovered as a
fragment of adjoint logic by removing all of the persistent
propositions except for $GA$; the usual ${!}A$ is then definable as
$FGA$. Lax logic, on the other hand, is recovered by removing all of
the linear propositions except for $FX$; the usual lax modality
$\ocircle X$ is then definable as $GFX$.

My previous work has used the language of normal linear logic,
but they enforces an extra condition of {\it separation}. In our
current development, the separation condition says that each atomic
proposition either {\it always} appears as a subformula of
${\pbang}p^+$ or it {\it never} appears as a subformula of
${\pbang}p^+$. If we enforce separation, then it is easy to encode our
optimized focused logic as a fragment of a naturally focused adjoint
logic. Positive atomic propositions that are always associated with
${\pbang}$ are encoded as {\it persistent} positive atomic
propositions $x^+$, and positive atomic propositions that are never
associated with ${\pbang}$ are encoded as {\it linear} positive atomic
propositions $a^+$. The proposition ${\pbang}p^+$ can then be
translated as $F x^+$, where $x^+$ is the translation of $p^+$ as a
persistent positive atomic proposition.

In this way, adjoint logic explains why, in linear logic, we can't
naturally substitute positive propositions for positive atomic
propositions when those positive atomic propositions appear suspended
in the persistent linear context: because these propositions are
actually stand-ins for {\it persistent} propositions, not for linear
propositions, and we are working in a fragment of the logic that has
no interesting persistent propositions other than atomic propositions
$x$ and the negative inclusion $G A$ back into linear propositions.

Adjoint logic, because it requires a syntactic differentiation of
persistent and linear atomic propositions, still does not allow us to
pleasantly embed the structure of focused linear logic with the atom
and bang optimizations when separation is not enforced.  The flaw is
due to the atom optimization, which allows us to successfully right
focus on an positive atomic proposition $p^+$ if the proposition
appears suspended in the linear context {\it or} in the persistent
context. This is not natural in adjoint logic, which forces us to
syntactically specify the context where we expect to find the
proposition. A more complicated translation in to adjoint logic that
associated each atomic proposition $p^+$ with a persistent proposition
$x_p^+$ and a linear proposition $a_p^+$ would suffice; a subgoal
$p^+$ could be represented as $a_p^+ \oplus F x_p^+$, for instance.
Such a translation would be unsatisfactory both because it further
complicates the previously simple interpretation of atomic
propositions and because the translation of an atomic proposition is
non-uniform -- it depends on the contextual information of whether
that proposition it will ultimately appear on the left and the right
of the sequent.

I have conjectured that a variant of adjoint logic which does {\it
  not} syntactically differentiate between persistent and linear
propositions might be a better target for faithfully embedding focused
linear logic with and without the atom and bang optimizations; but
this is outside the scope of this thesis.

\subsection{Concurrent equality}

Consider the sequent in focused linear logic:
\[
\mildseq{a^+ \lolli {\uparrow}(b^+ \otimes c^+), ~
  b^+ \lolli {\uparrow}d^+, ~
  c^+ \lolli {\uparrow}e^+, ~
  d^+ \otimes e^+ \lolli {\uparrow}f^+ ~~}
  {~~
  \langle a^+ \rangle
  ~~}
  {~~f^+}
\]
Let $\Gamma = \left(a^+ \lolli {\uparrow}(b^+ \otimes c^+), ~
  b^+ \lolli {\uparrow}d^+, ~
  c^+ \lolli {\uparrow}e^+, ~
  d^+ \otimes e^+ \lolli {\uparrow}f^+ \right)$.
There are two different focused derivations of this
sequent, the one that transitions $\langle b^+ \rangle$ to $\langle
d^+ \rangle$ first, and the one that transitions 
$\langle c^+ \rangle$ to $\langle e^+ \rangle$ first:
\[
\infer
{\mildseq{\Gamma}{\langle a^+ \rangle}{f^+}}
{\infer
{\mildseq{\Gamma}{\langle b^+ \rangle, \langle c^+ \rangle}{f^+}}
{\infer
{\mildseq{\Gamma}{\langle d^+ \rangle, \langle c^+ \rangle}{f^+}}
{\infer
{\mildseq{\Gamma}{\langle d^+ \rangle, \langle e^+ \rangle}{f^+}}
{\infer
{\mildseq{\Gamma}{\langle f^+ \rangle}{f^+}}
{}}}}}
\qquad
\deduce
{\mathstrut}
{\deduce
{\mathstrut}
{\deduce
{\mathstrut}
{\mbox{\it vs.}}}}
\qquad
\infer
{\mildseq{\Gamma}{\langle a^+ \rangle}{f^+}}
{\infer
{\mildseq{\Gamma}{\langle b^+ \rangle, \langle c^+ \rangle}{f^+}}
{\infer
{\mildseq{\Gamma}{\langle b^+ \rangle, \langle e^+ \rangle}{f^+}}
{\infer
{\mildseq{\Gamma}{\langle d^+ \rangle, \langle e^+ \rangle}{f^+}}
{\infer
{\mildseq{\Gamma}{\langle f^+ \rangle}{f^+}}
{}}}}}
\]
If we think about these two proofs in terms of the series of
transitions they embody, it's not so clear that they are different. In
both cases, there is an $a^+$ resource that transitions to a $b^+$
resource and a $c^+$ resource, and then $b^+$ transitions to $d^+$
while, independently, the $c^+$ transitions to $e^+$. Then, finally,
the $d^+$ and $e^+$ combine to transition to $f^+$, which completes
the trace. The independence here is key: if two focusing phases
consume different resources and both end focus with ${\uparrow}_L$ (as
opposed to ${\it id}^-$), then we can treat them as independent and
concurrent steps in the process of proving the same right hand side;
{\it concurrent equality} is the equivalence relation on focused
proofs that treats all proofs that differ only in the interleaving of
independent and concurrent steps as equal.  This equivalence
relation was used in the definition of CLF \cite{watkins02concurrent},
but only in conjunction with the lax modality, an approach we will
follow in Chapter 4.

The equivalence relation on focused proofs that concurrent equality
gives rise to is related to the equivalence relation on proofs given
by {\it multifocusing} \cite{chaudhuri08canonical}.  Multifocusing 
appears to provide an even coarser notion of equivalence on focused
proofs than concurrent equality does. In particular, these two 
distinct focusing proofs are {\it not} concurrently equal: the
proof on the right succeeds at proving $\langle c^- \rangle$ in one
step, but leaves a subgoal in which $b^+$ is proved indirectly,
whereas the proof at the right first transitions 
from having $\langle a^+ \rangle$ and $a^+ \lolli {\uparrow} b^+$ 
resources to having a $\langle b^+ \rangle$ resource, and only
then proves $\langle c^- \rangle$, leaving a subgoal in which $b^+$
is proved directly. 
\[
\infer
{\mildseq{\cdot}
  {~~
   \langle a^+ \rangle, ~
   a^+ \lolli {\uparrow}b^+, ~
   {\downarrow}{\uparrow}b^+ \lolli c^-
   ~~}
  {~~\langle c^- \rangle}}
{\infer
{\mildseq{\cdot}
  {~~
   \langle a^+ \rangle, ~
   a^+ \lolli {\uparrow}b^+
   ~~}
  {b^+}}
{\infer
{\mildseq{\cdot}
  {~~
   \langle b^+ \rangle
   ~~}
  {b^+}}
{}}}
\deduce{\mathstrut}
{\deduce{\mathstrut}
{\mbox{\it vs.}\mathstrut}}
\infer
{\mildseq{\cdot}
  {~~
   \langle a^+ \rangle, ~
   a^+ \lolli {\uparrow}b^+, ~
   {\downarrow}{\uparrow}b^+ \lolli c^-
   ~~}
  {~~\langle c^- \rangle}}
{\infer
{\mildseq{\cdot}
  {~~
   \langle b^+ \rangle, ~
   {\downarrow}{\uparrow}b^+ \lolli c^-
   ~~}
  {~~\langle c^- \rangle}}
{\infer
{\mildseq{\cdot}
  {~~
   \langle b^+ \rangle
   ~~}
  {~~b^+}}
{}}}
\]
While there is no full account of multifocusing in intuitionistic
logic, the analogue of this sequent in classical linear logic has only
one multifocused proof. In classical linear logic, multifocusing
offers a very fundamental normal form: any two proofs that can be made
equal by locally permuting inference rules have the same multifocused
proof. 

CLF's concurrent equality, restricted to an association with the lax
modality, will be sufficient for the logical framework in Chapter
4. In fact, for the fragment of the the logic in Chapter 3 that
comprises our logical framework in Chapter 4, I conjecture that
concurrent equality and the equality given by multifocusing
coincide.\footnote{This obviously means that the example above will be
  outside the logical that comprises the logical framework.}  This
conjecture is obviously difficult to make precise, much less prove,
without a general theory of multifocusing in intuitionistic logic.


\section{Revisiting our notation}
\label{sec:linnote}

Andreoli, in his 2001 paper introducing the idea of synthetic
inference rules \cite{andreoli01focussing}, observed that the atom
optimization can lead to an exponential explosion in the number of
synthetic rules associated with a proposition.  For instance, if $a^+
\otimes b^+ \lolli {\uparrow}c^+$ appears in $\Gamma$, the atom
optimization means that the following are all synthetic inference
rules for that proposition:
\[
\infer
{\mildseq{\Gamma}{\Delta, \langle a^+ \rangle, \langle b^+ \rangle}{U}}
{\mildseq{\Gamma}{\Delta, \langle c^+ \rangle}{U}}
\quad
\infer
{\mildseq{\Gamma, \langle a^+ \rangle}{\Delta, \langle b^+ \rangle}{U}}
{\mildseq{\Gamma, \langle a^+ \rangle}{\Delta, \langle c^+ \rangle}{U}}
\]\[
\infer
{\mildseq{\Gamma, \langle b^+ \rangle}{\Delta, \langle a^+ \rangle}{U}}
{\mildseq{\Gamma, \langle b^+ \rangle}{\Delta, \langle c^+ \rangle}{U}}
\quad
\infer
{\mildseq{\Gamma, \langle a^+ \rangle, \langle b^+ \rangle}{\Delta}{U}}
{\mildseq{\Gamma, \langle a^+ \rangle, \langle b^+ \rangle}
   {\Delta, \langle c^+ \rangle}{U}}
\]
Andreoli suggests coping with this problem by restricting the form of
propositions so that positive atoms never appear in the persistent
context. From our perspective, this is a rather unusual
recommendation, since it just returns us to linear logic without the
atom optimization!

The proliferation of inference rules under the atom optimization is a
problem if, for instance, we need to represent synthetic inference
rules on a computer. Correctly viewed, however, the problem is merely
one of notation. It's already the case that, in writing sequent
calculus rules, we generally tacit use of a fairly large number of
notational conventions, at least relative to Gentzen's original
formulation where all contexts were treated as sequences of
propositions \cite{gentzen35untersuchungen}.  For instance, the
bottom-up reading of the ${\one}_R$ rule's conclusion,
$\mildseq{\Gamma}{\cdot}{[\one]}$, indicates the presence of an
additional premise checking that the linear context is empty, and the
conclusion $\mildseq{\Gamma}{\Delta_1, \Delta_2}{[A \otimes B]}$ of
the ${\tensor}_R$ rule indicates the condition that the context can be
split into two parts.

We deal with the apparent proliferation of rules by adding a new
matching construct for the conclusion of rules: we can say that
$\Gamma; \Delta$ matches $\Gamma; \Delta' / \langle p^+ \rangle$
either when $\langle p^+ \rangle \in \Gamma$ and $\Delta = \Delta'$ or
when $\Delta = \Delta', \langle p^+ \rangle$. We can also iterate
this construction, so that $\Gamma; \Delta$ matches
$\Gamma; \Delta_n / \langle p^+_1 \rangle, \ldots, \langle p^+_n \rangle$
if $\Gamma; \Delta$ matches $\Gamma; \Delta_1 / \langle p^+_1 \rangle$,
$\Gamma; \Delta_1$ matches $\Gamma; \Delta_2 / \langle p^+_2 \rangle$,
\ldots and $\Gamma; \Delta_{n-1}$ matches 
$\Gamma; \Delta_n / \langle p^+_n \rangle$.  Armed with this notation,
we can create a concise synthetic connective:
\[
\infer
{\mildseq{\Gamma}{\Delta/\langle a^+ \rangle, \langle b^+ \rangle}{U}}
{\mildseq{\Gamma}{\Delta, \langle c^+ \rangle}{U}}
\]

This modified notation need not only be used in synthetic connectives: we 
can also use it to combine the two positive identity rules. Furthermore, 
by giving $\Gamma; \Delta / A^-$ the obviously analogous me meaning, we can
fuse the ${\it focus}_L$ rule and the ${\it copy}_L$ rule into a single
rule that is unconcerned with whether the proposition in question came
from the persistent or linear contexts: 
\[
\infer[{\it id}^+]
{\mildseq{\Gamma}{\cdot/\langle A^+ \rangle}{[A^+]}}
{}
\quad
\infer[{\it focus}^*_L]
{\mildseq{\Gamma}{\Delta/A^-}{U}}
{\mildseq{\Gamma}{\Delta, [A^-]}{U}}
\]

\input{figs/fig-linear-alt}

Going yet one more step, we could use this notation to revise
the original definition of linear logic in Figure~\ref{fig:linear}.
The {\it copy} rule in that presentation sticks out as the only 
rule that doesn't deal directly with a connective, but we can eliminate
it by using the $\Gamma; \Delta/A$ matching construct. The resulting
presentation, shown in Figure~\ref{fig:linear-alt}, is equivalent
to the presentation in Figure~\ref{fig:linear}.

\bigskip
\begin{theorem}
$\seq{\Gamma}{\Delta}{C}$ if and only if $\altseq{\Gamma}{\Delta}{C}$.
\end{theorem}

\begin{proof}
The reverse direction is a straightforward induction: each rule in 
Figure~\ref{fig:linear-alt} can be translated as the related rule
in Figure~\ref{fig:linear} along with (potentially) an instance of 
the ${\it copy}$ rule.

The forward direction requires a lemma that the {\it copy} rule is
admissible according to the rules of Figure~\ref{fig:linear-alt}; this
lemma can be established by straightforward induction. Having
established the lemma, the forward direction is a straightforward
induction on derivations, applying the admissibility lemma whenever the 
{\it copy} rule is encountered.
\end{proof}


\section{A warning about normalization}
\label{sec:warning}

Talk about equivalence, Chris's unpublished work, and where
the focalization theorem given by this approach is deficient -- 