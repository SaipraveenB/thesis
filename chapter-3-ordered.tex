\chapter{Substructural logic}

Linear logic is the most famous of the {\it substructural logics}.
Persistent logic admits the three so-called {\it structural rules} of
weakening (premises need not be used), contraction (premises may be
used multiple times) and exchange (the ordering of premises are
irrelevant). Substructural logics, then, are logics that do not admit
these structural rules -- linear logic has only exchange, {\it affine}
logic (which is frequently conflated with linear logic by programming
language designers) has exchange and weakening, and {\it ordered}
logic, first investigated as a proof theory by Lambek
\cite{lambek58mathematics}, lacks all three.  Calling logics like
linear, affine, and ordered logic \underline{sub}structural relative
to persistent logic (which is structural) is greatly unfair to the
substructural logics. Girard's linear logic can express persistent
provability using the exponential connective ${!}A$, and this idea is
generally applicable in substructural logics -- for instance, it was
applied by Polakow and Pfenning to Lambek's ordered logic
\cite{polakow99natural}.\robnote{Check citation.} It is certainly too
late to advocate for these logics to be understood as
\underline{super}structural logics, but that is undoubtedly what they
are: generalizations of persistent logic that introduce more 
expressive power.

In this chapter, I define a first-order ordered linear logic with a
lax modality (henceforth \ollll, for ordered linear lax logic), in
both unfocused (Section~\ref{sec:ord-unfocused}) and focused
(Section~\ref{sec:ord-focused}) flavors. Then, following the
structural focalization methodology \cite{simmons11structural}, I
establish cut admissibility (Section~\ref{sec:ord-cut}), identity
expansion (Section~\ref{sec:ord-identity}) for focused~\ollll; with
these results, it is possible to prove the soundness and completeness
of focusing (Section~\ref{sec:ord-correctness}) for \ollll.  A
fragment of this system will form the basis of our logical framework
in Chapter 4, and that framework will, in turn, underpin the rest of
this thesis.

It is worth clarifying why I am presenting the much richer \ollll~here
if only the fragment detailed in Chapter~4 is needed. There are two
main reasons.  First, while we will use only a fragment of this logic
in Chapter 4, other fragments of the logic may well be interesting and
useful for other purposes. Second, the presentation in this chapter,
and in particular the discussion of substructural contexts in
Section~\ref{sec:contexts}, introduces a presentation style and
infrastructure that I believe will generalize well to richer logics,
such as the logic of bunched implications~\cite{pym02semantics},
non-commutative linear logic (or ``rigid logic'')
\cite{simmons09linear}, subexponential logics
\cite{nigam09algorithmic}, and so on. (This is not a claim I will
substantiate in this thesis, however.)

In summary, the choice to present a full account of focusing in
\ollll~is in keeping with as Andreoli's instance that we should seek,
where possible, to avoid the confusion that results from conflating
``defining a foundational paradigm or a [logic] programming language
(two objectives that should clearly be kept separate)''
\cite{andreoli01focussing}. Both the full logic \ollll~and the general
metadology followed in this chapter are general, foundational
paradigms from which it is possible to instantiate families of logic
programming languages and logical frameworks, even though we will
focus on the a particular logical framework starting in Chapter 4.

\section{Ordered linear lax logic}
\label{sec:ord-unfocused}

Ordered linear logic was the subject of Polakow's thesis
\cite{polakow01ordered}, and the adaptation of Fairtlough and
Mendler's lax logic \cite{fairtlough95propositional} (as reconstructed
by Pfenning and Davies \cite{pfenning01judgmental}) to linear logic is
the basis of the CLF logical framework
\cite{watkins02concurrent}. Putting the pieces together is a
relatively straightforward proof-theoretic exercise. There are three
contexts relevant to the propositional presentation of ordered linear
lax logic.  The persistent context $\Gamma$ and the linear context
$\Delta$ are multisets as before (so we think of $\Delta_1, \Delta_2$
as being equal to $\Delta_2, \Delta_1$, for instance). The ordered
context $\Omega$ is a sequence of propositions, as in Gentzen's
original presentation of sequent calculi, and {\it not} a multiset.
This means that the two ordered contexts $\Omega_1, \Omega_2$ and
$\Omega_2, \Omega_1$ are, in general, not the same.

\input{figs/fig-ordered-prop}

There are two sequents in \ollll.  The primary sequent is
$\oseq{\Gamma}{\Delta}{\Omega}{\istrue{A}}$, which says that $A$ is an
(ephemeral, ordered) resource derivable from the persistent resources
in $\Gamma$, the ephemeral resources in $\Delta$, and the ephemeral,
ordered resources in $\Omega$. There is also a second judgment,
$\oseq{\Gamma}{\Delta}{\Omega}{\islax{A}}$. The judgment $\islax{A}$
is usually interpreted as truth under an unspecified constraint; one
defining characteristic of the lax modality is that it if $\istrue{A}$
is derivable with some resources then $\islax{A}$ is derivable with
the same resources.

Compare this to the relationship between persistent and linear truth
in linear logic, where the defining characteristic is that a
persistent resource (associated with judgments of the form
$\ispers{A}$ in $\Gamma$) can always satisfy the need for an ephemeral
resource (associated with judgments of the form $\iseph{A}$ in
$\Delta$). In the previous chapter, we first encoded this relationship
as an explicit rule ${\it copy}$:
\[
\infer[{\it copy}]
{\seq{\Gamma, \ispers{A}}{\Delta}{\iseph{C}}}
{\seq{\Gamma, \ispers{A}}{\Delta, \iseph{A}}{\iseph{C}}}
\]
In Section~\ref{sec:linnote}, based on a discussion of synthetic
connectives under the atom optimization, we considered a revision
in which the ${\it copy}$ rule was admissible and 
left rules had conclusions that used the
construct $\altseq{\Gamma}{\Delta/A}{C}$,
which matches a sequent of the form $\altseq{\Gamma}{\Delta'}{C}$ 
if either $A \in \Gamma$ and $\Delta' = \Delta$ or if
$\Delta' = \Delta, A$. The $\oplus_L$ rule in this logic is as follows:
\[
\infer[{\oplus}_L]
{\altseq{\Gamma}{\Delta/A \oplus B}{\iseph{C}}}
{\altseq{\Gamma}{\Delta, \iseph{A}}{\iseph{C}}
 &
 \altseq{\Gamma}{\Delta, \iseph{B}}{\iseph{C}}}
\]

Lax truth can be considered along the same lines, accounting for the
fact that our primary judgment is $\istrue{A}$ ($A$ is an ordered
ephemeral resource) instead of $\iseph{A}$ ($A$ is an ephemeral
resource).  To follow existing judgmental presentations of lax logic,
we would include a distinct rule ${\it lax}$ that derives lax truth
from regular truth.
\[
\infer[{\it lax}]
{\Gamma; \Delta; \Omega \longrightarrow \islax{A}}
{\Gamma; \Delta; \Omega \longrightarrow \istrue{A}}
\]
The alternative is to make ${\it lax}$ admissible just as we made
${\it copy}$ admissible: we modify all the right rules with a construct
$\orseq{\Gamma}{\Delta}{\Omega}{A}$ that matches both 
sequents of the form $\otseq{\Gamma}{\Delta}{\Omega}{A}$
and sequents of the form $\oseq{\Gamma}{\Delta}{\Omega}{\islax{A}}$.
The use of this construct gives us right rules for 
$A \oplus B$ that look like this:
\[
\infer[{\oplus}_{R1}]
{\orseq{\Gamma}{\Delta}{\Omega}{A \oplus B}}
{\otseq{\Gamma}{\Delta}{\Omega}{A}}
\qquad
\infer[{\oplus}_{R2}]
{\orseq{\Gamma}{\Delta}{\Omega}{A \oplus B}}
{\otseq{\Gamma}{\Delta}{\Omega}{B}}
\]
The related notation on the left-hand side is the construct 
$\olseq{\Gamma}{\Delta}{\Omega_L}{A}{\Omega_R}$, which matches
the sequent $\oseq{\Gamma}{\Delta'}{\Omega'}{U}$ if
\begin{itemize}
\item $\Omega' = \Omega_L, A, \Omega_R$ and $\Delta' = \Delta$;
\item $\Omega' = \Omega_L, \Omega_R$ and $\Delta' = \Delta, A$;
\item $\Omega' = \Omega_L, \Omega_R$, $\Delta' = \Delta$, and $A \in \Gamma$.
\end{itemize}
As in the alternate presentation of linear logic (which made ${\it copy}$
admissible), both the ${\it copy}$ rule and a rule Polakow
called ${\it place}$ are admissible in the logic described
in Figure~\ref{fig:ordered-prop}.
\[
\infer-[{\it place}]
{\oseq{\Gamma}{\Delta, {A}}{\Omega_L, \Omega_R}{U}}
{\oseq{\Gamma}{\Delta}{\Omega_L, {A}, \Omega_R}{U}}
\qquad
\infer-[{\it copy}]
{\oseq{\Gamma, {A}}{\Delta}{\Omega_L, \Omega_R}{U}}
{\oseq{\Gamma, {A}}{\Delta}{\Omega_L, {A}, \Omega_R}{U}}
\]

\subsection{First-order logic}

The presentation in Figure~\ref{fig:ordered-prop} is propositional; by 
uniformly adding a first-order context $\Psi$ to all sequents, however,
it can be treated as first-order. We define 
quantification (existential and universal), as well as 
first-order equality, in Figure~\ref{fig:ordered-fo}.

\input{figs/fig-ordered-fo}

The equality judgment $t \doteq s$ deserves some attention. It is a
higher-order judgment, in the sense that it reflects over the
definition of simultaneous term substitutions $\Psi' \vdash \theta :
\Psi$ and over the equality judgment for first-order terms $t =
s$. This is a rule that, in general, will have countably many
premises; in the case of a trivially satisfiable equality problem like
$x \doteq x$ it will have one premise for each well-formed
substitution that substitutes a term of the appropriate type for
$x$. This exact style of presentation was used previously in 
\cite{simmons09weak}, but the approach is based on Schroeder-Heister's
treatment of definitional reflection \cite{schroeder93rules}.

There are two important special cases. First, an unsatisfiable 
equation on the left implies a contradiction, and the left rule
for equality is equivalent to one with no premises. For instance, this
means that
\[
\infer
{\olfseq{\Gamma}{\Delta}{\Omega_L}{0 \doteq 1}{\Omega_R}}
{}
\]
is derivable (where $0$ and $1$ are distinct constants, {\it not} the
propositions $\zero$ and $\one$). The other important special case is
when $t$ and $s$ have a {\it most general unifier} $\theta_{\it mgu}$,
which just means that for all $\Psi' \vdash \theta : \Psi$ such that
$\theta t = \theta s$, it is the case that $\theta = \theta' \circ
\theta_{\it mgu}$ for some $\theta'$.\footnote{Where $\circ$ is
  composition -- $(\theta' \circ \theta_{\it mgu})t = \theta'(\theta
  t)$.} In this case, the left rule for equality is equivalent to the
following rule:
\[
\infer
{\olfseq{\Gamma}{\Delta}{\Omega_L}{t \doteq s}{\Omega_R}}
{{\it mgu}(t, s) = \Psi \vdash \theta : \Psi'
 &
 \ofirstseq{\Psi'}{\theta\Gamma}{\theta\Delta}{\theta\Omega_L, \theta\Omega_R}{\theta U}}
\]

We have not yet thoroughly specified the type and term structure of
first-order individuals; in the next chapter we clarify that these
types and terms will actually be types and terms of Canonical LF.

\section{Substructural contexts}
\label{sec:contexts}

First-ordered linear lax logic has a lot of contexts -- the persistent
context $\Gamma$ the linear context $\Delta$, and the ordered context
$\Omega$, not to mention the first-order context $\Psi$. In most rules
these contexts just hang around, obscuring the logic's presentation
and ensuring that the {\LaTeX} code of figures and displays remains
permanently unreadable. And there are yet more contexts we might want to 
add, such as the affine contexts present in the Celf implementation
\cite{schacknielsen08celf}.

In this section, we will consider a more compact way of dealing with
the contexts which we think of as containing resources (persistent,
affine, linear, or ordered), though we will maintain the distinction
between resource contexts and first-order variable 
contexts $\Psi$.  The particular way we define substructural contexts
can be generalized substantially: it would be trivial to extend
this presentation to the affine modality or to the subexponentials
discussed by Nigam and Miller \cite{nigam09algorithmic}, and it should
be able to richer logics, such as the logic of bunched implication.

We write unified substructural contexts as either $\Delta$ or $\Xi$,
preferring the latter when there is a chance of confusing them with
linear contexts $\Delta$. For the purposes of encoding \ollll, we can
see these contexts as sequences, defined by the grammar
\[
\Xi ::= \cdot 
  \mid \Xi, x{:}\ispers{T}
  \mid \Xi, x{:}\iseph{T}
  \mid \Xi, x{:}\istrue{T}
\]
where each of the {\em variables} $x$ are distinct, so that the
context also represents a finite map from variables $x$ to {\it
  judgments} $\islvl{T}$, where $\mlvl$ is either $\mpers$, $\meph$,
or $\mtrue$.  By separating out a substructural context into three
subsequences of persistent, linear, and ordered judgments, we can
recover the presentations of contexts for \ollll~given in
Figure~\ref{fig:ordered-prop}. We will use this idea to informally
motivate our notation, writing $\Xi = \Gamma; \Delta; \Omega$.

The domain represented by the metavariable $T$ is arbitrary: when
discussing the unfocused logic given in Figure~\ref{fig:ordered-prop},
$T$ varies over unpolarized propositions $A$, but when discussing a
focused logic in Section~\ref{sec:ord-focused} it will vary over
stable negative propositions $A^-$, positive suspended propositions
$\susp{A^+}$, focused negative propositions $[A^-]$, and inverting
positive propositions $A^+$.



% The first step towards this understanding 
% has already been take in Figure~\ref{fig:linear-alt} and
% Figure~\ref{fig:ordered-prop}, which make it quite obvious that the
% context-{\it matching} notation that we perform in the conclusion of
% an inference rule may not be the same as the context-{\it extending}
% notation we use in the premise of that rule.

The key observation of this presentation was already present in the
unfocused logic presented in Figure~\ref{sec:ord-unfocused}: we need
to differentiate {\it constructions}, which appear in the premises of
rules, and {\it matching constructs}, which appear in the conclusions
of rules.  The notation $\Gamma; \Delta; \Omega_L/A\fuse B/\Omega_R$
that appears in the conclusion of ${\fuse}_L$ is a matching construct;
as discussed in Section~\ref{sec:ord-unfocused}, there are multiple
ways in which a context $\Gamma'; \Delta'; \Omega'$ could match this
context, because $A \fuse B$ could come from any of the three
contexts. However, $\Gamma; \Delta; \Omega_L,{A},{B},
\Omega_R$ in the premise of ${\fuse}_L$ is a construction, and is
unambiguously equal to only one context $\Gamma'; \Delta'; \Omega'$.

\subsection{Fundamental operations on contexts}

The first fundamental idea we consider is {\it singleton} contexts.
We construct a single-element context by writing $x{:}\islvl{T}$.
The corresponding matching construct on contexts is 
$x{:}{T}$. In unfocused linear logic, we say that $\Xi$ matches 
$x{:}{A}$ if its decomposition into persistent, linear, and 
ordered contexts matches $\Gamma; \cdot; /A/$. Specifically,

\bigskip
\begin{definition}[Sole membership]
  $\Xi$ matches $x{:}T$ if
\begin{itemize}
\item $\Xi$ contains no linear judgments and contains exactly
one
ordered judgment $x{:}\istrue{T}$ (corresponding to the situation where
$\Xi = \Gamma; \cdot; T$), 
\item $\Xi$ contains no ordered judgments and contains exactly
one linear judgement $x{:}\iseph{T}$ (corresponding to the situation where
$\Xi = \Gamma; T; \cdot$), or 
\item $\Xi$ contains only persistent judgments, including
$x{:}\ispers{T}$ (corresponding to the situation where
$\Xi = \Gamma, T; \cdot; \cdot$). 
\end{itemize}
\end{definition}
\bigskip

Sole membership is related to the initial sequents and the 
matching construct $\Gamma; \cdot;/A/$ for contexts that was used
in Figure~\ref{fig:ordered-prop}.
We could rewrite the {\it init} rule
from that figure as follows:
\[
\infer[{\it init}]
{x{:}p \Rightarrow /{p}/}
{}
\]
As in all rules involving matching constructs in the conclusion,
it is fair to view the matching construct as an extra premise; thus,
the ${\it init}$ rule above is the same as the ${\it init}$ rule 
below:
\[
\infer[{\it init}]
{\Xi \Rightarrow /{p}/}
{\Xi \textit{~matches~} x{:}p}
\]

The second basic operation on contexts requires a new concept, {\it
  frames} $\Theta$. Intuitively, we can look frames as substructural
contexts for \ollll~as a series of persistent, linear, and ordered
contexts where the ordered context is missing a particular piece. We
can write this missing piece as a box: $\Gamma; \Delta; \Omega_L,
\Box, \Omega_R$. Alternatively we can think of a frame as a one-hole
context or Huet-style zipper \cite{huet97zipper} over the structure of
substructural contexts. We will actually think of them essentially
linear LF functions $(\lambda\Xi.\, \Xi_L, \Xi, \Xi_R)$
\cite{simmons09linear}, or, more generally, as linear representational
functions defined by substitution.

The construction associated with frames, $\tackon{\Theta}{\Xi}$,
is just a straightforward operation of filling in the hole or 
beta-reducing the linear function; doing this requires that the 
variables in $\Theta$ and $\Xi$ be distinct. If we
think of $\Theta$ informally as $\Gamma; \Delta; \Omega_L, \Box,
\Omega_R$, then this is {\it almost} like the operation of filling in
the hole, as $\tackon{\Theta}{x{:}\istrue{A}} = \Gamma; \Delta;
\Omega_L, A, \Omega_R$. The main difference is that we can also use
the operation to insert linear propositions
($\tackon{\Theta}{x{:}\iseph{A}} = \Gamma; \Delta, A; \Omega_L,
\Omega_R$) and persistent propositions
($\tackon{\Theta}{x{:}\ispers{A}} = \Gamma, A; \Delta; \Omega_L,
\Omega_R$).

The matching construct for frames is a bit more complicated,
Informally, if we treat linear contexts as multisets and say that $\Xi
= \Gamma; \Delta, \Delta'; \Omega_L, \Omega', \Omega_R$, then we can
say $\Xi = \frameoff{\Theta}{\Xi'}$ in the case that $\Theta = \Gamma;
\Delta; \Omega_L, \Box, \Omega_R$ and $\Xi' = \Gamma; \Delta';
\Omega'$. The sub-context $\Xi'$, then, has been {\it framed off} from
$\Xi$, its frame is $\Theta$. If we only had ordered judgments
$\istrue{T}$, then the framing-off matching construct
$\frameoff{\Theta}{\Xi'}$ would be essentially the same as the
construction form $\tackon{\Theta}{\Xi'}$. However, persistent and
linear judgments can be reordered in the process of matching, and
persistent judgments always end up in both the frame and the 
framed-off context. 

\bigskip
\begin{definition}[Framing off]
$\Xi$ matches $\frameoff{\Theta}{\Xi'}$ if the union of the variables in 
$\Theta$ and $\Xi'$ is exactly the variables in $\Xi$ and
\begin{itemize}
\item if $x{:}\ispers{T} \in \Xi$, then the same mapping appears in 
  $\Theta$ and $\Xi'$;
\item if $x{:}\iseph{T} \in \Xi$ or $x{:}\istrue{T} \in \Xi$, 
  then the same mapping appears in $\Theta$ or $\Xi'$ (but not both); 
\item if the sequence of ordered propositions in $\Theta$ and $\Xi'$
  is consistent with the order in  $\Xi$; and 
\item if $x{:}\istrue{T} \in \Theta$, then either
  \begin{itemize}
  \item for all $y{:}\istrue{T'} \in \Xi'$, the mapping for $x$ appeared before
    the mapping for $y$ in $\Xi$, or
  \item for all $y{:}\istrue{T'} \in \Xi'$, the mapping for $x$ appeared after
    the mapping for $y$ in $\Xi$. 
  \end{itemize}
\end{itemize}
\end{definition}
\bigskip

An important derived matching construct is $\frameoff{\Theta}{x{:}T}$,
which matches $\Xi$ if $\Xi$ matches $\frameoff{\Theta}{\Xi'}$ for
some $\Xi'$ such that $\Xi'$ matches $x{:}T$.  This pattern is needed
to describe almost every left rule from Figure~\ref{fig:ordered-prop},
for instance:
\[
\infer[]
{\frameoff{\Theta}{x{:}A \oplus B} \Rightarrow U}
{\tackon{\Theta}{y{:}\istrue{A}} \Rightarrow U
 &
 \tackon{\Theta}{z{:}\istrue{B}} \Rightarrow U}
\quad
\infer[]
{\frameoff{\Theta}{x{:}A \with B} \Rightarrow U}
{\tackon{\Theta}{y{:}\istrue{A}} \Rightarrow U}
\quad
\infer[]
{\frameoff{\Theta}{x{:}A \with B} \Rightarrow U}
{\tackon{\Theta}{y{:}\istrue{B}} \Rightarrow U}
\]

We can also use this notation to describe the one of the
cut principles for ordered linear lax logic. 
\[
\infer-[{\it cut}]
{\frameoff{\Theta}{\Xi} \Rightarrow \istrue{C}}
{\Xi \Rightarrow \istrue{A}
 &
 \tackon{\Theta}{x{:}A\,{\it true}} \Rightarrow \istrue{C}}
\]
Especially for the eventual proof of this cut principle, it is important
to consider that this cut principle is equivalent to the one that describes
the matching as an explicit extra premise:
\[
\infer-[{\it cut}]
{\Xi' \Rightarrow \istrue{C}}
{\Xi \Rightarrow \istrue{A}
 &
 \tackon{\Theta}{x{:}A\,{\it true}} \Rightarrow \istrue{C}
 &
 \Xi' \textit{~matches~} \frameoff{\Theta}{\Xi}}
\]

\futurework{
As an aside: it need not always be the case that the same operation
used to describe 

The idea that the operators $\frameoff{\Theta}{\Xi}$
and $\tackon{\Theta}{\Xi}$ are sufficient to describe the 
cut principle is related to the display property, which fails
for some reasonable logics, such as Reed's queue logic
\cite{reed09queue}.}

\subsection{Multiplicative operations}

To describe the multiplicative connective of \ollll, including the critical
connective of implication, we need to have multiplicative operations on 
contexts. As a construction, $\mkconj{\Xi_L}{\Xi_R}$ is just the
syntactic concatenation of two contexts with distinct variable domains, and
the unit $\mkunit$ is just the empty sequence. The matching constructs
are more complicated to define as usual, but the intuition is, again, 
uncomplicated: if 
$\Xi = \Gamma; \Delta, \Delta'; \Omega_L, \Omega_R$, where linear contexts
are multisets and ordered contexts are sequences, then 
$\Xi = \mkconj{\Xi_L}{\Xi_R}$ if $\Xi_L = \Gamma; \Delta; \Omega_L$ and
$\Xi_R = \Gamma; \Delta'; \Omega_R$. Note that we are giving up on
having different notation for constructions and matching constructs: 
$\matchconj{\Xi_L}{\Xi_R}$ is a matching construct when it appears
in the conclusion of a rule, $\mkconj{\Xi_L}{\Xi_R}$ is a construction
when it appears in the premise of a rule.

\bigskip
\begin{definition}[Conjunction]
$\Xi$ matches $\matchunit$ if $\Xi$ contains only
persistent variables.

\smallskip
\noindent
$\Xi$ matches $\matchconj{\Xi_L}{\Xi_R}$ if the union 
of the variables in $\Xi_L$ and $\Xi_R$ is exactly the variables in $\Xi$
and 
\begin{itemize}
\item if $x{:}\ispers{T} \in \Xi$, then the same mapping appears in $\Xi_L$
  and $\Xi_R$;
\item if $x{:}\iseph{T} \in \Xi$ or $x{:}\istrue{T} \in \Xi$, then the
  same mapping appears in $\Xi_L$ or $\Xi_R$ (but not both); 
\item if the sequence of ordered propositions in $\Xi_L$ and $\Xi_R$
  is consistent with the order in  $\Xi$; and 
\item if $x{:}\istrue{T} \in \Xi_L$ and $y{:}\istrue{T'} \in \Xi_R$, then
  the mapping for $x$ appeared before the mapping for $y$ in $\Xi$. 
\end{itemize}
\end{definition}
\bigskip

The constructs for multiplicative conjunction are put to obvious use
in the description of multiplicative conjunction, which is essentially
just the propositional internalization of context conjunction:
\[
\infer
{\matchconj{\Xi_L}{\Xi_R} \Rightarrow /A \fuse B/}
{\Xi_L \Rightarrow \istrue{A} & \Xi_R \Rightarrow \istrue{B}}
\quad
\infer
{\frameoff{\Theta}{x{:}A \fuse B} \Rightarrow U}
{\tackon{\Theta}{\mkconj{y{:}A}{z{:}B}} \Rightarrow U}
\quad
\infer
{\cdot \Rightarrow \one}
{}
\quad
\infer
{\frameoff{\Theta}{x{:}\one} \Rightarrow U}
{\tackon{\Theta}{\cdot} \Rightarrow U}
\]\[
\infer
{\Xi \Rightarrow / A \lefti B /}
{x{:}\istrue{A}, \Xi \Rightarrow \istrue{B}}
\quad
\infer
{\frameoff{\Theta}{\Xi_A, x{:}A \lefti B} \Rightarrow U}
{\Xi_A \Rightarrow \istrue{A} & \tackon{\Theta}{y{:}\istrue{B}} \Rightarrow U}
\]
Implication makes deeper use of context conjunction:
% we can look at 
$\Xi$ matches
$\frameoff{\Theta}{\matchconj{\Xi_A}{x{:}A \lefti B}}$ 
%in two ways.
%One way of reading this notation 
exactly when there exist $\Xi'$ and $\Xi''$ such that 
% is 
$\Xi$ matches $\frameoff{\Theta}{\Xi'}$, 
$\Xi'$ matches $\matchconj{\Xi_A}{\Xi''}$,
and $x{:}A$ matches $\Xi''$. 

% The other
% way of reading the notation 
% is that $\Xi$ matches $\frameoff{\Theta'}{x{:}A \lefti B}$, and
% $\Theta'$ matches $\frameoff{\Theta}{\matchconj{\Xi_A}{\Box}}$, 
% It is critical that these two ways of 

% \bigskip
% \begin{definition}[Matching into frames]
% $\Theta$ matches $\frameoff{\Theta'}{\matchconj{\Xi_A}{\Box}}$ if, 
% for all $\Xi'$ and $\Xi_A$, $\Xi'$ matches $\matchconj{\Xi_A}{\Xi_B}$
% if and only if 

% For all $\Xi$, $\Xi_A$, $\Xi_B$ and $\Theta$, 
% \begin{itemize}
% \item there exists $\Xi'$ such that $\Xi$ matches $\frameoff{\Theta}{\Xi}$
%   and $\Xi$ matches $\matchconj{\Xi_A}{\Xi_B}$
%   if and only if there exists $\Delta'$ such that $\Xi$ matches 
%   $\frameoff{\Theta}{\Xi_A}$ and $\Theta$ matches 
%   $\frameoff{\Theta'}{}$
% \end{itemize}
% \end{definition}


% As a matching construct, $\Xi = \matchconj{\Xi_L}{\Xi_R}$ if every 
% $x{:}\ispers{T}$ in $\Xi$ appears in both $\Xi_L$ and $\Xi_R$, every 
% $x{:}\iseph{T}$ or $x{:}\istrue{T}$ 
% in $\Xi$ appears in exactly one of $\Xi_L$ or $\Xi_R$, and
% if furthermore every 
% $x{:}\iseph{T}$ $\Xi_L$


% As a construction form, 


\subsection{Exponential operations}

The exponentials ${!}$ and ${\gnab}$ do not have a construction form
associated with you -- or, alternatively, we can view the singleton
construction forms $x{:}\ispers{T}$ and $x{:}\iseph{T}$ as
construction forms associated with these exponentials. The matching
operation is quite simple: $\Xi$ matches $\restrictto{\Xi}{\mpers}$ if
$\Xi$ contains no ephemeral or ordered judgments -- in other words,
it says that $\Xi = \Gamma; \cdot; \cdot$. This form can then be used
to describe the right rule for ${!}A$ in unfocsed ordered logic:
\[
\infer
{\restrictto{\Xi}{\mpers} \Rightarrow /{!}A/}
{\Delta \Rightarrow \istrue{A}}
\]
Similarly, $\Xi$ matches $\restrictto{\Xi}{\meph}$ if $\Xi$ contains
no ordered judgments (that is, if $\Xi = \Gamma; \Delta; \cdot$).
$\Xi$ always matches $\restrictto{\Xi}{\mtrue}$; we don't ever
explicitly use this construct, but it allows us to generally refer to
$\restrictto{\Xi}{\mlvl}$ in the statement of cut admissibility.

The exponential matching constructs 
don't actually modify contexts in the way
other matching constructs do, but this is a consequence of the 
particular choice of logic we're considering. Given affine resources,
for instance, the matching construct associated with ${!}A$ would
clear the context of affine facts: 
$\Xi$ would match $\restrictto{\Xi'}{\mpers}$ if 
$\Xi'$ was $\Xi$ with all affine resources removed. 

We can describe a mirror-image operation on succeedents $U$.  $U$
matches $\restrictfrom{U}{\mlax}$ only if it has the form $\islax{T}$,
and $U$ always matches $\restrictfrom{U}{\mtrue}$ always, a degenerate
form that similarly allows us to refer to $\restrictfrom{U}{\mlvl}$ in
the statement of cut admissibility.

\section{Focused sequent calculus}
\label{sec:ord-focused}

A sequent in the focusing calculus has the form
$\foc{\Psi}{\Delta}{U}$, where $\Psi$ is the first-order variable
context, $\Delta$ is a substructural context described in the previous
section, and $U$ is a succeedent. The domain $T$ of the substructural
context consists of stable negative propositions $A^-$, positive
suspended propositions $\susp{A^+}$, focused negative propositions
$[A^-]$, and inverting positive propositions $A^+$.

The form of the succedent $U$ is $\islvl{T}$, where $\mlvl$ is either
$\mtrue$ or $\mlax$; in this way, $U$ is just a like a special
substructural context with exactly one element -- we don't need
to care about the name of the variable, because there's only one.  The
domain of $T$ for succedents is complementarly to the domain of $T$
for contexts: stable positive propositions $A^+$, negative suspended
propositions $\susp{A^-}$, focused positive propositions $[A^+]$, and
inverting negative propositions $A^-$.

\subsection{Restrictions on the form of sequents}

A sequent $\foc{\Psi}{\Delta}{U}$ is {\it stable} when the context
$\Delta$ and succeedent $U$ contain only stable propositions ($A^-$ in
the context, $A^+$ in the succedent) and suspended propositions
($\susp{A^+}$ in the context, $\susp{A^-}$ in the succeedent). We
repeat the focusing constraint discussed in Chapter 2: there is only
ever at most one focused proposition in a sequent, and if there is
focused proposition in the sequent, then the sequent is otherwise
stable. A restriction on the rules ${\it focus}_L$ and ${\it focus}_R$
is sufficient to enforce this restriction: reading rules from
top-down, we can only use a rule ${\it focus}_L$ or ${\it focus}_R$ to
prove a stable sequent, and reading rules from bottom-up, we can only
apply ${\it focus}_L$ or ${\it focus}_R$ when we are searching for a
proof of a stable sequent.

Because there is always a distinct focused proposition in a sequent,
we do not need a variable name to reference the focused proposition in
a context $\Delta$ any more than we need a variable name to reference
the unique member of the context-like succeedent $U$. Therefore, we
write $\istrue{[B^-]}$ instead of $x{:}\istrue{[B^-]}$. Furthermore,
we restrict focused propositions and inverting propositions so that
they are always associated with the judgment $\mtrue$. With this
restriction, we can write $[A^-]$ and $x{:}A^+$ instead of
$\istrue{[A^-]}$ and $x{:}\istrue{A^+}$ in $\Delta$, and we can write
$[A^+]$ and $A^-$ instead of $\istrue{[A^+]}$ and $\istrue{A^-}$ for
$U$.

In a confluent presentation of focused logic like the one given for
linear logic in Chapter 2, that would be as far as we could take our
simplifications. However, this presentation will match the fixed
presentation of logic from the structural focalization development
\cite{simmons11structural}. Specifically, if there is more than one
invertible proposition in a sequent, {\it only} the leftmost one will
be treated as eligible to have a rule applied to it. All the
propositions in $\Delta$ are treated as being to the left of the
succeedent $U$, so we always prioritize inversion on positive
propositions in $\Delta$. With this additional restriction, it is
always unambiguous which proposition we are referring to in an
invertible rule, and we write $A^+$ instead of $x{:}A^+$ or
$x{:}\istrue{A^+}$.

We will maintain the notational convention in this chapter that
first-order variables are written as $a$, variables associated
with stable negative propositions are written as $x$, and variables
associated with suspended positive propositions are written as 
$z$, but this convention will be abandoned in future chapters.

\subsection{Polarized propositions}

The propositions of ordered logic are fundamentally sorted into
positive propositions $A^+$ and negative propositions $A^-$; both
classes, and the inclusions between them, 
as shown in Figure~\ref{fig:ordered}. The
positive propositions have a refinement, {\it permeable} propositions
$A^+_\mpers$, that is analgaous to the refinement discussed for linear
logic in Section~\ref{sec:permeable}. There is also a more generous
refinement, the {\it mobile} propositions, $A^+_\meph$, for positive
propositions that bottom out with one of the two modalities ${!}$ and
${\gnab}$. We introduce atomic propositions $p^+$ that stand for
arbitrary positive propositions, $p^+_\meph$ that stand for arbitrary
mobile propositions, and $p^+_\mpers$ that stand for arbitrary
permeable propositions. We treat $A^+_\mtrue$ and $p^+_\mtrue$ as synonomous
with $A^+$ and $p^+$, respectively, which allows us to generically
refer to $A^+_\mlvl$ and $p^+_\mlvl$ in rules like $\eta^+$ and in the
statement of the identity expansion theorem.

\begin{figure}
\begin{align*}
A^+ & ::= p^+ \mid p^+_\meph \mid p^+_\mpers
        \mid {\downarrow}A^- \mid {\gnab}A^- \mid {!}A^- 
        \mid \one \mid A^+ \fuse B^+ \mid \zero \mid A^+ \oplus B^+ 
        \mid \exists x. A^+ \mid t \doteq s
\\
A^+_\meph & ::= p^+_\meph \mid p^+_\mpers
        \mid {\gnab}A^- \mid {!}A^- 
        \mid \one \mid A^+_\meph \fuse B^+_\meph
        \mid \zero \mid A^+_\meph \oplus B^+_\meph
        \mid \exists x. A^+_\meph \mid t \doteq s
\\
A^+_\mpers & ::= p^+_\mpers 
        \mid {!}A^- 
        \mid \one \mid A^+_\mpers \fuse B^+_\mpers 
        \mid \zero \mid A^+_\mpers \oplus B^+_\mpers
        \mid \exists x. A^+_\mpers \mid t \doteq s
\\
A^- & ::= p^- \mid p^-_\mlax 
        \mid {\uparrow}A^+ \mid {\ocircle}A^+
        \mid A^+ \righti B^- \mid A^+ \lefti B^-
        \mid \top \mid A^- \with B^-
        \mid \forall x.A^-
\\
A^-_\mlax & ::= p^-_\mlax 
        \mid {\ocircle}A^+
        \mid A^+ \righti B^-_\mlax \mid A^+ \lefti B^-_\mlax
        \mid \top \mid A^-_\mlax \with B^-_\mlax
        \mid \forall x.A^-_\mlax
\end{align*}
\caption{Propositions of focused \ollll.}
\label{fig:ordered}
\end{figure}

Negative propositions also have a refinement, $A^-_\mlax$, for
negative propositions that always bottom out as a modal
${\ocircle}A^+$.  This is interesting as a formal artifact and there
is very little overhead involved in putting it into our development,
but I don't claim to fully understand what the refinement means or what the
inclusion of right-permeable atomic propositions $p^-_\mlax$ mean for
the structure of proofs. Certainly we do {\it not} want to include
such propositions in the logical framework, as to do so would break the
distinction between concurrent and deductive proofs that will
be discussed in Section~\ref{sec:framework-logicprog}.

The presentation of the modalities, and the logic that we now present,
emphasises the degree to which the shifts ${\uparrow}$ and
${\downarrow}$ have much of the character of modalities in a focused
substructural logic; this point was explored in depth by Laurent in
his thesis \cite{laurent02etude}. The upshift ${\uparrow}A^+$ is like
an ordered variant of the lax modality ${\ocircle}A^+$ that puts no
constraints on the form of the succeedant, and the downshift
${\downarrow}A^-$ is like an ordered variant of the persistent and
linear modalities ${!}A^-$ and ${\gnab}A^-$ that puts no constraints
on the form of the context.

\subsection{Derivations and proof terms}
\label{sec:ord-proof-terms}

\input{figs/fig-foc-mall}
\input{figs/fig-foc-add}
\input{figs/fig-foc-fo}

The multiplicative and exponential fragment of focused \ollll~is given
in Figure~\ref{fig:foc-mall}, the additive fragment is given in
Figure~\ref{fig:foc-add}, and the first-order connectives are treated
in Figure~\ref{fig:foc-fo}. These rules are all written with sequents
of the form $\foct{\Psi}{\Delta}{E}{U}$, where $E$ is a {\it proof
  term} that corresponds to the sequent. We get the logic in terms of
sequents $\foc{\Psi}{\Delta}{U}$ by ignoring these proof terms. Just
as sequent forms are divided into the right-focused, inverting, and
left-focused sequents, we divide expressions into {\it values} $V$,
which are associated with right-focused sequents; {\it terms} $N$,
which are associated with inverting sequents; and {\it spines} $\Sp$,
which are associated with left-focused sequents. The structure of
values, terms, and spines is as follows:
\begin{align*}
V & ::= z                     %% z
   \mid \tbangr{N}            %% !N
   \mid \tgnabr{N}            %% $N
   \mid \tdownr{N}            %% N
%
   \mid \toner
   \mid \tfuser{V_1}{V_2}     %% V1 * V2
%  
   \mid \tinl{V}
   \mid \tinr{V}
   \mid \texistsr{t}{V}
   \mid \tunifr
 \\
N & ::= \tfocusr{V}           %% V
   \mid \tfocusl{x}{\Sp}      %% x @ Sp
   \mid \tetap{z}{N}          %% <z>.N
   \mid \tbangl{x}{N}         %% !x.N
   \mid \tgnabl{x}{N}         %% $x.N
   \mid \tdownl{x}{N}         %% x.N
   \mid \tupr{N}              %% N
   \mid \tlaxr{N}             %% {N}
\\ & ~~~~ %
   \mid \tetan{N} 
   \mid \tupr{N}
   \mid \tfusel{N}            %% *N
   \mid \tlamr{N}             %% >N
   \mid \tlaml{N}             %% <N
%
   \mid \toplusl{N_1}{N_2}    %% [N1,N2]
   \mid \ttopr 
   \mid \twithr{N_1}{N_2}     %% N1 & N2
   \mid \texistsl{a}{N}
   \mid \tforallr{a}{N}
   \mid \phi
\\
S & ::= \tnil                 %% nil
   \mid \tupl{N}              %% N
   \mid \tlaxl{N}             %% {N} 
   \mid \tappr{V}{\Sp}        %% V > Sp
   \mid \tappl{V}{\Sp}        %% V < Sp
   \mid \tpione{\Sp}          %% fst Sp
   \mid \tpitwo{\Sp}          %% snd Sp
   \mid \tforalll{t}{\Sp}
\end{align*}

Expressions are treated as in the structural focalization methodlogy
\cite{simmons11structural}. It is possible to take a view of
expressions as {\it extrinsically} typed, which means we consider both
well-typed and ill-typed expressions, and the well-typed expressions
are those for which the sequent $\foct{\Psi}{\Delta}{E}{U}$ is
derivable. However, we will take the view that expressions are
intrinsically typed representatives of derivations: that is,
$\foct{\Phi}{\Delta}{V}{[A^+]}$ is expresses that $V$ is a derivation
of the sequent $\foc{\Psi}{\Delta}{[A^+]}$. Justifying this close
correspondence requires us to take care that the structure of
expressions is faithful to the structure of proofs. This is the
primary reason that we don't introduce the patterns that are common in
other proof term assignments for focused logic
\cite{krishnaswami09focusing}.

There are two caveats to the idea that expressions are representatives
of derivations. One caveat is that, in order for there to be an actual
correspondance between expressions and terms, we need to annotate all
variables with the judgment they are associated with, and we need to
annotate $\tinr{V}$, $\tinl{V}$, $\tpione{\Sp}$, and $\tpitwo{\Sp}$
terms with the type of the branch not taken. Pfenning writes these as
superscripts \cite{pfenning08church}, but which we will follow Girard
in leaving them implicit \cite{girard89proofs}. The second caveat is
that if $\foct{\Psi}{\Delta}{E}{U}$, then $\foct{\Psi,
  a{:}\tau}{\Delta, x{:}\ispers{A^+}}{E}{U}$ as well. Therefore, even
given appropriate annotations, when we say that some expression $E$ is
a derivation of $\foc{\Psi}{\Delta}{U}$, that does not mean that it is
{\it uniquely} a derivation of that sequent. Fundamentally, this is
just a statement of the fact that weakening holds for persistent facts
and first-order variables in our logic.

The use of proof terms allows for a greatly compressed discussion of
cut admissibility and identity expansion. Furthermore, we will reuse
much of this notation in our formulation of a logical framework in the
next chapter. 

\subsection{Variable substitution}

The introduction of first-order unification forces us to very explicitly
handle a point that, in other contexts, may be left somewhat ambiguous:
that the variables introduced by universal quantifers (on the right)
and existential quantifiers (on the left) are {\it variables} and, as
such, are defined by substitution: a sequent with free variables is a 
{\it generic} representative of all the sequents that can be obtained
by substituting terms in for those free variables. This intuition
is formalized by the variable substitution theorem, Theorem~\ref{thm:varsubst}.

\bigskip
\begin{theorem}[Variable substitution]
\label{thm:varsubst}
If $\Psi' \vdash \sigma : \Psi$ and $\foc{\Psi}{\Delta}{U}$, then 
$\foc{\Psi'}{\sigma\Delta}{\sigma{U}}$.
\end{theorem}

\begin{proof}
On the level of proof terms, 
we are given $E$, a expression corresponding to a derivation of
$\foc{\Psi}{\Delta}{U}$; we are defining the operation $\sigma{E}$,
an expression corresponding to a derivation of 
$\foc{\Psi'}{\sigma\Delta}{\sigma{U}}$.

\paragraph{Propositional fragment}
For the exponential, multiplicative, and additive fragments, this
operation is simple to define at the level of proof terms, and we will
omit most of the cases: $\sigma(\tfuser{V_1}{V_2}) =
\tfuser{\sigma{V_1}}{\sigma{V_2}}$, $\sigma(\tdownl{x}{N}) =
\tdownl{x}{\sigma N}$, and so on. However, this compact notation does
capture a great deal of complexity. In particular, it is important to
emphasize that we need lemmas saying that variable substitution is
compatible with all the context matching operations from
Section~\ref{sec:contexts}.  In full detail, these two simple cases
would be:

$\sigma(\tfuser{V_1}{V_2}) = \tfuser{\sigma{V_1}}{\sigma{V_2}}$ --
We are given a proof of $\foc{\Psi}{\Delta}{[A^+ \fuse B^+]}$ that
ends with the ${\fuse}_R$ rule; the subderivations are
$V_1$, a derivation of $\foc{\Psi}{\Delta_1}{[A^+]}$, and
$V_2$, a derivation of $\foc{\Psi}{\Delta}{[B^+]}$. Furthermore, we know that
$\Delta$ matches $\Delta_1, \Delta_2$. We need a lemma that
tells us that $\sigma\Delta$ matches $\sigma\Delta_1, \sigma\Delta_2$;
then, by rule ${\fuse}_R$, it suffices to show that
$\foc{\Psi'}{\sigma\Delta_1}{\sigma{A^+}}$ (which we have by the 
induction hypothesis on $\sigma$ and $V_1$) and that
$\foc{\Psi'}{\sigma\Delta_2}{B^+}$ (which we have by the induction hypothesis
on $\sigma$ and $V_2$). 

$\sigma(\tdownl{x}{N}) = \tdownl{x}{\sigma N}$ -- We are given a proof
of $\foc{\Psi}{\Delta}{U}$ that ends with ${\downarrow}_L$; 
the subderivation is $N$, a derivation of
$\foc{\Psi}{\tackon{\Theta}{x{:}\istrue{A^-}}}{U}$. Furthermore, we know that
$\Delta$ matches $\frameoff{\Theta}{{\downarrow}A^-}$. We need a lemma
that tells us that $\sigma\Delta$ matches
$\frameoff{\sigma\Theta}{{\downarrow}\sigma A^-}$; then, by 
rule ${\downarrow}_L$, it suffices to show 
$\foc{\Psi'}{\tackon{\sigma\Theta}{x{:}\istrue{\sigma{A^-}}}}{U}$ (which
we have by the induction hypothesis on $\sigma$ and $N$).


\paragraph{First-order fragment} For the first-order fragment, we need
to be more careful, especially for considering unification, which notably
does {\it not} require an invocation of the induction hypothesis. The cases
for the $\exists$ quantifier mimic the ones we give for the $\forall$
quantifier.

$\sigma(\phi) = \left( \mathsf{fn}~\sigma' \Rightarrow \phi (\sigma'
  \circ \sigma) \right)$ -- We are given a proof of
$\foc{\Psi}{\Delta}{U}$ that ends with ${\doteq}_L$; we know that
$\Delta$ matches $\frameoff{\Theta}{t \doteq s}$, and the
subderivation is $\phi$, a function from substitutions 
$\Psi'' \vdash \sigma'' : \Psi$ that unify $t$ and $s$ to derivations
of $\foc{\Psi''}{\tackon{\sigma''\Theta}{\cdot}}{\sigma''{U}}$. We need a lemma
that tells us that $\sigma\Delta$ matches 
$\frameoff{\sigma\Theta}{\sigma t \doteq \sigma s}$; then, by rule
${\doteq}_L$, it suffices to show that for all 
$\Psi'' \vdash \sigma' : \Psi'$ that unify $\sigma t$ and $\sigma s$, 
there exists a derivation of 
$\foc{\Psi''}{\tackon{\sigma'(\sigma \Theta)}{\cdot}}{\sigma'(\sigma U)}$,
which is the same thing as a derivation of 
$\foc{\Psi''}{\tackon{(\sigma' \circ \sigma) \Theta}{\cdot}}
    {(\sigma' \circ \sigma) U}$. We have that 
$\Psi'' \vdash \sigma' \circ \sigma : \Psi$, and certainly 
$\sigma' \circ \sigma$ unifies $t$ andn $s$, so we can finish by
applying $\sigma' \circ \sigma$ to $\phi$.

$\sigma(\tforallr{x}{N}) = \tforallr{x}{(\sigma, x/x)N}$ -- We are
given a proof of $\foc{}{}{}$ that ends with $\forall_R$; the subderivation
is $N$, a derivation of $\foc{\Psi,x{:}\tau}{\Delta}{A^-}$. Because
$\sigma(\forall{x}{:}{\tau}.A^-) 
 = \forall{x}{:}{\sigma\tau}.(\sigma, x/x){A^-}$,
by rule $\forall_R$ it suffices to show 
$\foc{\Psi', x{:}\sigma\tau}{\sigma\Delta}{(\sigma, x/x)A^-}$, 
which is the same thing
as $\foc{\Psi', x{:}\sigma\tau}{(\sigma, x/x)\Delta}{(\sigma, x/x)A^-}$.
The result
follows by the induction hypothesis on $(\sigma, x/x)$ and $N$. 

$\sigma(\tforalll{t}{\Sp}) = \tforalll{\sigma{t}}{\sigma\Sp}$ -- 
We are given a proof of $\foc{\Psi}{\Delta}{U}$ that ends with 
$\forall_L$; the subderivation is $\Sp$, a derivation of 
$\foc{\Psi}{\tackon{\Theta}{[t/x]\Sp}}{U}$. Furthermore,
we know that $\Delta$ matches $\frameoff{\Theta}{[\forall x{:}\tau.A]}$.
We need a lemma that tells us that $\sigma\Delta$ matches
$\frameoff{\sigma\Theta}{[\forall x{:}\tau.(\sigma, x/x)A^-]}$; then,
by rule $\forall_L$, it suffices to show 
$\foc{\Psi'}{\tackon{\sigma\Theta}{[[t/x](\sigma, x/x)A^-]}}
  {\sigma{U}}$, which is the same thing as
$\foc{\Psi'}{\tackon{\sigma\Theta}{[\sigma([t/x]A^-)]}}
  {\sigma{U}}$. This follows by the induction hypothesis on $\sigma$ and
$\Sp$.
\end{proof}

Note that, in the case for $\forall_R$, the substitution $\sigma$ was 
applied to the first-order type $\tau$ as well as to the context
and succeedent $\Delta$ and $U$, hinting at the fact that our first-order
terms are dependently typed.

Given we write the constructive content of the variable substitution
theorem as $\sigma{E}$, were $E$,
we can also write Theorem~\ref{thm:varsubst} as an admissible
rule in one of two ways, both with and without proof terms:
\[
\infer-[{\it varsubst}]
{\foct{\Psi'}{\sigma{\Delta}}{\sigma{E}}{\sigma{U}}}
{\foct{\Psi}{\Delta}{E}{U}}
\qquad
\infer-[{\it varsubst}]
{\foc{\Psi'}{\sigma{\Delta}}{\sigma{U}}}
{\foc{\Psi}{\Delta}{U}}
\]
We will tend towards the expression-annotated presentations, such as
the one on the left, in this chapter.

\subsection{Focal substition}

Both cut admissibility and identity expansion depend on the same
focal substition theorem that was considered for linear logic in 
Section~\ref{sec:lin-suspended}. Both of these theorems use the
compound matching construct $\frameoff{\Theta}{\restrictto{\Delta}{\mlvl}}$,
a pattern that will also be used in the proof of cut admissibility: 
$\Delta'$ matches $\frameoff{\Theta}{\restrictto{\Delta}{\mlvl}}$
if $\Delta'$ matches $\frameoff{\Theta}{\Delta''}$
and $\Delta''$ matches $\restrictto{\Delta}{\mlvl}$.

Pay attention to the way this compound matching construct is being
used. In unfocused linear logic, using the compound notation
effectively means that, for unfocused linear logic, three cut
principles: \smallskip
\begin{itemize}
\item If $\oseq{\Gamma}{\cdot}{\cdot}{A}$ 
      and $\oseq{\Gamma, A}{\Delta'}{\Omega'}{C}$,
      then $\oseq{\Gamma}{\Delta'}{\Omega'}{C}$.
\item If $\oseq{\Gamma}{\Delta}{\cdot}{A}$ 
      and $\oseq{\Gamma}{\Delta', A}{\Omega'}{C}$,
      then $\oseq{\Gamma}{\Delta', \Delta}{\Omega'}{C}$.
\item If $\oseq{\Gamma}{\Delta}{\Omega}{A}$ 
      and $\oseq{\Gamma}{\Delta'}{\Omega_L, A, \Omega_R}{C}$,
      then $\oseq{\Gamma}{\Delta',\Delta}{\Omega_L, \Omega, \Omega_R}{C}$.
\end{itemize}
\smallskip
can be captured by the single theorem statement that,
if $\Delta \Rightarrow \istrue{A}$, 
$\tackon{\Theta}{x{:}\islvl{A}} \Rightarrow \istrue{C}$,
and $\Delta'$ matches $\frameoff{\Theta}{\restrictto{\Delta}{\mlvl}}$,
then $\Delta' \Rightarrow C$. As an admissible rule, we would write this
as follows:
\[
\infer-[{\it unfocused\mbox{-}cut}]
{\frameoff{\Theta}{\restrictto{\Delta}{\mlvl}} \Rightarrow \istrue{C}}
{\Delta \Rightarrow \istrue{A}
 &
 \tackon{\Theta}{x{:}\islvl{A}} \Rightarrow \istrue{C}}
\]

In the negative focal substition (as the leftist substitutions of
cut admissibility), there will be a corresponding use of
$\restrictfrom{U}{\mlvl}$ to capture that we can use a proof of
$\istrue{A}$ to discharge a hypothesis of $\istrue{A}$ in a proof of
$\istrue{C}$ or a proof of $\islax{C}$, but that a proof of
$\islax{A}$ can only discharge a hypothesis of $\istrue{A}$ in a proof
of $\islax{C}$. 

\bigskip
\begin{theorem}[Focal substitution]~
\begin{itemize}
\item If $\foc{\Psi}{\Delta}{[A^+]}$,
      $\foc{\Psi}{\tackon{\Theta'}{z{:}\islvl{\susp{A^+}}}}{U}$,
      and $\Delta'$ matches $\frameoff{\Theta'}{\restrictto{\Delta}{\mlvl}}$, 
      then $\foc{\Psi}{\Delta'}{U}$
\end{itemize}
\end{theorem}

\begin{proof}
Both proofs proceed by induction over the derivation $E$ containing
the suspended proposition; this is the second derivation in positive 
focal substitution and the first derivation in negative focal substitution. 
\end{proof}

The computational content of positive focal substitution is the
substitution of a value $V$ for a variable $z$ in an expression $E$, 
written $[V/z]E$. The computational
content of negative focal substitution is the substitution of a spine $\Sp$,
which represents
representing a continuation, into an expression $E$ waiting on that 
continuation, written $[E]\Sp$. As admissible rules, focal substitution is
represented as follows:
\[
\infer-[{\it subst}^+]
{\foct{\Psi}{\frameoff{\Theta'}{\restrictto{\Delta}{\mlvl}}}{[V/z]E}{U}}
{\foct{\Psi}{\Delta}{V}{[A^+]}
 &
 \foct{\Psi}{\tackon{\Theta'}{z{:}\islvl{\susp{A^+}}}}{E}{U}}
\]
\[
\infer-[{\it subst}^-]
{\foct{\Psi}{\frameoff{\Theta'}{\Delta}}{[ E ] \Sp}{\restrictfrom{U}{\mlvl}}}
{\foct{\Psi}{\Delta}{E}{\islvl{\langle A^- \rangle}}
 &
 \foct{\Psi}{\tackon{\Theta'}{[A^-]}}{\Sp}{U}}
\]

\section{Cut admissibility}
\label{sec:ord-cut}

Our compact statement of cut admissibility has four parts. Between
them, they capture the cases of Pfenning's structural cut admissiblity
proof without repretition. 

\bigskip
\begin{theorem}[Cut admissibility]~
\begin{enumerate}
\item If $\foc{\Psi}{\Delta}{[ A^+ ]}$, ~
         $\foc{\Psi}{\tackon{\Theta'}{A^+}}{U}$, ~ and
         $\Xi$ matches $\frameoff{\Theta'}{{\Delta}}$,\\
      then $\foc{\Psi}{\Xi}{U}$.
\item If $\foc{\Psi}{\Delta}{A^-}$, ~
         $\foc{\Psi}{\tackon{\Theta'}{[A^-]}}{U}$, ~
         $\Delta$ is stable, ~ and
         $\Xi$ matches $\frameoff{\Theta'}{\Delta}$,\\
      then $\foc{\Psi}{\Xi}{U}$.
\item If $\foc{\Psi}{\Delta}{\islvl{A^+}}$, ~
         $\foc{\Psi}{\tackon{\Theta'}{A^+}}{\restrictto{U}{\mlvl}}$, ~
         $\Theta'$ and $U$ are stable, ~ and
         $\Xi$ matches $\frameoff{\Theta'}{\Delta}$,\\
      then $\foc{\Psi}{\Xi}{\restrictto{U}{\mlvl}}$.
\item If $\foc{\Psi}{\restrictto{\Delta}{\mlvl}}{\istrue{A^-}}$, ~
         $\rfoc{\Psi}{\tackon{\Theta'}{x{:}\islvl{A^-}}}{C^+}$, ~
         $\Delta$ is stable, ~ and
         $\Xi$ matches $\frameoff{\Theta'}{\restrictto{\Delta}{\mlvl}}$,
      then $\rfoc{\Psi}{\Xi}{C^+}$
\end{enumerate}
\end{theorem}

\begin{proof}~

\subsection{Positive principal substitution}
Positive principal substitution encompasses all the {\it principal cuts}
from Pfenning's structural cut admissibility proof for which the principal
formula is positive. The constructive content of this part is a function
$(\subst{V}{N})^{A^+})$ that normalizes a value against a term. Induction
is on the structure of the positive type. The admissible rule associated with
principal positive substitution is ${\it cut}^+$. 
\[
\infer-[{\it cut}^+]
{\foct{\Psi}{\frameoff{\Theta'}{\Delta}}{(\subst{V}{N})^{A^+}}{U}}
{\foct{\Psi}{\Delta}{V}{[A^+]}
 &
 \foct{\Psi}{\tackon{\Theta'}{A^+}}{N}{U}
 &
 \stableL{\Theta'}
 &
 \stableR{U}}
\]
We have to be careful, in the positive principal substitution
associated with the type $A^+ \fuse B^+$, to maintain violate the
invariant that, in an unstable context, we only ever tack on the {\it
  leftmost} positive proposition.

\bigskip

$(\subst{z}{\tetap{z'}{N}})^{p^+_{\mlvl}} = [z/z']N$ ---
We have a derivation $\foc{\Psi}{\Delta}{[p^+_\mlvl]}$ using
the rule ${\it id}^+$

$(\subst{z}{\tetap{z'}{N}})^{p^+_\meph}$

$(\subst{z}{\tetap{z'}{N}})^{p^+_\mpers}$

$(\subst{}{})^{{\downarrow}A^-}$

$(\subst{}{})^{{\gnab}A^-}$

$(\subst{}{})^{{!}A^-}$

$(\subst{}{})^{\one}$

$(\subst{}{})^{A^+ \fuse B^+}$

$(\subst{}{})^{\zero}$

$(\subst{}{})^{A^+ \oplus B^+}$

$(\subst{}{})^{\exists x.A^+}$

$(\subst{}{})^{t \doteq s}$

\subsection{Negative principal substitution}
Negative principal substitution encompass all the {\it principal cuts}
from Pfenning's structural cut admissibility proof for which the
principal formula is negative. The constructive content of this part
is a function $(\subst{N}{\Sp})^{A^-}$ that normalizes a term against
a spine; a similar function appears in presentations of hereditary
substution for LF \cite{watkins02concurrent}. Induction is on the
structure of the negative type. The admissible rule associated with
negative principal substitution is ${\it cut}^-$:
\[
\infer-[{\it cut}^-]
{\foct{\Psi}{\frameoff{\Theta'}{\Delta}}{(\subst{N}{\Sp})^{A^-}}{U}}
{\foct{\Psi}{\Delta}{N}{A^-}
 &
 \foct{\Psi}{\tackon{\Theta'}{[A^-]}}{\Sp}{U}
 &
 \stableL{\Delta}}
\]

$(\subst{}{})^{p^-}$

$(\subst{}{})^{p^-_\mlax}$

$(\subst{}{})^{{\uparrow}A^+}$

$(\subst{}{})^{{\ocircle}A^+}$

$(\subst{}{})^{A^+ \righti B^-}$

$(\subst{}{})^{A^+ \lefti B^-}$

$(\subst{}{})^{\top}$

$(\subst{}{})^{A^- \with B^-}$

$(\subst{}{})^{\forall x.A^-}$

\subsection{Leftist substitution}
In focal substition, the positive case
corresponds to our usual intuitions about cut admissibility and the
negative case is strange.  In cut admissibility, the situation is
reversed: rightist substitutions (considered in
Section~\ref{sec:rsubst} below), associated with negative principal
cut formals, look like normal substitutions, and the leftist 
substitutions, considered here, are strange, as they break
apart the expression that proves $A^+$ rather than the term
we are substiting into.

Leftist substitutions encompass all the {\it left commutative cuts}
from Pfenning's structural cut admissibility proof.
The constructive content of leftist substitution is a function
$\lsubst{E}{N}$. Induction is on the first subterm, as we crawl 
through $E$ looking for places where focus takes place on the 
right. The admissible rule associated with leftist substition is
${\it lcut}$:
\[
\infer-[{\it lcut}]
{\foct{\Psi}{\frameoff{\Theta'}{\Delta}}{\lsubsta{E}{N}{A^+}}{\restrictfrom{U}{\mlvl}}}
{\foct{\Psi}{\Delta}{E}{\islvl{A^+}}
 &
 \foct{\Psi}{\tackon{\Theta'}{A^+}}{N}{U}
 &
 \stableL{\Theta'}
 &
 \stableR{U}}
\]

\subsection{Right commutative cuts}\label{sec:rsubst}
Rightist substitutions encompass all the {\it right commutative cuts}
from Pfenning's structural cut admissibility proof.  The constructive
content of this part is a function $\rsubsta{N}{x}{E}{A^-}$. Induction
is on the second subterm, as we crawl through $E$ looking for places
where $x$ is mentioned while focusing on the left.
The admissible rule associated with rightist substition is
${\it rcut}$:
\[
\infer-[{\it rcut}]
{\foct{\Psi}{\frameoff{\Theta'}{\restrictto{\Delta}{\mlvl}}}{\rsubsta{N}{x}{E}{A^-}}{U}}
{\foct{\Psi}{\Delta}{N}{A^-}
 &
 \foct{\Psi}{\tackon{\Theta'}{x{:}\islvl{A^-}}}{E}{U}
 & 
 \stableL{\Delta}}
\]

\end{proof}

\bigskip
\bigskip
\bigskip

\begin{theorem}[Cut admissibility]~
\begin{enumerate}
\item If $\foc{\Psi}{\Delta}{[ A^+ ]}$
      and $\foc{\Psi}{\tackon{\Theta'}{A^+}}{U}$, 
      then $\foc{\Psi}{\frameoff{\Theta'}{{\Delta}}}{U}$.
\item If $\foc{\Psi}{\Delta}{A^-}$ 
      and $\lfoc{\Psi}{\Theta'}{A^-}{U}$, 
      then $\foc{\Psi}{\frameoff{\Theta'}{\Delta}}{U}$.
\item[3a.] 
      If $\foc{\Psi}{\Delta}{\islvl{A^+}}$
      and $\foc{\Psi}{\tackon{\Theta'}{A^+}}{\restrictto{U}{\mlvl}}$
      then $\foc{\Psi}{\frameoff{\Theta'}{\Delta}}{\restrictto{U}{\mlvl}}$.
\item[3b.] 
      If $\foc{\Psi}{\tackon{\Theta}{[A^-]}}{\islvl{A^+}}$
      and $\foc{\Psi}{\tackon{\Theta'}{A^+}}{\restrictto{U}{\mlvl}}$
      then $\foc{\Psi}{\frameoff{\Theta'}{\frameoff{\Theta}{[A^-]}}}{\restrictto{U}{\mlvl}}$.
\item[4a.]
      If $\foc{\Psi}{\restrictto{\Delta}{\mlvl}}{\istrue{A^-}}$
      and $\rfoc{\Psi}{\tackon{\Theta'}{x{:}\islvl{A^-}}}{C^+}$,
      then $\rfoc{\Psi}{\frameoff{\Theta'}{\restrictto{\Delta}{\mlvl}}}{C^+}$
\item[4b.]
      If $\foc{\Psi}{\restrictto{\Delta}{\mlvl}}{\istrue{A^-}}$
      and $\foc{\Psi}{\tackon{\Theta'}{x{:}\islvl{A^-}}}{U}$
\item[4c.]
      If $\foc{\Psi}{\restrictto{\Delta}{\mlvl}}{\istrue{A^-}}$
      and $\foc{\Psi}{\tackon{\tackon{\Theta'}{x{:}\islvl{A^-}}}{[B^-]}}{U}$,
      then $\foc{\Psi}{\frameoff{\frameoff{\Theta'}{\restrictto{\Delta}{\mlvl}}}{[B^-]}}{U}$.
\end{enumerate}
\end{theorem}

\section{Identity expansion}
\label{sec:ord-identity}

\section{Correctness of focusing}
\label{sec:ord-correctness}

\subsection{Erasure}

\subsection{De-focalization}

\subsection{Unfocused admissibility}

\subsection{Focalization}


