\chapter{Substructural logic}

Linear logic is the most famous of the {\it substructural logics}.
Persistent logic admits the three so-called {\it structural rules} of
weakening (premises need not be used), contraction (premises may be
used multiple times) and exchange (the ordering of premises are 
irrelevant). Substructural logics, then, are logics that do not admit
these structural rules -- linear logic has only exchange, 
{\it affine} logic (which is frequently conflated with linear logic
by programming language designers) has exchange and weakening, and
{\it ordered} logic, first investigated as a proof theory by Lambek
\cite{lambek58mathematics}, lacks all three. 

Calling logics like linear, affine, and ordered logic
\underline{sub}structural relative to persistent logic (which is
structural) is arguably unfair to the substructural logics. Girard's
linear logic can express persistent provability using the exponential
connective ${!}A$, and this idea is generally applicable in
substructural logics -- for instance, it was applied by Polakow and
Pfenning to Lambek's ordered logic
\cite{polakow99natural}.\robnote{Check citation.} 

This section defines a first-order ordered linear logic that includes
a lax modality; a fragment of this system will form the basis of our
logical framework in Chapter 4. The full focusing system for ordered
linear lax logic is presented in this chapter in keeping with
Andreoli's insistence that we avoid the confusion that results from
conflating ``defining a foundational paradigm or a [logic] programming
language (two objectives that should clearly be kept separate)''
\cite{andreoli01focussing}, a concern that is applicable for logical
frameworks as well as for logic programming
languages.\footnote{As Twelf, Lollimon, and Celf
  show, logical frameworks can frequently be given straightforward
  life as logic programming languages
  \cite{pfenning99system,lopez05monadic,schacknielsen08celf}.} I 
also believe that a full exposition of this logic is useful because
of the possibility that many other interesting and tractable logic
programming languages can be defined as fragments of this logic.

\section{Ordered linear lax logic}

Ordered linear logic was the subject of Polakow's thesis
\cite{polakow01ordered}, and the adaptation of Fairtlough and
Mendler's lax logic \cite{fairtlough95propositional} (as reconstructed
by Pfenning and Davies \cite{pfenning01judgmental}) to linear logic is
the basis of the CLF logical framework
\cite{watkins02concurrent}. Putting the pieces together 
is a straightforward proof-theoretic exercise. There are three contexts
relevant to the propositional presentation of ordered linear lax logic.
The persistent context $\Gamma$ and the linear context $\Delta$
are multisets as before (so we think of $\Delta_1, \Delta_2$ as being
equal to $\Delta_2, \Delta_1$, for instance). The ordered context
$\Omega$ is a sequence of propositions, as in Gentzen's original
presentation of sequent calculi, and {\it not} a multiset.
This means that the two ordered
contexts $\Omega_1, \Omega_2$ and $\Omega_2, \Omega_1$ are, in general,
not the same.

\input{figs/fig-ordered-prop}

There are two judgments in ordered linear lax logic (henceforth \ollll). 
The primary
judgment is $\oseq{\Gamma}{\Delta}{\Omega}{\istrue{A}}$, which says
that $A$ is an (ephemeral, ordered) resource derivable from the
persistent resources in $\Gamma$, the ephemeral resources in $\Delta$,
and the ephemeral, ordered resources in $\Omega$. There is also a
second judgment, $\oseq{\Gamma}{\Delta}{\Omega}{\islax{A}}$. The
judgment $\islax{A}$ is usually interpreted as truth under an
unspecified constraint; one defining characteristic of the lax
modality is that it if $\istrue{A}$ is derivable with 
some resources then $\islax{A}$ is derivable with the same resources.

Compare this to the relationship between persistent and lax truth in
linear logic, where the defining characteristic is that a persistent
resource (associated with judgments of the form 
$\ispers{A}$ in $\Gamma$) can
always satisfy the need for an ephemeral resource (associated with
judgments of the form $\iseph{A}$ in $\Delta$). In the previous
chapter, we first encoded this relationship as an explicit rule
${\it copy}$:
\[
\infer[{\it copy}]
{\seq{\Gamma, \ispers{A}}{\Delta}{\iseph{C}}}
{\seq{\Gamma, \ispers{A}}{\Delta, \iseph{A}}{\iseph{C}}}
\]
In Section~\ref{sec:linnote}, based on a discussion of synthetic
connectives under the atom optimization, we considered a revision
in which the ${\it copy}$ rule was admissible and 
left rules had conclusions that used the
construct $\altseq{\Gamma}{\Delta/A}{C}$,
which matches a sequent of the form $\altseq{\Gamma}{\Delta'}{C}$ 
if either $A \in \Gamma$ and $\Delta' = \Delta$ or if
$\Delta' = \Delta, A$. The $\oplus_L$ rule in this logic is as follows:
\[
\infer[{\oplus}_L]
{\altseq{\Gamma}{\Delta/A \oplus B}{\iseph{C}}}
{\altseq{\Gamma}{\Delta, \iseph{A}}{\iseph{C}}
 &
 \altseq{\Gamma}{\Delta, \iseph{B}}{\iseph{C}}}
\]

Lax truth can be considered along the same lines, accounting for the
fact that we're now using the much briefer $\istrue{A}$ instead of
$\iseph{A}$ to capture the judgment associated with ordered, ephemeral
resources.  To follow existing judgmental presentations of lax logic,
we would include a distinct rule ${\it lax}$ that derives lax truth
from regular truth.
\[
\infer[{\it lax}]
{\Gamma; \Delta; \Omega \longrightarrow \islax{A}}
{\Gamma; \Delta; \Omega \longrightarrow \istrue{A}}
\]
The alternative is to rewrite all the right rules with a construct
$\orseq{\Gamma}{\Delta}{\Omega}{A}$ that matches both 
sequents of the form $\otseq{\Gamma}{\Delta}{\Omega}{A}$
and sequents of the form $\oseq{\Gamma}{\Delta}{\Omega}{\islax{A}}$.
The use of this construct gives us right rules for 
$A \oplus B$ that look like this:
\[
\infer[{\oplus}_{R1}]
{\orseq{\Gamma}{\Delta}{\Omega}{A \oplus B}}
{\otseq{\Gamma}{\Delta}{\Omega}{A}}
\qquad
\infer[{\oplus}_{R2}]
{\orseq{\Gamma}{\Delta}{\Omega}{A \oplus B}}
{\otseq{\Gamma}{\Delta}{\Omega}{B}}
\]
The related notation on the left-hand side is the construct 
$\olseq{\Gamma}{\Delta}{\Omega_L}{A}{\Omega_R}$, which matches
the sequent $\oseq{\Gamma}{\Delta'}{\Omega'}{U}$ if
\begin{itemize}
\item $\Omega' = \Omega_L, A, \Omega_R$ and $\Delta' = \Delta$;
\item $\Omega' = \Omega_L, \Omega_R$ and $\Delta' = \Delta, A$;
\item $\Omega' = \Omega_L, \Omega_R$, $\Delta' = \Delta$, and $A \in \Gamma$.
\end{itemize}
This style of presentation, use in Figure~\ref{fig:ordered-prop},
excludes the ${\it copy}$ and ${\it place}$ rules from Polakow's 
presentation; both rules are, however, admissible.

\subsection{First-order logic}

The presentation in Figure~\ref{fig:ordered-prop} is propositional; by 
uniformly adding a first-order context $\Psi$ to all sequents, however,
it can be treated as first-order. We define 
quantification (existential and universal), as well as 
first-order equality, in Figure~\ref{fig:ordered-fo}.

\input{figs/fig-ordered-fo}

The equality judgment $t \doteq s$ deserves some attention. It is a
higher-order judgment, in the sense that it reflects over the
definition of simultaneous term substitutions $\Psi' \vdash \theta :
\Psi$ and over the equality judgment for first-order terms $t =
s$. This is a rule that, in general, will have countably many
premises; in the case of a trivially satisfiable equality problem like
$x \doteq x$ it will have one premise for each well-formed
substitution that substitutes a term of the appropriate type for
$x$. This exact style of presentation was used previously in 
\cite{simmons09weak}, but the approach is based on Schroeder-Heister's
treatment of definitional reflection \cite{schroeder93rules}.

There are two important special cases. First, an unsatisfiable 
equation on the left implies a contradiction, and the left rule
for equality is equivalent to one with no premises. For instance, this
means that
\[
\infer
{\olfseq{\Gamma}{\Delta}{\Omega_L}{0 \doteq 1}{\Omega_R}}
{}
\]
is derivable. The other important special case is when
$t$ and $s$ have a  {\it most general unifier} $\theta_{\it mgu}$, 
which just means that for all $\Psi' \vdash \theta : \Psi$ such that 
$\theta t = \theta s$,
it is the case that $\theta = \theta' \circ \theta_{\it mgu}$ for some 
$\theta'$.\footnote{Where $\circ$ is composition -- 
$(\theta' \circ \theta_{\it mgu})t = \theta'(\theta t)$.} In this
case, the left rule for equality is equivalent to the following
rule:
\[
\infer
{\olfseq{\Gamma}{\Delta}{\Omega_L}{t \doteq s}{\Omega_R}}
{{\it mgu}(t, s) = \Psi \vdash \theta : \Psi'
 &
 \ofirstseq{\Psi'}{\theta\Gamma}{\theta\Delta}{\theta\Omega_L, \theta\Omega_R}{\theta U}}
\]

We have not yet thoroughly specified the type and term structure of
first-order individuals; in the next chapter we clarify that these
types and terms will actually be types and terms of Canonical LF.

\section{Substructural contexts}
\label{sec:contexts}

First-ordered linear lax logic has a lot of contexts -- the persistent
context $\Gamma$ the linear context $\Delta$, and the ordered context
$\Omega$, not to mention the first-order context $\Psi$. In most rules
these contexts just hang around, obscuring the logic's presentation
and ensuring that the {\LaTeX} code of figures and displays remains
permanently unreadable. And there are yet more contexts we might want to 
add, such as the affine contexts present in the Celf implementation.

In this section, we will consider a more compact way of dealing with
the contexts which we think of as containing resources (persistent,
affine, linear, or ordered), though we will maintain the distinction
between resource contexts and first-order variable 
contexts $\Psi$.  The particular way we define substructural contexts
can be generalized substantially: it would be trivial to extend
this presentation to the affine modality or to the subexponentials
discussed by Nigam and Miller \cite{nigam09algorithmic}. Furthermore,
the notation is designed so that it might scale to more interesting
substructural logics, such as non-commutative linear logic (``rigid
logic'' \cite{simmons09linear}) or the logic of bunched implications
\cite{pym02semantics}, even though our specific implementation
of that notation may not.

We write unified substructural contexts as either $\Delta$ or $\Xi$,
preferring the latter when there is a chance of confusing them with
linear contexts $\Delta$. For the purposes of encoding \ollll, we can
see these contexts as sequences, defined by the grammar
\[
\Xi ::= \cdot 
  \mid \Xi, x{:}\ispers{T}
  \mid \Xi, x{:}\iseph{T}
  \mid \Xi, x{:}\istrue{T}
\]
where each of the {\em variables} $x$ are distinct, so that the context
also represents a finite map $\to$ $\rightarrow$ from variables $x$ to 
{\it judgments}
$\islvl{T}$, where $\mlvl$ is either $\mpers$, $\meph$, or $\mtrue$. 
The domain $T$ is arbitrary: when
discussing the unfocused logic given in Figure~\ref{fig:ordered-prop},
$T$ varies over unpolarized propositions $A$, but when discussing a
focused logic it varies over negative propositions $A^-$ and suspended
positive propositions $\langle A^+ \rangle$. 

By sorting out a substructural context into three subsequences of
persistent, linear, and ordered contexts, we can recover the
presentations of contexts for \ollll~given in
Figure~\ref{fig:ordered-prop}. We will use this to informally motivate
our notation, writing $\Xi = \Gamma; \Delta; \Omega$.


% The first step towards this understanding 
% has already been take in Figure~\ref{fig:linear-alt} and
% Figure~\ref{fig:ordered-prop}, which make it quite obvious that the
% context-{\it matching} notation that we perform in the conclusion of
% an inference rule may not be the same as the context-{\it extending}
% notation we use in the premise of that rule.


\subsection{Fundamental operations on contexts}

In this presentation, most operations on contexts have two forms: a
{\it matching construct} that is used in the conclusion of derivation
rules, and a {\it construction construct} that is used in the
premises.  We introduced this distinction when we emphasized the
matching construct $\Gamma; \Delta/A$ in Figure~\ref{fig:linear-alt}
and the related matching construct $\Gamma; \Delta;
\Omega_L/A/\Omega_R$ notation in Figure~\ref{fig:ordered-prop}, but we
will be much more explicit about it in the following discussion.

The first fundamental idea we consider is {\it singleton} contexts.
We construct a single-element context by writing $x{:}\islvl{T}$, The
corresponding matching construct on contexts is {\it sole membership},
$x{:}T \subseteq \Xi$.

\bigskip
\begin{definition}[Sole membership]
We say that $x{:}T$ 
is the sole member of ${\Xi}$, which we write as $x{:}T \subseteq \Xi$,
when
\begin{itemize}
\item $\Xi$ contains no linear judgments and contains exactly
one
ordered judgment $x{:}\istrue{T}$ (corresponding to the situation where
$\Xi = \Gamma; \cdot; T$), 
\item $\Xi$ contains no ordered judgments and contains exactly
one linear judgement $x{:}\iseph{T}$ (corresponding to the situation where
$\Xi = \Gamma; T; \cdot$), and 
\item $\Xi$ contains only persistent judgments, including
$x{:}\ispers{T}$ (corresponding to the situation where
$\Xi = \Gamma, T; \cdot; \cdot$). 
\end{itemize}
\end{definition}
\bigskip

Sole membership is related to the initial sequents and the 
matching construct $\Gamma; \cdot;/A/$ for contexts that was used
in Figure~\ref{fig:ordered-prop}.
We could have instead written the {\it init} rule
from that figure as follows:
\[
\infer[{\it init}]
{\Xi \Rightarrow /{p}/}
{x{:}p \subseteq \Xi}
\]

The second basic operation on contexts requires a new concept, {\it
  frames} $\Theta$. Intuitively, we can look frames as substructural
contexts for \ollll~as a series of persistent, linear, and ordered
contexts where the ordered context is missing a particular piece. We
can write this missing piece as a box: $\Gamma; \Delta; \Omega_L,
\Box, \Omega_R$. Alternatively we can think of a frame as a one-hole
context or Huet-style zipper \cite{huet97zipper} over the structure
of substructural contexts. We will actually think of them
as closest to linear substitution functions, like a Linear ML function
$(\mathsf{fn}~\Xi\Rightarrow \Xi_L, \Xi, \Xi_R)$, though we'll 
treat $\Box$ as a distinguished variable standing for the bound variable 
$\Xi$ (that is, representing the missing part of the context)
\cite{simmons09linear}.

The construction form associated with frames, $\tackon{\Theta}{\Xi}$,
is just a straightforward operation of filling in the hole; it
requires that the variables in $\Theta$ and $\Xi$ be distinct. If we
think of $\Theta$ informally as $\Gamma; \Delta; \Omega_L, \Box,
\Omega_R$, then this is {\it almost} like the operation of filling in
the hole, as $\tackon{\Theta}{x{:}\istrue{A}} = \Gamma; \Delta;
\Omega_L, A, \Omega_R$. The main difference is that we can also use
the operation to insert linear propositions
($\tackon{\Theta}{x{:}\iseph{A}} = \Gamma; \Delta, A; \Omega_L,
\Omega_R$) and persistent propositions
($\tackon{\Theta}{x{:}\ispers{A}} = \Gamma, A; \Delta; \Omega_L,
\Omega_R$).

The matching construction for frames is a bit more complicated,
Informally, if we treat linear contexts as multisets and say that $\Xi
= \Gamma; \Delta, \Delta'; \Omega_L, \Omega', \Omega_R$, then we can
say $\Xi = \frameoff{\Theta}{\Xi'}$ in the case that $\Theta = \Gamma;
\Delta; \Omega_L, \Box, \Omega_R$ and $\Xi' = \Gamma; \Delta';
\Omega'$. The sub-context $\Xi'$, then, has been framed off from
$\Xi$, its frame is $\Theta$. If we only had ordered judgments
$\istrue{T}$, then the framing-off matching construct
$\frameoff{\Theta}{\Xi'}$ would be essentially the same as the
construction form $\tackon{\Theta}{\Xi'}$. However, persistent and
linear judgments can be reordered in the process of matching, and
persistent judgments always end up in both the frame and the 
framed-off context. 

\bigskip
\begin{definition}[Framing off]
$\Xi = \frameoff{\Theta}{\Xi'}$ if the union of the variables in 
$\Theta$ and $\Xi'$ is exactly the variables in $\Xi$ and
\begin{itemize}
\item if $x{:}\ispers{T} \in \Xi$, then the same mapping appears in 
  $\Theta$ and $\Xi'$;
\item if $x{:}\iseph{T} \in \Xi$ or $x{:}\istrue{T} \in \Xi$, 
  then the same mapping appears in $\Theta$ or $\Xi'$ (but not both); and
\item if $x{:}\iseph{T} \in \Theta$, then either
  \begin{itemize}
  \item for all $y{:}\istrue{T'} \in \Xi'$, the mapping for $x$ appeared before
    the mapping for $y$ in $\Xi$, or
  \item for all $y{:}\istrue{T'} \in \Xi'$, the mapping for $x$ appeared after
    the mapping for $y$ in $\Xi$. 
  \end{itemize}
\end{itemize}
\end{definition}
\bigskip

An important derived matching construct is $\frameoff{\Theta}{x{:}T}$,
which matches $\Xi$ if $\Xi = \frameoff{\Theta}{\Xi'}$ and 
$x{:}T \subseteq \Xi'$.  This pattern is used in almost every
left rule from Figure~\ref{fig:ordered-prop}, for instance:
\[
\infer[]
{\frameoff{\Theta}{x{:}A \oplus B} \Rightarrow U}
{\tackon{\Theta}{y{:}\istrue{A}} \Rightarrow U
 &
 \tackon{\Theta}{z{:}\istrue{B}} \Rightarrow U}
\quad
\infer[]
{\frameoff{\Theta}{x{:}A \with B} \Rightarrow U}
{\tackon{\Theta}{y{:}\istrue{A}} \Rightarrow U}
\quad
\infer[]
{\frameoff{\Theta}{x{:}A \with B} \Rightarrow U}
{\tackon{\Theta}{y{:}\istrue{B}} \Rightarrow U}
\]

We can also use this notation to describe the one of the
cut principles for ordered linear lax logic. 
\[
\infer-[{\it cut}]
{\frameoff{\Theta}{\Xi} \Rightarrow \istrue{C}}
{\Xi \Rightarrow \istrue{A}
 &
 \tackon{\Theta}{x{:}A\,{\it true}} \Rightarrow \istrue{C}}
\]

\futurework{
As an aside: it need not always be the case that the same operation
used to describe 

The idea that the operators $\frameoff{\Theta}{\Xi}$
and $\tackon{\Theta}{\Xi}$ are sufficient to describe the 
cut principle is related to the display property, which fails
for some reasonable logics, such as Reed's queue logic
\cite{reed09queue}.}

\subsection{Multiplicative operations}

To describe the multiplicative connective of \ollll, including the critical
connective of implication, we need to have multiplicative operations on 
contexts. As a construction form, $\mkconj{\Xi_L}{\Xi_R}$ is just the
syntactic concatenation of two contexts with distinct variable domains, and
the unit $\mkunit$ is just the empty sequence. The matching constructs
are more complicated to define as usual, but the intuition is, again, 
uncomplicated: if 
$\Xi = \Gamma; \Delta, \Delta'; \Omega_L, \Omega_R$, where linear contexts
are multisets and ordered contexts are sequences, then 
$\Xi = \mkconj{\Xi_L}{\Xi_R}$ if $\Xi_L = \Gamma; \Delta; \Omega_L$ and
$\Xi_R = \Gamma; \Delta'; \Omega_R$.

\bigskip
\begin{definition}[Conjunction]
As a matching construct, $\Xi = \matchunit$ if $\Xi$ contains only
persistent variables.

As a matching construct, $\Xi = \matchconj{\Xi_L}{\Xi_R}$ if the union 
of the variables in $\Xi_L$ and $\Xi_R$ is exactly the variables in $\Xi$
and 
\begin{itemize}
\item if $x{:}\ispers{T} \in \Xi$, then the same mapping appears in $\Xi_L$
  and $\Xi_R$;
\item if $x{:}\iseph{T} \in \Xi$ or $x{:}\istrue{T} \in \Xi$, then the
  same mapping appears in $\Xi_L$ or $\Xi_R$ (but not both); and
\item if $x{:}\istrue{T} \in \Xi_L$ and $y{:}\istrue{T'} \in \Xi_R$, then
  the mapping for $x$ appeared before the mapping for $y$ in $\Xi$. 
\end{itemize}
\end{definition}
\bigskip

The constructs for multiplicative conjunction are put to obvious use
in the description of multiplicative conjunction, which is essentially
just the propositional internalization of context conjunction:
\[
\infer
{\matchconj{\Xi_L}{\Xi_R} \Rightarrow /A \fuse B/}
{\Xi_L \Rightarrow \istrue{A} & \Xi_R \Rightarrow \istrue{B}}
\quad
\infer
{\frameoff{\Theta}{x{:}A \fuse B} \Rightarrow U}
{\tackon{\Theta}{\mkconj{y{:}A}{z{:}B}} \Rightarrow U}
\quad
\infer
{\cdot \Rightarrow \one}
{}
\quad
\infer
{\frameoff{\Theta}{x{:}\one} \Rightarrow U}
{\tackon{\Theta}{\cdot} \Rightarrow U}
\]\[
\infer
{\Xi \Rightarrow / A \lefti B /}
{x{:}\istrue{A}, \Xi \Rightarrow \istrue{B}}
\quad
\infer
{\frameoff{\Theta}{\Xi_A, x{:}A \lefti B} \Rightarrow U}
{\Xi_A \Rightarrow \istrue{A} & \tackon{\Theta}{y{:}\istrue{B}} \Rightarrow U}
\]
Implication makes deeper use of context conjunction:
we can look at the compound matching construct
$\frameoff{\Theta}{\matchconj{\Xi_A}{x{:}A \lefti B}}$ in two ways.
One way of reading this notation 
is that $\Xi$ matches $\frameoff{\Theta}{\Xi'}$, $\Xi'$ matches
$\matchconj{\Xi_A}{\Xi''}$, and $x{:}A \subseteq \Xi''$. The other
way of reading the notation 
is that $\Xi$ matches $\frameoff{\Theta'}{x{:}A \lefti B}$, and
$\Theta'$ matches $\frameoff{\Theta}{\matchconj{\Xi_A}{\Box}}$, 
It is critical that these two ways of 

\bigskip
\begin{definition}[Matching into frames]
$\Theta$ matches $\frameoff{\Theta'}{\matchconj{\Xi_A}{\Box}}$ if, 
for all $\Xi'$ and $\Xi_A$, $\Xi'$ matches $\matchconj{\Xi_A}{\Xi_B}$
if and only if 

For all $\Xi$, $\Xi_A$, $\Xi_B$ and $\Theta$, 
\begin{itemize}
\item there exists $\Xi'$ such that $\Xi$ matches $\frameoff{\Theta}{\Xi}$
  and $\Xi$ matches $\matchconj{\Xi_A}{\Xi_B}$
  if and only if there exists $\Delta'$ such that $\Xi$ matches 
  $\frameoff{\Theta}{\Xi_A}$ and $\Theta$ matches 
  $\frameoff{\Theta'}{}$
\end{itemize}
\end{definition}


As a matching construct, $\Xi = \matchconj{\Xi_L}{\Xi_R}$ if every 
$x{:}\ispers{T}$ in $\Xi$ appears in both $\Xi_L$ and $\Xi_R$, every 
$x{:}\iseph{T}$ or $x{:}\istrue{T}$ 
in $\Xi$ appears in exactly one of $\Xi_L$ or $\Xi_R$, and
if furthermore every 
$x{:}\iseph{T}$ $\Xi_L$


As a construction form, 


\subsection{Exponential operations}

\[
\infer-[{\it cut}]
{\frameoff{\Theta}{\Xi} \Rightarrow \istrue{C}}
{\Xi{\downharpoonright}_{\it lvl} \Rightarrow \istrue{A}
 &
 \tackon{\Theta}{x{:}A\,{\it lvl}} \Rightarrow \istrue{C}}
\quad
\infer-[\{{\it cut}\}]
{\frameoff{\Theta}{\Xi} \Rightarrow \islax{C}}
{\Xi \Rightarrow \islax{A}
 &
 \tackon{\Theta}{x{:}\istrue{A}} \Rightarrow \istrue{C}}
\]


We need one fundamental . A context $\Xi$

The fundamental constructor of a context is 
$x{:}A\,{\it lvl}$



We now have a {\it lot} of 

We will treat {\it substructural contexts} $\Delta$ are maps from {\it
  variables} to {\it judgments}. There are two kinds of variables: 
those that map {\it extent}.




Two of the fundamental properties of $\Delta$ are 
$x{:}\langle A^+ \rangle_l \sqsubseteq \Delta$. In linear logic, this
is the statement that $x{:}A \in \Gamma$, whereas in 

\[\small
\begin{array}{|c|c|c|c|}
%
\begin{array}{c}
\makebox[1.8in]{\it Dual intuitionistic linear logic}\medskip
\\
\infer
{\Gamma; p^+ \vdash p^+ \mathstrut}
{\mathstrut}
\end{array}
%
&
%
\begin{array}{c}
\makebox[1.8in]{\it Pfenning-Davies S4}\medskip
\\
\infer
{\Delta; \Gamma, p^+ \mathstrut \vdash p^+}
{\mathstrut}
\end{array}
%
&
%
\begin{array}{c}
\makebox[1.8in]{\it Unified}\medskip
\\
\infer
{\Delta \vdash [ p^+ ]_l \mathstrut}
{x{:}\langle p^+ \rangle_l \subseteq \Delta \mathstrut}
\end{array}
%
\end{array}
\]

\[\small
\begin{array}{|c|c|c|c|}
%
\begin{array}{c}
\makebox[1.8in]{\it Dual intuitionistic linear logic}\medskip
\\
\infer
{\Gamma; \Delta, A \with B \vdash C \mathstrut}
{\Gamma; \Delta, A \vdash C \mathstrut}
\end{array}
%
&
%
\begin{array}{c}
\makebox[1.8in]{\it Pfenning-Davies S4}\medskip
\\
\infer
{\Delta; \Gamma, A \wedge^- B \mathstrut \vdash C}
{\Delta; \Gamma, A \wedge^- B, A \vdash C \mathstrut}
\end{array}
%
&
%
\begin{array}{c}
\makebox[1.8in]{\it Unified}\medskip
\\
\infer
{\Theta\{ x{:} A \with B \} \vdash U \mathstrut}
{\Theta\{ x_1{:} A \} \vdash U \mathstrut}
\end{array}
%
\end{array}
\]


\[\small
\begin{array}{|c|c|c|c|}
%
\begin{array}{c}
\makebox[1.8in]{\it Dual intuitionistic linear logic}\medskip
\\
\infer
{\Gamma; \Delta_1, \Delta_2 \vdash A \otimes B \mathstrut}
{\Gamma; \Delta_1 \vdash A & \Gamma; \Delta_2 \vdash B \mathstrut}
\end{array}
%
&
%
\begin{array}{c}
\makebox[1.8in]{\it Pfenning-Davies S4}\medskip
\\
\infer
{\Delta; \Gamma \vdash A \wedge^+ B \mathstrut}
{\Delta; \Gamma \vdash A & \Delta; \Gamma \vdash B \mathstrut}
\end{array}
%
&
%
\begin{array}{c}
\makebox[1.8in]{\it Unified}\medskip
\\
\infer
{\Delta_1 \bowtie \Delta_2 \vdash A \otimes B \mathstrut}
{\Delta \vdash A & \Delta_2 \vdash B \mathstrut}
\end{array}
%
\end{array}
\]



\section{Focused sequent calculus}

A sequent in the focusing calculus has the form
$\foc{\Psi}{\Delta}{U}$, where $\Psi$ is the first-order variable
context, $\Delta$ is a substructural context described in the previous
section, and $U$ is a succeedent. The domain $T$ of the substructural
context consists of stable negative propositions $A^-$, positive
suspended propositions $\susp{A^+}$, focused negative proposition
$[A^-]$, and inverting positive propositions $A^+$.

The form of the succedent $U$ is $\islvl{T}$, where $\mlvl$ is either
$\mtrue$ or $\mlax$; in this way, $U$ is just a like a special
substructural context with exactly one element -- we don't need
to care about the name of the variable, because there's only one.  The
domain of $T$ for succedents is complementarly to the domain of $T$
for contexts: stable positive propositions $A^+$, negative suspended
propositions $\susp{A^-}$, focused positive propositions $[A^+]$, and
inverting negative propositions $A^-$.

\subsection{Restrictions on the form of sequents}

A sequent $\foc{\Psi}{\Gamma}{U}$ is {\it stable} when the context
$\Delta$ and succeedent $U$ contain only stable propositions ($A^-$ in
the context, $A^+$ in the succedent) and suspended propositions
($\susp{A^+}$ in the context, $\susp{A^-}$ in the succeedent). We
repeat the focusing constraint discussed in Chapter 2: there is only
ever at most one focused proposition in a sequent, and if there is
focused proposition in the sequent, then the sequent is otherwise
stable. A restriction on the rules ${\it focus}_L$ and ${\it focus}_R$
is sufficient to enforce this restriction: reading rules from
top-down, we can only use a rule ${\it focus}_L$ or ${\it focus}_R$ to
prove a stable sequent, and reading rules from bottom-up, we can only
apply ${\it focus}_L$ or ${\it focus}_R$ when we are searching for a
proof of a stable sequent.

Because there is always a distinct focused proposition in a sequent,
we do not need a variable name to reference the focused proposition in
a context $\Delta$ any more than we need a variable name to reference
the unique member of the context-like succeedent $U$. Therefore, we
write $\istrue{[B^-]}$ instead of $x{:}\istrue{[B^-]}$. Furthermore,
we restrict focused propositions and inverting propositions so that
they are always associated with the judgment $\mtrue$. With this
restriction, we can write $[A^-]$ and $x{:}A^+$ instead of
$\istrue{[A^-]}$ and $x{:}\istrue{A^+}$ in $\Delta$, and we can write
$[A^+]$ and $A^-$ instead of $\istrue{[A^+]}$ and $\istrue{A^-}$ for
$U$.

In a confluent presentation of focused logic like the one given for
linear logic in Chapter 2, that would be as far as we could take our
simplifications. However, this presentation will match the fixed
presentation of logic from the structural focalization development
\cite{simmons11structural}. Specifically, if there is more than one
invertible proposition in a sequent, {\it only} the leftmost one will
be treated as eligible to have a rule applied to it. All the
propositions in $\Delta$ are treated as being to the left of the
succeedent $U$, so we always prioritize inversion on positive
propositions in $\Delta$. With this additional restriction, it is
always unambiguous which proposition we are referring to in an
invertible rule, and we write $A^+$ instead of $x{:}A^+$ or
$x{:}\istrue{A^+}$.

\subsection{Polarized propositions}

The propositions of ordered logic are fundamentally sorted into
positive propositions $A^+$ and negative propositions $A^-$
as shown in Figure~\ref{fig:ordered}. The
positive propositions have a refinement, {\it permeable} propositions
$A^+_\mpers$, that is analgaous to the refinement discussed for linear
logic in Section~\ref{sec:permeable}. There is also a more generous
refinement, the {\it mobile} propositions, $A^+_\meph$, for positive
propositions that bottom out with one of the two modalities ${!}$ and
${\gnab}$. We introduce atomic propositions $p^+$ that stand for
arbitrary positive propositions, $p^+_\meph$ that stand for arbitrary
mobile propositions, and $p^+_\mpers$ that stand for arbitrary
permeable propositions.

\begin{figure}
\begin{align*}
A^+ & ::= p^+ \mid p^+_\meph \mid p^+_\mpers
        \mid {\downarrow}A^- \mid {\gnab}A^- \mid {!}A^- 
        \mid \one \mid A^+ \fuse B^+ \mid \zero \mid A^+ \oplus B^+ 
        \mid \exists x. A^+ \mid t \doteq s
\\
A^+_\meph & ::= p^+_\meph \mid p^+_\mpers
        \mid {\gnab}A^- \mid {!}A^- 
        \mid \one \mid A^+_\meph \fuse B^+_\meph
        \mid \zero \mid A^+_\meph \oplus B^+_\meph
        \mid \exists x. A^+_\meph \mid t \doteq s
\\
A^+_\mpers & ::= p^+_\mpers 
        \mid {!}A^- 
        \mid \one \mid A^+_\mpers \fuse B^+_\mpers 
        \mid \zero \mid A^+_\mpers \oplus B^+_\mpers
        \mid \exists x. A^+_\mpers \mid t \doteq s
\\
A^- & ::= p^- \mid p^-_\mlax 
        \mid {\uparrow}A^+ \mid {\ocircle}A^+
        \mid A^+ \righti B^- \mid A^+ \lefti B^-
        \mid \top \mid A^- \with B^-
        \mid \forall x.A^-
\\
A^-_\mlax & ::= p^-_\mlax 
        \mid {\ocircle}A^+
        \mid A^+ \righti B^-_\mlax \mid A^+ \lefti B^-_\mlax
        \mid \top \mid A^-_\mlax \with B^-_\mlax
        \mid \forall x.A^-_\mlax
\end{align*}
\caption{Propositions of focused \ollll.}
\label{fig:ordered}
\end{figure}

Negative propositions also have a refinement, $A^-_\mlax$, for
negative propositions that always bottom out as a modal
${\ocircle}A^+$.  This is interesting as a formal artifact and there
is very little overhead involved in putting it into our development,
but I don't claim to understand what the refinement means or what the
inclusion of right-permeable atomic propositions $p^-_\mlax$ mean for
the structure of proofs.

The presentation of the modalities emphasises the degree to which
the shifts ${\uparrow}$ and ${\downarrow}$ have much of the character
of modalities in a focused substructural logic; this point was
explored in depth by Laurent \cite{laurent02etude}.

\subsection{Derivations and proof terms}
\label{sec:ord-proof-terms}

\input{figs/fig-foc-mall}
\input{figs/fig-foc-add}
\input{figs/fig-foc-fo}

The multiplicative and exponential fragment of focused \ollll~is given
in Figure~\ref{fig:foc-mall}, the additive fragment is given in
Figure~\ref{fig:foc-add}, and the first-order connectives are treated
in Figure~\ref{fig:foc-fo}. These rules can be understood as a regular
logic by ignoring the {\it proof terms} that appear to immedately to
the right of an arrow.

The sequent form that includes proof terms is written as
$\foct{\Psi}{\Delta}{E}{U}$, where $E$ is an {\it expression}.  Just
as sequent forms are divided into the right-focused, inverting, and
left-focused sequents, we divide expressions into {\it values} $V$,
which are associated with right-focused sequents; {\it terms} $N$,
which are associated with inverting sequents; and {\it spines} $\Sp$,
which are associated with left-focused sequents. The structure of
values, terms, and spines is as follows:
\begin{align*}
V & ::= z                     %% z
   \mid \tbangr{N}            %% !N
   \mid \tgnabr{N}            %% $N
   \mid \tdownr{N}            %% N
%
   \mid \toner
   \mid \tfuser{V_1}{V_2}     %% V1 * V2
%  
   \mid \tinl{V}
   \mid \tinr{V}
   \mid \texistsr{t}{V}
   \mid \tunifr
 \\
N & ::= \tfocusr{V}           %% V
   \mid \tfocusl{x}{\Sp}      %% x @ Sp
   \mid \tetap{z}{N}          %% <z>.N
   \mid \tbangl{x}{N}         %% !x.N
   \mid \tgnabl{x}{N}         %% $x.N
   \mid \tdownl{x}{N}         %% x.N
   \mid \tupr{N}              %% N
   \mid \tlaxr{N}             %% {N}
\\ & ~~~~ %
   \mid \tetan{N} 
   \mid \tupr{N}
   \mid \tfusel{N}            %% *N
   \mid \tlamr{N}             %% >N
   \mid \tlaml{N}             %% <N
%
   \mid \toplusl{N_1}{N_2}    %% [N1,N2]
   \mid \ttopr 
   \mid \twithr{N_1}{N_2}     %% N1 & N2
   \mid \texistsl{a}{N}
   \mid \tforallr{a}{N}
   \mid \phi
\\
S & ::= \tnil                 %% nil
   \mid \tupl{N}              %% N
   \mid \tlaxl{N}             %% {N} 
   \mid \tappr{V}{\Sp}        %% V > Sp
   \mid \tappl{V}{\Sp}        %% V < Sp
   \mid \tpione{\Sp}          %% fst Sp
   \mid \tpitwo{\Sp}          %% snd Sp
   \mid \tforalll{t}{\Sp}
\end{align*}

Expressions are treated as in the structural 
focalization methodlogy \cite{simmons11structural}. It is possible
to take a view of expressions as {\it extrinsically} typed, which
means we consider both well-typed and ill-typed expressions, and 
the well-typed expressions are those for which the sequent 
$\foct{\Psi}{\Delta}{E}{U}$ is derivable. However, we will take
the view that expressions are intrinsically typed representatives 
of derivations: that is, instead of talking about derivations 
$\mathcal D$ of the sequent $\foc{\Psi}{\Delta}{[A^+]}$, we will just talk
about values $V$ focused on $A^+$ using $\Delta$. The judgment
$\foct{\Phi}{\Delta}{V}{[A^+]}$ is a convienent way of writing this 
down, but values, terms, and spines are themselves representatives
of derivations. 

There are two caveats to the idea that expressions are representatives
of derivations. One caveat is that, in order for there to be an actual
correspondance between expressions and terms, we need to annotate all
variables with the judgment they are associated with, and we need to
annotate $\tinr{V}$, $\tinl{V}$, $\tpione{\Sp}$, and $\tpitwo{\Sp}$
terms with the type of the branch not taken. Pfenning writes these as
superscripts \cite{pfenning08church}, but which we will follow Girard
in leaving them implicit \cite{girard89proofs}. The second caveat is
that if $\foct{\Psi}{\Delta}{V}{U}$, then $\foct{\Psi}{\Delta,
  x{:}\ispers{A^+}}{V}{U}$ as well. In other words, expressions are
only representaives of derivations modulo weakening.

As before, ${\doteq}_L$ requires some more explanation. 
The proof term for this rule is just a function $\phi$ from unifying
substitutions $\sigma$ to terms $N$ that correspond to proofs of 

\bigskip
\begin{theorem}
If $\Psi' \vdash \sigma : \Psi$ and $\foc{\Psi}{\Delta}{U}$, then 
$\foc{\Psi'}{\sigma\Delta}{\sigma{U}}$.
\end{theorem}

\begin{proof}
On the level of proof terms, 
we are given $E$, a expression corresponding to a derivation of
$\foc{\Psi}{\Delta}{U}$; we are defining the operation $\sigma{E}$,
an expression corresponding to a derivation of 
$\foc{\Psi'}{\sigma\Delta}{\sigma{U}}$.

For the exponential, multiplicative, and additive fragments, this
operation is simple to define at the level of proof terms:
$\sigma(\tfuser{V_1}{V_2}) = \tfuser{\sigma{V_1}}{\sigma{V_2}}$,
$\sigma(\tdownl{x}{N}) = \tdownl{x}{\sigma N}$, and so on. However,
this compact notation does capture a great deal of complexity. In
particular, while it is basically self-evident that it is important to emphasize that we need lemmas saying
that variable substitution is compatible with all the non-trivial
context matching operations from Section~\ref{sec:contexts}.  In full
detail, these two simple cases would be:

$\sigma(\tfuser{V_1}{V_2}) = \tfuser{\sigma{V_1}}{\sigma{V_2}}$ --
We are given a proof of $\foc{\Psi}{\Delta}{[A^+ \fuse B^+]}$ that
ends with the ${\fuse}_R$ rule; the subderivations are
$V_1$, a derivation of $\foc{\Psi}{\Delta_1}{[A^+]}$, and
$V_2$, a derivation of $\foc{\Psi}{\Delta}{[B^+]}$. Furthermore, we know that
$\Delta$ matches $\Delta_1, \Delta_2$. We need a lemma that
tells us that $\sigma\Delta$ matches $\sigma\Delta_1, \sigma\Delta_2$;
then, by rule ${\fuse}_R$, it suffices to show that
$\foc{\Psi'}{\sigma\Delta_1}{\sigma{A^+}}$ (which we have by the 
induction hypothesis on $\sigma$ and $V_1$) and that
$\foc{\Psi'}{\sigma\Delta_2}{B^+}$ (which we have by the induction hypothesis
on $\sigma$ and $V_2$). 

$\sigma(\tdownl{x}{N}) = \tdownl{x}{\sigma N}$ -- We are given a proof
of $\foc{\Psi}{\Delta}{U}$ that ends with ${\downarrow}_L$; 
the subderivation is $N$, a derivation of
$\foc{\Psi}{\tackon{\Theta}{x{:}\istrue{A^-}}}{U}$. Furthermore, we know that
$\Delta$ matches $\frameoff{\Theta}{{\downarrow}A^-}$. We need a lemma
that tells us that $\sigma\Delta$ matches
$\frameoff{\sigma\Theta}{{\downarrow}\sigma A^-}$; then, by 
rule ${\downarrow}_L$, it suffices to show 
$\foc{\Psi'}{\tackon{\sigma\Theta}{x{:}\istrue{\sigma{A^-}}}}{U}$ (which
we have by the induction hypothesis on $\sigma$ and $N$).

 By the induction hypothesis on
$V_1$, we have a derivation of $\foc{\Psi'}{\sigma\Delta_1}{A^+}$, and
by the induction hypothesis on $V_2$ we have a derivation of
$\foc{\Psi'}{\sigma\Delta_2}{B^+}$; therefore, because by lemma
$\sigma\Delta$ matches 




 a derivation
of $\foc{\Psi}{\Delta}{U}$
\[
\infer-[{\it varsubst}]
{\foct{\Phi'}{\sigma\Delta}{\sigma{E}}{\sigma{U}}}
{\foct{\Phi}{\Delta}{E}{U}}
\]
All the propositional cases are totally straightforward:
\[
\infer-[{\it varsubst}]
{\foct{\Phi}{\Delta_1, \Delta_2}{\sigma(\tfuser{V_1}{V_2})}{[A^+ \fuse B^+]}}
{}
\]
\end{proof}


The use of proof terms allows for a greatly compressed discussion of
cut admissibility and identity expansion. Furthermore, we will reuse
much of this notation in our formulation of a logical framework in the
next chapter. 

\section{Cut admissibility}

\begin{itemize}
\item $\Delta{\downarrow}_{\mpers}$ -- identity function whenever $\Delta$ contains only $x{:}\ispers{T}$
\item $\Delta{\downarrow}_{\meph}$ -- identity function whenever $\Delta$ contains only $x{:}\ispers{T}$ and $x{:}\iseph{T}$
\item $\Delta{\downarrow}_{\mtrue}$ -- identity function
\end{itemize}

\begin{itemize}
\item $U{\downarrow}_{\mtrue}$ -- identity function whenever $U$ is $\istrue{A}$
\item $U{\downarrow}_{\mlax}$ -- identity function
\end{itemize}






\begin{theorem}[Cut admissibility]~
\begin{enumerate}
\item If $\foc{\Psi}{\Delta}{[ A^+ ]}$, ~
         $\foc{\Psi}{\tackon{\Theta'}{A^+}}{U}$, ~ and
         $\Xi$ matches $\frameoff{\Theta'}{{\Delta}}$,\\
      then $\foc{\Psi}{\Xi}{U}$.
\item If $\foc{\Psi}{\Delta}{A^-}$, ~
         $\foc{\Psi}{\tackon{\Theta'}{[A^-]}}{U}$, ~
         $\Delta$ is stable, ~ and
         $\Xi$ matches $\frameoff{\Theta'}{\Delta}$,\\
      then $\foc{\Psi}{\Xi}{U}$.
\item If $\foc{\Psi}{\Delta}{\islvl{A^+}}$, ~
         $\foc{\Psi}{\tackon{\Theta'}{A^+}}{\restrictto{U}{\mlvl}}$, ~
         $\Theta'$ and $U$ are stable, ~ and
         $\Xi$ matches $\frameoff{\Theta'}{\Delta}$,\\
      then $\foc{\Psi}{\Xi}{\restrictto{U}{\mlvl}}$.
\item If $\foc{\Psi}{\restrictto{\Delta}{\mlvl}}{\istrue{A^-}}$, ~
         $\rfoc{\Psi}{\tackon{\Theta'}{x{:}\islvl{A^-}}}{C^+}$, ~
         $\Delta$ is stable, ~ and
         $\Xi$ matches $\frameoff{\Theta'}{\restrictto{\Delta}{\mlvl}}$,
      then $\rfoc{\Psi}{\Xi}{C^+}$
\end{enumerate}
\end{theorem}

\bigskip
\bigskip
\bigskip

\begin{theorem}[Cut admissibility]~
\begin{enumerate}
\item If $\foc{\Psi}{\Delta}{[ A^+ ]}$
      and $\foc{\Psi}{\tackon{\Theta'}{A^+}}{U}$, 
      then $\foc{\Psi}{\frameoff{\Theta'}{{\Delta}}}{U}$.
\item If $\foc{\Psi}{\Delta}{A^-}$ 
      and $\lfoc{\Psi}{\Theta'}{A^-}{U}$, 
      then $\foc{\Psi}{\frameoff{\Theta'}{\Delta}}{U}$.
\item[3a.] 
      If $\foc{\Psi}{\Delta}{\islvl{A^+}}$
      and $\foc{\Psi}{\tackon{\Theta'}{A^+}}{\restrictto{U}{\mlvl}}$
      then $\foc{\Psi}{\frameoff{\Theta'}{\Delta}}{\restrictto{U}{\mlvl}}$.
\item[3b.] 
      If $\foc{\Psi}{\tackon{\Theta}{[A^-]}}{\islvl{A^+}}$
      and $\foc{\Psi}{\tackon{\Theta'}{A^+}}{\restrictto{U}{\mlvl}}$
      then $\foc{\Psi}{\frameoff{\Theta'}{\frameoff{\Theta}{[A^-]}}}{\restrictto{U}{\mlvl}}$.
\item[4a.]
      If $\foc{\Psi}{\restrictto{\Delta}{\mlvl}}{\istrue{A^-}}$
      and $\rfoc{\Psi}{\tackon{\Theta'}{x{:}\islvl{A^-}}}{C^+}$,
      then $\rfoc{\Psi}{\frameoff{\Theta'}{\restrictto{\Delta}{\mlvl}}}{C^+}$
\item[4b.]
      If $\foc{\Psi}{\restrictto{\Delta}{\mlvl}}{\istrue{A^-}}$
      and $\foc{\Psi}{\tackon{\Theta'}{x{:}\islvl{A^-}}}{U}$
\item[4c.]
      If $\foc{\Psi}{\restrictto{\Delta}{\mlvl}}{\istrue{A^-}}$
      and $\foc{\Psi}{\tackon{\tackon{\Theta'}{x{:}\islvl{A^-}}}{[B^-]}}{U}$,
      then $\foc{\Psi}{\frameoff{\frameoff{\Theta'}{\restrictto{\Delta}{\mlvl}}}{[B^-]}}{U}$.
\end{enumerate}
\end{theorem}

\section{Identity expansion}

\section{Polarization and erasure}


\section{Unfocused admissibility}

\section{Soundness and completeness}


