\chapter{Ordered abstract machines}
\label{chapter-absmachine}

This chapter centers around two transformations on logical
specifications.  Taken together, the operationalization transformation
(Section~\ref{sec:operationalization}), and the defunctionalization
transformation (Section~\ref{sec:defunctionalization}) allow us to
establish the logical correspondence between the deductive SLS
specification of a natural semantics and the concurrent SLS
specification of an abstract machine.

Natural semantics specifications are very common in the literature,
and are also easy to encode in either the deductive fragment of
\sls~or in a purely deductive logical framework like LF.  We will
continue to use the natural semantics specification of call-by-value
evaluation for the lambda calculus as our running example:
\[
\infer[{\sf ev/lam}]
{\lambda x. e \Downarrow \lambda x. e \mathstrut}
{}
\quad
\infer[{\sf ev/app}]
{e_1\,e_2 \Downarrow v \mathstrut}
{e_1 \Downarrow \lambda x.e
 &
 e_2 \Downarrow v_2
 &
 [v_2/x]e \Downarrow v \mathstrut}
\]

Abstract machine semantics are less prevalent than natural
semantics. The most well-known is almost certainly Landin's SECD
machine \cite{landin64mechanical}, though the abstract machine is much
more similar to Danvy's SC machine from \cite{danvy03rational} and
Harper's $\mathcal K\{{\sf nat}{\rightharpoonup}\}$ system from
\cite[Chapter 27]{harper12practical}.  The abstract machine semantics
that we will show to be has two states $s$. The state $s = k \rhd e$
represents the expression $e$ being evaluated on top of the stack $k$,
and the state $s = k \lhd v$ represents the value $v$ being returned
to the stack $k$. Stacks $k$ are sequences of frames $f$
with the form $((\ldots({\sf halt}; f_1); \ldots); f_n)$, and each
frame $f$ either has the form $\Box\,e_2$ (an application frame
waiting for an evaluated function to be returned to it) or the form
$(\lambda x.e)\,\Box$ (an application frame with an evaluated function
waiting for an evaluated value to be returned to it). Given states,
stacks, and frames, we can define a ``classical'' abstract machine for
call-by-value evaluation of the lambda calculus as a transition system
with four transition rules:
\begin{align*}
{\sf absmachine/lam}{:} & ~~ k \rhd \lambda x.e ~ \mapsto ~ k \lhd \lambda x.e
\\
{\sf absmachine/app}{:} & ~~ k \rhd e_1\,e_2 ~ \mapsto ~ (k; \Box\,e_2) \rhd e_1
\\
{\sf absmachine/app1}{:} & ~~ 
  (k; \Box\,e_2) \lhd \lambda x.e ~ \mapsto ~ (k; (\lambda x.e)\,\Box) \rhd e_2
\\
{\sf absmachine/app2}{:} & ~~
  (k; (\lambda x.e)\,\Box) \lhd v_2 ~ \mapsto ~ k \rhd [v_2/x]e
\end{align*}

The operational intuition for these rules is precisely the same as the
operational intuition for the rewriting rules given in
Section~\ref{sec:intro-ssos}. This is not coincidental: the
\sls~specification from the introduction adequately encodes the
transition system $s \mapsto s'$ defined above, a point that we will
make precise in Section~\ref{sec:nat-ssos-adequacy}. The
\sls~specification from the introduction is {\it also} the result of
applying the operationalization and defunctionalization
transformations to the \sls~encoding of the natural semantics given
above, so the these two transformations combined with the adequacy
arguments at either end constitute a logical correspondence between
natural semantics and abstract machines. 

As discussed in Section~\ref{sec:the-point-is-modular-extension}, it
is interesting to put existing specification styles into logical
correspondence, but that is not our main reason for being interested
in the logical correspondence. Instead, we are primarily interested in
exploring the set of programming language features that can be
modularly integrated into a transformed \sls~specification that could
not be integrated into a natural semantics specification.  In
Section~\ref{sec:richer-ordered-abstract} we explore a selection of
these features, including mutable storage, call-by-need evaluation,
and recoverable failure.

\section{Logical transformation: operationalization}
\label{sec:operationalization}

The intuition behind operationalization is rather simple: we examine
the behavior of a deductive computation and then encode that
operational intuition as a concurrent computation.  Before presenting
the general transformation, we will motivate this transformation using
our natural semantics specification of call-by-value-evaluation. 

The definition of $e \Downarrow v$ is moded with $e$ as an input and
$v$ as an output, so it is meaningful to talk about being given a
particular expression $e$ and using deductive computation to search
for a $v$ such that $e \Downarrow v$ is derivable.  Consider a
recursive search procedure implementing this particular deductive
computation:
\begin{itemize}
\item
      If $e = \lambda x. e'$, 
      it is possible to derive 
      $\lambda x. e' \Downarrow \lambda x. e'$
      with the rule ${\sf ev/lam}$.
\item
       If $e = e_1\,e_2$,
       attempt to derive 
       $e_1\,e_2 \Downarrow v$
       using the rule ${\sf ev/app}$ by doing the following:
    \begin{enumerate}
    \item Search for a $v_1$ such that 
          $e_1 \Downarrow v_1$ is derivable.
    \item Assert that $v_1 = \lambda x.e'$ for some
          $e'$; fail if it is not.
    \item Search for a $v_2$ such that 
          $e_2 \Downarrow v_2$ is derivable.
    \item Search for a $v$ such that 
          $[v_2/x]e \Downarrow v$ is derivable.
    \end{enumerate}
% \item
%       If $e = e_1 \arb e_2$,
%       attempt to derive $e_1 \arb e_2 \Downarrow v$ using
%       the rule ${\sf ev/choose1}$ by searching for a 
%       $v$ such that $e_1 \Downarrow v$ is derivable.
% \item
%       If $e = e_1 \arb e_2$,
%       attempt to derive $e_1 \arb e_2 \Downarrow v$ using 
%       the rule ${\sf ev/choose2}$ by searching for a 
%       $v$ such that $e_2 \Downarrow v$ is derivable.  
\end{itemize}
%
The goal of the operationalization transformation is to implement this
deductive computation as a concurrent computation. The first step in
doing so is to introduce two new ordered atomic propositions.  The
proposition ${\sf eval}\,\interp{e}$ is the starting point, indicating
that we want to search for a $v$ such that $e \Downarrow v$, and the
proposition ${\sf retn}\,\interp{v}$ indicates the successful
completion of this procedure. Therefore, searching for a $v$ such that
$e \Downarrow v$ is derivable will be analogous to building a trace $T
:: x_e{:}\susp{{\sf eval}\,\interp{e}} \leadsto^* x_v{:}\susp{{\sf
    retn}\,\interp{e}}$ with concurrent computation.

Representing the first case is straightforward: if we are evaluating
$\lambda x.e$, then we have succeeded and can return $\lambda x.e$. 
This is encoded in the rule ${\sf ev/lam}$. 
\[
{\sf ev/lam} : {\sf eval}\,({\sf lam}\,\lambda x.\,E\,x)
   \lefti \{ {\sf retn}\,({\sf lam}\,\lambda x.\,E\,x) \}
\]
Because the second rule involves both recursion and multiple subgoals,
we will generalize our picture of the process state to allow us to store a
stack of unfinished work in the ordered context, growing out to the
right. Our new understanding, then, is that contexts either have the
form $x{:}\susp{{\sf eval}\,\interp{e}}, \Delta$ or the form $x{:}\susp{{\sf
  retn}\,\interp{v}}, \Delta$. In the process of concurrently
computing a trace $x_e{:}\susp{{\sf eval}\,\interp{e_1\,e_2}}, \Delta
\leadsto^* x_r{:}\susp{{\sf retn}\,\interp{v}}, \Delta$, each of the
recursive calls to the search procedure will involve a sub-trace of the
form
%
\[x_e{:}\susp{{\sf eval}\,\interp{e'}}, y{:}\istrue{A^-}, \Delta
  \leadsto^*
  x_r{:}\susp{{\sf retn}\,\interp{v'}}, y{:}\istrue{A^-}, \Delta\]
%
where $A^-$ is a negative proposition that is prepared to interact
with the final ${\sf retn}\,\interp{v'}$ proposition to kickstart the
rest of the computation.

It's helpful to work backwards: in the fourth step, we have found
$E\,x = \interp{e}$ (where $e$ potentially has $x$ free) and $V_2 =
\interp{v_2}$, and the recursive call is to ${\sf
  eval}\,\interp{[v_2/x]e}$, which is the same thing as ${\sf
  eval}\,(E\,V_2)$. If the recursive call successfully returns, the
context will contain a suspended atomic proposition of the form ${\sf
  retn}\,V$ where $V = \interp{v}$, and the search procedure as a
whole is complete: the answer is $v$.  Thus, the negative proposition
that implements the continuation can be written as $(\forall V. {\sf
  retn}\,V \lefti \{ {\sf retn}\,V \})$. The positive proposition that
will create this sub-computation can be written as follows:
\begin{align*}
{\it Step_4}(E,V_2) & \equiv {\sf eval}\,(E\,V_2) 
\fuse {\downarrow}(\forall V.\, {\sf retn}\,V \lefti \{ {\sf retn}\,V \})
%
\intertext{Moving backwards, in the third step we have a $E_2 =
  \interp{e_2}$ that we were given and $E\,x = \interp{e}$ that we
  have computed. The recursive call is to ${\sf
    eval}\,\interp{e_2}$, and assuming that it completes, we need
  to begin the fourth step. The positive proposition that will 
  create this sub-computation can be written as follows:}
%
{\it Step_3}(E_2,E) & \equiv {\sf eval}\,E_2 
\fuse {\downarrow}(\forall V_2.\,
  {\sf retn}\,V_2 \lefti \{ {\it Step_4}(E,V_2) \})
%
\intertext{Finally, the first two steps can be handled together. We have
$E_1 = \interp{e_1}$ and $E_2 = \interp{e_2}$; the recursive
call is to ${\sf eval}\,\interp{e_1}$. Once the
recursive call completes, we can enforce that the returned value has
the form $\interp{\lambda x.e}$ before proceeding
to the continuation.}
{\it Step_{1,2}}(E_1, E_2) & \equiv {\sf eval}\,E_1
\fuse {\downarrow}(\forall E.\, {\sf retn}\,({\sf lam}\,\lambda x.\,E\,x)
\lefti \{ {\it Step_3}(E_2, E)\})
\end{align*}
Thus, the rule implementing this entire portion of the search
procedure is 
\[
\forall E_1.\,\forall E_2.\,
{\sf eval}\,({\sf app}\,E_1\,E_2) \righti \{ {\it
  Step_{1,2}}(E_1, E_2) \}
\]
The \sls~encoding of our example natural semantics is shown in
Figure~\ref{fig:example-transform-cbv} alongside the transformed
specification, which has the form of an ordered abstract machine
semantics, though it is different than the ordered abstract machine
semantics presented in the introduction. We say the specification
above is {\it higher-order}, as ${\sf ev/app}$ is a rule that, when it
participates in a transition, produces a new rule $(\forall E.\,{\sf
  retn}\,({\sf lam}\,\lambda x.\,E\,x) \lefti \{ \ldots \})$ that
lives in the context. The ordered abstract machine semantics from the
introduction was {\it first-order}, because the head $\{ A^+ \}$ of
every concurrent rule contains only positive atomic propositions.  We
discuss the defunctionalization transformation, which allows us to
derive first-order specifications from specifications that are
higher-order in this way, in Section~\ref{sec:defunctionalization}
below.

\begin{figure}
\begin{minipage}[b]{0.36\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-ev.sls}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.64\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-ev-ssos.sls}
\end{minipage}
\caption{A natural semantics for CBV (left) and the corresponding (higher-order)
  ordered abstract machine (right).}
\label{fig:example-transform-cbv}
\end{figure}

The intuitive connection between natural semantics specifications and
concurrent specifications has been explored previously and
independently by Schack-Nielsen \cite{schacknielsen07induction} and by
Cruz and Favonia \cite{cruz12parallel}; Schack-Nielsen proves the
equivalence of the two specifications, whereas Cruz and Favonia used
the connection informally. The contribution of this section is to
describe a general transformation (of which
Figure~\ref{fig:example-transform-cbv} is one instance) and to prove
the transformation correct in general. 

In Section~\ref{sec:trans-subset} we will present the subset of
specifications that our operationalization transformation handles, and
in Section~\ref{sec:trans-basic} we present the most basic form of the
transformation.  In
Sections~\ref{sec:trans-tail}~and~\ref{sec:trans-par} we extend the
basic transformation to be both tail-recursion optimizing and
parallelism-enabling. Finally, in
Section~\ref{sec:operationalization-correct}, we establish the
correctness of the overall transformation.

\subsection{Transformable signatures}
\label{sec:trans-subset}

The starting point for the operationalization transformation is a
deductive signature that is well-moded in the sense described in
Section~\ref{sec:framework-modes}. Every declared negative predicate
will either remain defined by deductive proofs (we write the atomic
propositions built with these predicates as $p_d^-$, $d$ for
deductive) or will be transformed so that it is concurrently defined
(we write the atomic propositions built with these predicates as
$p_c^-$, $c$ for concurrent).

For the purposes of describing and proving the correctness of the
operationalization transformation, we will assume that all transformed
atomic propositions $p_d^-$ have two arguments where the first
argument is moded as an input and the second is an output. That is,
their predicates are declared as follows:
\begin{align*}
& {\sf a} : \tau_1 \rightarrow \tau_2 \rightarrow {\sf prop}.\\
& {\sf \#mode~a~{+}~{-}}.
\end{align*}
Without dependency, two-place relations are sufficient for describing
$n$-place relations.\footnote{As an example, to handle addition on
  natural numbers, defined as a three-place relation ${\sf add} : {\sf
    nat} \rightarrow {\sf nat} \rightarrow {\sf nat} \rightarrow {\sf
    type}$ with its usual mode (${\sf add}~{+}~{+}~{-}$), we define a
  unique type ${\sf add\_in}$ with one binary constructor ${\sf
    add\_c} : {\sf nat} \rightarrow {\sf nat} \rightarrow {\sf
    add\_in}$. Then we can declare (${\sf add'} : {\sf add\_in}
  \rightarrow {\sf nat} \rightarrow {\sf type}$) with mode (${\sf
    add'}~{+}~{-}$).}  It should be possible to handle dependent
predicates (that is, those with declarations of the form ${\sf a} :
\Pi x{:}\tau_1.\,\tau_2(x) \rightarrow {\sf type}$), but we will not do
so here.

The restriction on signatures furthermore enforces that all rules must
be of the form ${\sf r} : C$ or ${\sf r} : D$, where $C$ and $E$ are
refinements of the negative propositions of \sls~that are defined as
follows:
\begin{align*}
C & ::= p^-_{c} 
    \mid \forall x{:}\tau.\, C
    \mid p^+_\mpers \lefti C
    \mid {!}p^-_c \lefti C
    \mid {!}G \lefti C \\
D & ::= p^-_{d}
    \mid \forall x{:}\tau.\, D
    \mid p^+_\mpers \lefti D
    \mid {!}p^-_c \lefti D
    \mid {!}G \lefti C \\
G & ::= p^-_d 
    \mid \forall x{:}\tau.\, G
    \mid p^+_\mpers \lefti G
    \mid {!}D \lefti G
\end{align*}
If {\it all} propositions are to remain deductive, then the
propositions $p^-_c$ and $C$ are irrelevant, and this restriction
describes all persistent, deductive specifications -- essentially, any
signature that could be executed by the standard logic programming
interpretation of LF \cite{pfenning98elf}. On the other hand, if all
propositions are to be transformed, then the propositions $p^-_d$ and
$D$ are irrelevant and this restriction amounts to restricting
rules to the Horn fragment.

All propositions $C$ are equivalent (at the level of synthetic
inference rules) to propositions of the form $\forall
\overline{x_0}\ldots \forall \overline{x_n}.\,A^+_n \lefti \ldots
\lefti A^+_1 \lefti {\sf a}\,t_{0}\,t_{n-1}$, where the $\forall
\overline{x_i}$ are shorthand for a series of universal quantifiers
$\forall {x_{i1}}{:}{\tau_{i1}} \ldots \forall {x_{\it
    ik}}{:}{\tau_{\it ik}}$ and where each variable in
$\overline{x_i}$ does not appear in $t_0$ (unless $i = 0$) nor in any
$A^+_j$ with $j < i$ but does appear in $A^+_i$ (or $t_0$ if $i =
0$). Therefore, when we consider moded proof search, the variables
bound in $\underline{x_0}$ are all fixed by the query and those bound
in the other $\underline{x_i}$ are all fixed by the output position of
the $i^{\rm th}$ premise.

\subsection{Basic transformation}
\label{sec:trans-basic}

The operationalization transformation $\transop{\Sigma}$
operates on SLS signatures $\Sigma$ that have the form described in the
previous section. We
will first give the transformation on signatures; the transformation
of rule declarations ${\sf r} : C$ is the key case.

Each two-place predicate ${\sf a}$ that we plan to operationalize gets
turned into two one-place predicates ${\sf eval\_a}$ and ${\sf
  retn\_a}$.  We will write $\opsubst{X}$ for the operation of
substituting all occurrences of $p^-_c = {\sf a}\,t_1\,t_2$ with
$({\sf eval\_a}\,t_1 \lefti \{ {\sf retn\_a}\,t_2 \})$ in $X$. This
substitution operation is used on propositions, contexts, and frames;
it appears in the transformation of rules ${\sf r} : D$ below.

\begin{itemize}
\item $\transop{\cdot} = \cdot$
\item $\transop{\Sigma, {\sf a} : \tau_1 \rightarrow \tau_2
    \rightarrow {\sf prop}} = \transop{\Sigma}, ~ {\sf eval\_a} :
  \tau_1 \rightarrow {\sf prop\,ord}, ~ {\sf retn\_a} : \tau_2
  \rightarrow {\sf prop\,ord}$ \\ {\it (if $\sf a$ is one of the
    predicates that we are translating)}
\item $\transop{\Sigma, {\sf a} : K} = \transop{\Sigma}, ~ {\sf a}
  : K$ {\it (otherwise)}
\item $\transop{\Sigma, {\sf c} : \tau} = \transop{\Sigma}, ~ {\sf
    c} : \tau$ 
\item $\transop{\Sigma, {\sf r} : C} = \transop{\Sigma}, ~ {\sf r}
  : \forall \overline{x_0}.\, {\sf eval\_a}\,t_0 \lefti \llbracket A^+_1,
  \ldots, A^+_n \rrbracket (t_{n+1}, {\sf id})$ \\ {\it (where $C$ is
    equivalent to $\forall \overline{x_0}\ldots \forall
    \overline{x_n}.\, A^+_n \lefti \ldots \lefti A^+_1 \lefti {\sf
      a}\,t_{0}\,t_{n+1}$)}
\item $\transop{\Sigma, {\sf r} : D} = \transop{\Sigma}, ~ {\sf r}
  : \opsubst{D}$
\end{itemize}

The transformation of a proposition $C$ of the form $\forall
\overline{x_0}\ldots \forall \overline{x_n}.\,A^+_n \lefti \ldots
\lefti A^+_1 \lefti {\sf a}\,t_{0}\,t_{n+1}$ involves the definition
$\opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma}$, where $\sigma$
substitutes only for variables in $\overline{x_j}$ where $j < i$. The
function is defined inductively on the length of the sequence
$A^+_i,\ldots,A^+_n$.

\begin{itemize}
\item $\opbasic{}{t_{n+1}}{\sigma} = \{ {\sf retn\_a}\,(\sigma{t_{n+1}}) \}$
\item $\opbasic{p^+_\mpers,A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma} 
  = \forall \overline{x_i}.\, (\sigma{p^+_\mpers}) \lefti \opbasic{A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma}$
\item $\opbasic{{!}p^-_c,A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma}$
  \\
  $~ \qquad = \{ {\sf eval\_b}\,({\sigma}t^{\it in}_i) \fuse
  (\forall\overline{x_i}.\, {\sf retn\_b}\,(\sigma{t^{\it out}_i})
  \lefti \opbasic{A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma}) \}$\\
  {\it (where $p^-_c$ is ${\sf b}\,t^{\it in}_i\,t^{\it out}_i$)}
\item $\opbasic{{!}G,A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma} = \forall
  \overline{x_i}.\, {!}(\sigma\opsubst{G}) \lefti
  \opbasic{A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma}$
\end{itemize}

\noindent
This operation is slightly more general than it needs to be to
describe the transformation on signatures, because the substitution
$\sigma$ will always just be the identity substitution ${\sf id}$.
Non-identity substitutions arise during the proof of correctness, which
is why we introduced them here.

We have already given an example of the this basic operationalization
transformation, as Figure~\ref{fig:example-transform-cbv} is an
instance of this transformation.

\subsection{Tail-recursion}
\label{sec:trans-tail}

Consider again our motivating example, the procedure for that takes
expressions $e$ and searches for expressions $v$ such that $e
\Downarrow v$ is derivable. If we were to implement that procedure as
a functional program, the procedure would be {\it tail-recursive}. In
the procedure that handles the case when $e = e_1\,e_2$, the last step
is that the search procedure is invoked recursively. If and when that
callee returns $v$, then the caller will also return $v$.

Tail-recursion is significant in functional programming because
tail-recursive calls can be implemented without allocating a stack
frame: when a compiler makes this more efficient choice, we say it is
performing {\it tail-recursion optimization}.\footnote{Or {\it tail-call optimization}, as a tail-recursive function call is just a
  specific instance of a tail call.} An analogous opportunity for
tail-recursion optimization also arises in our logical compilation
procedure. In our motivating example, the last step in the $e_1\,e_2$
case was operationlized as a positive proposition of the form ${\sf
  eval}\,(E\, V) \fuse (\forall v.\,{\sf retn}\,v \lefti \{ {\sf
  retn}\,v \})$. In a successful search, the process state 
\[ x{:}{\sf
  eval}\,(E\, V), y{:}\istrue{(\forall v.\,{\sf retn}\,v \lefti \{
  {\sf retn}\,v \})}, \Delta\]
will concurrently compute until the
state 
\[ x'{:}{\sf retn}\,V, y{:}\istrue{(\forall v.\,{\sf retn}\,v \lefti
  \{ {\sf retn}\,v \})}, \Delta\] is reached, at which point the next
step \[y'{:}{\sf retn}\,V, \Delta\] is reached in one step by focusing
on $y$. 

If we operationalize the last step in the $e_1\,e_2$ case as ${\sf
  eval}\,(E\,V)$ instead of as ${\sf eval}\,(E\, V) \fuse (\forall
v.\,{\sf retn}\,v \lefti \{ {\sf retn}\,v \})$, we will reach the same
final state with one less transition. The tail-recursion optimizing
version of the operationalization transformation creates concurrent
computations that avoid these useless steps.

We cannot perform tail recursion in general. The obvious reason for
this to be the case is when the output of the last subgoal is
different from the output of the goal. For example, the rule ${\sf r}
: \forall{x}.\,\forall{y}.\,{!}{\sf a}\,x\,y \lefti {\sf a}\,({\sf
  c}\,x)\,({\sf c}\,y)$, will translate to
\[ {\sf r} : \forall{x}.\,{\sf eval\_a}\,({\sf c}\,x) \lefti \{ {\sf
  eval\_a}\,x \fuse (\forall y.\, {\sf retn\_a}\,y \lefti \{ {\sf
  retn\_a}\,({\sf c}\,y) \} ) \} \] There is no opportunity for
tail-recursion optimization, because the output of the last search
procedure, $t^{\it out}_n = y$, is different than the value returned
down the stack, $t_{n+1} = {\sf c}\,y$. This case corresponds to
functional programs that cannot be tail-call optimized.

More subtly, we cannot even eliminate all cases where $t^{\it out}_n =
t_{n+1}$ unless these terms are {\it fully general}. We say that
$t_{n+1}$ with type $\tau$ is fully general if all of its free
variables are in $\overline{x_n}$ (and therefore not fixed by the
input of any other premise) and if, for any variable-free term $t'$ of
type $\tau$, there exists a substitution $\sigma$ such that $t =
{\sigma}t_{n+1}$. The simplest example way to do this is to force
$t_{n+1} = t^{\it out}_n = y$ where $y = \overline{x_n}$.\footnote{It
  is also possible to have a fully general $t_{n+1} = {\sf
    c}\,y_1\,y_2$ if, for instance, ${\sf c}$ has type $\tau_1
  \rightarrow \tau_2 \rightarrow {\sf foo}$ and there are no other
  constructors of type ${\sf foo}$. However, we also have to check
  that there are no other first-order variables in $\Psi$ with types
  like $\tau_3 \rightarrow {\sf foo}$ that could be used to make other
  terms of type ${\sf foo}$. The technology to handle this, worlds
  checking and subordination analysis, is well-understood and surveyed
  elsewhere \cite{harper07mechanizing}, but this is tangential to the
  current discussion.} This condition doesn't have an analogue in
functional programming, because it corresponds to the possibility that
moded deduction computation can perform pattern matching on {\it
  outputs} and fail if the pattern match fails.

The tail-recursive procedure can be described by adding a new 
case to the definition of 
$\opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma}$:

\begin{itemize}
\item $\opbasic{{!}{\sf a}\,t^{\it in}_n\,t_{n+1}}{t_{n+1}}{\sigma} 
  = \{{\sf eval\_a}\,({\sigma}{t^{\it in}_n})\}$
\\
  {\it (where $t_{n+1}$ is fully general)}
\end{itemize}
This case overlaps with the third case of the definition given
in Section~\ref{sec:trans-basic}, which indicates that tail-recursion
optimization can be applied or not in a nondeterministic manner.

\begin{figure}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-ev-ssos-tail.sls}
\caption{A higher-order ordered abstract machine semantics for the CBV
  evaluation.}
\label{fig:cbv-ev-ssos-tail}
\end{figure}

\subsubsection{Example}

Operationalizing the natural semantics from
\ref{fig:example-transform-cbv} results in the ordered abstract
machine in Figure~\ref{fig:cbv-ev-ssos-tail}.  A more dramatic
illustration of tail-call optimization can be given if we consider a
big-step evaluation function that is based on a small-step structural
operational semantics (SOS) specification. In SOS specifications,
single-step evaluation is defined as the two-place relation ${\sf
  step} : {\sf exp} \rightarrow {\sf exp} \rightarrow {\sf prop}$
(moded ${\sf exp}\,{+}\,{-}$) that makes use of the helper judgment
${\sf value} : {\sf exp} \rightarrow {\sf prop}$ (moded ${\sf
  value}\,{+}$). We will not define these propositions here, but we do
so later on in Section~\ref{sec:evaluationcontexts}.

Given the definition of ${\sf step}\,\interp{e}\,\interp{e'}$, it is
easy to define big-step evaluation ${\sf ev}\,\interp{e}\,\interp{v}$
as a series of small steps:

\smallskip
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-sos-steps.sls}
\smallskip

\begin{figure}
\begin{minipage}[b]{0.55\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-sos-proc2.sls}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-sos-proc.sls}
\end{minipage}
\caption{The transformation of a trivial big-step semantics, both
  without (left) and with (right) tail-recursion optimization.}
\label{fig:sos-tailrecursion}
\end{figure}

Running this specification through the operationalization
transformation and only operationalizing the ${\sf ev}$ predicate
results in what I consider to be the most boring substructural
operational semantics specification, shown in
Figure~\ref{fig:sos-tailrecursion} both without the tail-recursion
optimization (left) and with the tail-recursion optimization (right).

\begin{figure}
\begin{align*}
& x_1{:}{\sf eval}\,\interp{e_1} \\
\leadsto ~ & x_2{:}{\sf eval}\,\interp{e_2}, 
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} \\
\leadsto ~ & x_3{:}{\sf eval}\,\interp{e_3}, 
  y_2{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})}, 
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} \\
\leadsto ~ & \cdots\\
\leadsto ~ & x_n{:}{\sf eval}\,\interp{v}, 
  y_{n-1}{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})}, 
  \cdots,
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} \\
\leadsto ~ & z_n{:}{\sf retn}\,\interp{v}, 
  y_{n-1}{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})}, 
  \cdots,
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} \\
\leadsto ~ & \cdots \\
\leadsto ~ & z_3{:}{\sf retn}\,\interp{v}, 
  y_2{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})}, 
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} \\
\leadsto ~ & z_2{:}{\sf retn}\,\interp{v}, 
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} \\
\leadsto ~ & z_1{:}{\sf retn}\,\interp{v}
\end{align*}
\caption{Example trace with the non-tail-recursion-optimized
  semantics in Figure~\ref{fig:sos-tailrecursion}}
\label{fig:example-proc-non-tail-recursive-trace}
\end{figure}

The tail-recursion optimized translation is definitely superior for
this example. Concurrent proofs for the non-tail-recursion-optimized
specification build up an enormous stack of useless copies of the
proposition $(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})$,
as shown in Figure~\ref{fig:example-proc-non-tail-recursive-trace}.
In contrast, the tail-recursion optimized version on the right hand
side of Figure~\ref{fig:sos-tailrecursion} takes half as many steps,
and each step is smaller, simpler, and the overall trace does a better
job of actually capturing the linear computation that is actually
involved in describing a big-step semantics using a small-step
structural operational semantics:
\[
x_1{:}{\sf eval}\,\interp{e_1} 
 ~\leadsto~
x_2{:}{\sf eval}\,\interp{e_2}
 ~\leadsto~
x_3{:}{\sf eval}\,\interp{e_3}
 ~\leadsto~ \cdots ~\leadsto~
x_n{:}{\sf eval}\,\interp{v}
 ~\leadsto~ 
z{:}{\sf retn}\,\interp{v}
\]

\subsection{Parallelism}
\label{sec:trans-par}

Both the basic transformation and the tail-recursive transformation
are sequential: if $x{:}{\sf eval}\,\interp{e} \leadsto^* \Delta$,
then the process state $\Delta$ contains at most one proposition ${\sf
  eval}\,\interp{e'}$ or ${\sf retn}\,\interp{v}$ that can potentially
be a part of any further transition. Put differently, the first two
operationalization transformations express deductive computation as a
concurrent computation that does not exhibit concurrency (sequential
computation being a special case of concurrent computation).

Sometimes, this is what we want: in
Section~\ref{sec:nat-ssos-adequacy} we will see that the sequential
tail-recursion-optimized abstract machine is what we want to
adequately represent the traditional on-paper abstract machines for
the call-by-value lambda calculus. In general, however, when distinct
subgoals do not have input-output dependencies (that is, when none of
subgoal $i$'s outputs are inputs to subgoal $i+1$), deductive computation
can search for subgoal $i$ and $i+1$ simultaneously, and this can 
be represented in the operationalization transformation.

In the previous transformations, our process states were structured
such that every negative proposition $A^-$ was waiting on a single
${\sf retn}$ to be computed to its left; at that point, the negative
proposition could be focused on, which effectively invokes the
continuation stored in that negative proposition. If we ignore the
first-order structure of the concurrent computation, the intermediate
states of look like this:
\[
  (\mbox{subgoal 1}), y{:}\istrue{({\sf retn} \lefti {\it cont})}
\]
Note that $(\mbox{subgoal 1})$ is intended to represent some nonempty
sequence of ordered propositions, not a single proposition. With the
parallelism-enabling transformation, subgoal 1 can even be performing
parallel search for its own subgoals:
\[
 (\mbox{subgoal 1.1}), (\mbox{subgoal 1.2}), 
   y_1{:}\istrue{({\sf retn}_{1.1} \fuse {\sf retn}_{1.2} \lefti {\it cont}_1)}, 
   y{:}\istrue{({\sf retn} \lefti {\it cont})}
\]
The two subcomputations $(\mbox{subgoal 1.1})$ and $(\mbox{subgoal
  1.2})$ are next to one another in the ordered context, but the
structure of transformed specifications ensures that the only way they
can interact is if they both finish (becoming $z_{1.1}{:}\susp{{\sf
    retn}_{1.1}}$ and $z_{1.2}{:}\susp{{\sf retn}_{1.2}}$), which will
allow us to focus on $y_1$ and begin working on the continuation ${\it
  cont}_1$. The principle at work is the same one that says that postfix
notations like Reverse Polish notation are unambiguous: there's always
only one way to reconstruct the tree of subgoals. 

To allow for the transformed programs to have parallelism, we again
add a new case to the function that transforms propositions $C$ in the
signature.  In this case, the new case will actually subsume the old
case that dealt with sequences of the form ${!}p_c^-,
A^+_{i+1},\ldots,A^+_n$; that old case is now an instance of the
general case where $i = j$. 

\begin{itemize}
\item $\opbasic{{!}p^-_{ci},\ldots,{!}p^-_{cj},A^+_{j+1},\ldots,A^+_n}{t_{n+1}}{\sigma}$
  \\
  $~ \qquad = \{ {\sf eval\_bi}\,({\sigma}t^{\it in}_i) 
                    \fuse \ldots \fuse
                 {\sf eval\_bj}\,({\sigma}t^{\it in}_j)$
  \\
  $~ \qquad \qquad (\forall\overline{x_i}\ldots\forall\overline{x_j}.\, 
     {\sf retn\_bi}\,(\sigma{t^{\it out}_i})
     \fuse \ldots \fuse 
     {\sf retn\_bj}\,(\sigma{t^{\it out}_j})$
  \\
  $~ \qquad \qquad \quad
   \lefti \opbasic{A^+_{j+1},\ldots,A^+_n}{t_{n+1}}{\sigma}) \}$\\
  {\it (where
   $p^-_{ck}$ is ${\sf bk}\,t^{\it in}_k\,t^{\it out}_k$ 
   and $FV(t_k^{\it in}) \notin (\overline{x_i} \cup \ldots \cup \overline{x_j})$ 
   for $i \leq k \leq j$)}
\end{itemize}

\noindent
Note that the second side condition on the free variables of inputs is
necessary if the resulting term is to be well-scoped, and is trivially 
satisfied in the sequential case where $i = j$. 

\begin{figure}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-ev-ssos-par.sls}
\caption{The parallel, tail-recursion optimized ordered abstract machine for
 call-by-value evaluation.}
\label{fig:cbv-ev-ssos-par}
\end{figure}

The result of running the natural semantics from
Figure~\ref{fig:example-transform-cbv} through the parallel and
tail-recursion optimizing ordered abstract machine is shown in
Figure~\ref{fig:cbv-ev-ssos-par}; it represents that we can
search for the subgoals $e_1 \Downarrow \lambda x.e$ and
$e_2 \Downarrow v_2$ in parallel. We cannot, of course, run either
of these subgoals in parallel with the third subgoal 
$[v_2/x]e \Downarrow v$ because the input $[v_2/x]e$ mentions the outputs
of both of the previous subgoals. 

\subsection{Correctness}
\label{sec:operationalization-correct}

The correctness of the basic, tail-recursion-optimizing, and parallel
transformations all follow from the correctness of the parallel
transformation; because the transformation is nondeterministic, the
previously presented transformations are just instances of this most
general one.

\bigskip
\begin{theorem}[No effect on the LF fragment]
  $\Psi \vdash_\Sigma t : \tau$ if and only if $\Psi
  \vdash_{\transop{\Sigma}} t : \tau$.
\end{theorem}

\begin{proof}
Straightforward induction in both directions; the transformation 
leaves the LF-relevant part of the signature unchanged.
\end{proof}

\begin{theorem}[Soundness of operationalization]
XXX Soundness
\end{theorem}

\begin{proof}
XXX Proof
\end{proof}

\begin{theorem}[Completeness of operationalization]
If all propositions in $\Gamma$ have the form 
$x{:}D$ or $z{:}\susp{p^+_{\mpers}}$, then
\begin{enumerate}
\item  
If $\slss{\Sigma}{\Psi}{\Gamma}{\susp{p^-_d}}$,
then $\slss{\transop{\Sigma}}{\Psi}{\opsubst{\Gamma}}{\susp{p^-_d}}$.
\item  
If $\slss{\Sigma}{\Psi}{\Gamma, [D]}{\susp{p^-_d}}$,
then $\slss{\transop{\Sigma}}{\Psi}{\opsubst{\Gamma}, [\opsubst{D}]}{\susp{p^-_d}}$.
\item  
If $\slss{\Sigma}{\Psi}{\Gamma}{G}$,
then $\slss{\transop{\Sigma}}{\Psi}{\opsubst{\Gamma}}{\opsubst{G}}$.
\item
If $\Delta$ matches $\frameoff{\Theta}{\Gamma}$ 
and $\slss{\Sigma}{\Psi}{\Gamma}{\susp{p^-_c}}$
(where $p^-_c = {\sf a}\,t\,s$),\\
then
$(\Psi; \tackon{\opsubst{\Theta}}{x{:}\susp{{\sf eval\_a}\,t}}) 
  \leadsto^*_{\transop{\Sigma}}
 (\Psi; \tackon{\opsubst{\Theta}}{y{:}\susp{{\sf retn\_a}\,s}})$.
\end{enumerate}
\end{theorem}

\begin{proof}
Mutual induction on the size 
of the input derivation.

The first three parts are straightforward. In part 1, we have
$\slst{\Sigma}{\Psi}{\Gamma}{\tfocusl{h}{\Sp}}{\susp{p^-_d}}$ where
either $h = x$ and $x{:}D \in \Gamma$ or else $h = {\sf r}$ and ${\sf
  c}{:}D \in \Sigma$. In either case the necessary result is
$\tfocusl{h}{\Sp'}$, where we get $\Sp'$ from the induction hypothesis
(part 2) on $\Sp$.

In part 2, we proceed by case analysis on the proposition $D$ in focus. 
The only interesting case is where $D = {!}p^-_c \lefti D'$
\begin{itemize}
\item If $D = p_d^-$, $\Sp = \tnil$ and $\tnil$ gives the desired result.

\item If $D = \forall x{:}\tau.\,D'$ or $D = p^+_{\sf
    pers} \lefti D'$, then $\Sp = (\tforalll{t}{\Sp'})$ 
  or $\Sp = (\tappl{z}{\Sp'})$ (respectively). The necessary result is
  $(\tforalll{t}{\Sp''})$ 
  or $(\tappl{z}{\Sp''})$ (respectively) where we get $\Sp''$ from the
  induction hypothesis (part 2) on $\Sp'$. 

\item If $D = {!}p^-_c \lefti D'$ and $p^-_c = {\sf a}\,t_1\,t_2$, then 
  $\Sp = (\tappl{\tbangr{\tetan{N}}}{\Sp'})$
  and $\opsubst{D} = {!}({\sf eval}\,t_1 \lefti \{ {\sf retn}\,t_2 \}) \lefti \opsubst{D'}$.

  \begin{tabbing}
  $\slst{\Sigma}{\Psi}{\Gamma}{N}{\susp{p^-_c}}$
  \` (given)
  \\
  $\slst{\Sigma}{\Psi}{\Gamma, [D]}{\Sp'}{\susp{p^-_d}}$
  \` (given)
  \\
  $T :: (\Psi; \opsubst{\Gamma}, x{:}{\sf eval_a}\,t_1)
    \leadsto^*_{\transop{E}} (\Psi; \opsubst{\Gamma}, y{:}{\sf eval_a}\,t_2)$
  \` (ind. hyp. (part 2) on $N$)
  \\
  $\slst{\transop{\Sigma}}{\Psi}{\Gamma, [\opsubst{D'}]}{\Sp''}{\susp{p_d^-}}$
  \` (ind. hyp. (part 2) on $\Sp'$)
  \\
  $\slst{\transop{\Sigma}}{\Psi}{\opsubst{\Gamma}}
    {\tlaml{\tetap{x}{\,\tlet{T}{y}}}}
    {{\sf eval\_a}\,t_1 \lefti \{ {\sf retn\_a}\,t_2 \}}$
  \` (construction)
  \\
  $\slst{\transop{\Sigma}}{\Psi}{\opsubst{\Gamma}, [\opsubst{D}]}
    {\tappl{\tbangr{(\tlaml{\tetap{x}{\,\tlet{T}{y}}})}}{\Sp'}}
    {\susp{p^-_d}}$
  \` (construction)
  \end{tabbing}
\item If $D = {!}G \lefti D'$, then $\Sp =
  (\tappl{\tbangr{\tetan{N}}}{\Sp'})$. The necessary result is
  $(\tappl{\tbangr{\tetan{N'}}}{\Sp''})$; we get $N'$ from the
  induction hypothesis (part 3) on $N$ and get $\Sp''$ from the induction
  hypothesis (part 2) on $\Sp'$.
\end{itemize}

The cases of part 3 follow the same pattern as the ones from part 2, but
without the interesting case (which is excluded from the refinement $G$);
the result follows by the induction hypothesis (part 3 or part 1). 

In part 4, we have $\slst{\Sigma}{\Psi}{\Gamma}{\tfocusl{\sf
    c}{\Sp}}{\susp{p^-_d}}$, where ${\sf r}{:}C \in \Sigma$ and the
proposition $C$ is equivalent to
$\forall{\overline{x_0}}\ldots\forall{\overline{x_n}}.\, A^+_n \lefti
\ldots \lefti A^+_1 \lefti {\sf a}\,t_0\,t_{n+1}$ as described in
Section~\ref{sec:trans-basic}. This means we can decompose $\Sp$ to
get $\sigma_i = (\overline{s_0}/\overline{x_0},\ldots,
\overline{s_i}/\overline{x_i})$ (for some terms $\overline{s_0} \ldots
\overline{s_i}$ that have the correct type in $\Psi$) and a value
$\slst{\Sigma}{\Psi}{\Gamma}{V_i}{[\sigma_i{A^+_i}]}$ for each $0 \leq
i \leq n$. We also have $t = \sigma_0{t_0}$ and $s = \sigma_n{t_{n+1}}$.

Because 
${\sf r}{:}\forall\overline{x_0}.\,{\sf eval\_a}\,t_0 \lefti \opbasic{A_1, \ldots, A_n}{t_{n+1}}{{\sf id}} \in \transop{\Sigma}$, by left-focusing
on that constant
it suffices to show that there is a $\Sp'$ such that 
$\slst{\transop\Sigma}{\Psi}{\Gamma,[
\opbasic{A_1^+, \ldots, A_n^+}{t_{n+1}}{\sigma_0}
]}{\Sp'}{\susp{\{C^+\}}}$ and a trace of the form
$T :: (\Psi; \tackon{\opsubst{\Theta}}{C^+}) \leadsto^*_{\transop{\Sigma}}
 (\Psi; \tackon{\opsubst{\Theta}}{y{:}{\sf retn\_a}\,})$. We will prove this
by induction on the length of the trace; the general statement is that
there is a $\Sp'$ such that 
$\slst{\transop\Sigma}{\Psi}{\Gamma,[
\opbasic{A_i^+, \ldots, A_n^+}{t_{n+1}}{\sigma_{i-1}}
]}{\Sp'}{\susp{\{C^+\}}}$ and a trace of the form
$T :: (\Psi; \tackon{\opsubst{\Theta}}{C^+}) \leadsto^*_{\transop{\Sigma}}
 (\Psi; \tackon{\opsubst{\Theta}}{y{:}{\sf retn\_a}\,s})$. We proceed
by case analysis on the definition of the operationalization transformation:
\begin{itemize}
\item $\opbasic{}{t_{n+1}}{\sigma_n} = \{ {\sf retn\_a}\,(\sigma_{n}{t_{n+1}}) \}$

  \bigskip
  This is a base case: $\Sp' = \tnil$ and because $\sigma_n{t_{n+1}} = s$, 
  $(\Psi; \tackon{\opsubst{\Theta}}{{\sf retn\_a}\,(\sigma_{n}{t_{n+1}})})$ decomposes
  to $(\Psi; 
  \tackon{\opsubst{\Theta}}{y{:}{\susp{{\sf retn\_a}\,(\sigma_{n}{t_{n+1}})}}})$
  in an inversion phase.
  \bigskip

\item $\opbasic{{!}{\sf a}\,t^{\it in}_n\,t_{n+1}}{t_{n+1}}{\sigma_{n-1}} 
  = \{{\sf eval\_a}\,(\sigma_{n-1}{t^{\it in}_n})\}$

  \bigskip
  We are given a value 
  $\slst{\Sigma}{\Psi}{\Gamma}{\tbangr{N}}
   {[{\bang}{\sf a}\,\sigma_n{t^{\it in}_n}\,\sigma_n{t_{n+1}}]}$;
  observe that $\sigma_{n-1}{t_n^{\it in}} = \sigma_n{t_n^{\it in}}$

  \smallskip
  This is also a base case of the inner induction: $\Sp' = \tnil$ and
  $(\Psi; 
  \tackon{\opsubst{\Theta}}
  {{\sf eval\_a}\,(\sigma_{n}{t^{\it in}_n})})$
  decomposes to 
  $(\Psi; 
  \tackon{\opsubst{\Theta}}
  {x_{n}{:}{\susp{{\sf eval\_a}\,(\sigma_{n}{t^{\it in}_n})}}})$.
  We must demonstrate a trace the rest of the way to
  $(\Psi; \tackon{\opsubst{\Theta}}{y{:}{\sf retn\_a}\,s})$. Because 
  $s = \sigma_n{t_{n+1}}$, this is established by the 
  outer induction hypothesis (part 4) on $N$. 
  \bigskip

\item $\opbasic{p^+_\mpers,A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}} 
  = \forall \overline{x_i}.\, \sigma_{i-1}{p^+}_\mpers \lefti \opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}}$

  \begin{tabbing}
  $\slst{\Sigma}{\Psi}{\Gamma}{z}{[\sigma_i{p^+_\mpers}]}$
  \` (given) 
  \\
  $\sigma_i = (\sigma_{i-1}, \overline{s_i}/\overline{x_i})$.
  \` (definition of $\sigma_i$)
  \\
  $\slst{\transop{\Sigma}}{\Psi}{\Gamma, [\opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma_{i}}]}{\Sp'}{\susp{\{ C^+ \} }}$
  \` (by inner ind. hyp.)
  \\
  $T :: (\Psi; \tackon{\opsubst{\Theta}}{C^+}) \leadsto^*_{\transop{\Sigma}}
   (\Psi; \tackon{\opsubst{\Theta}}{y{:}{\sf retn\_a}\,s})$
  \` (by inner ind. hyp.)
  \\
  $\slst{\transop{\Sigma}}{\Psi}
    {\Gamma, [\forall \overline{x_i}.\, \sigma_{i-1}{p^+}_\mpers 
                \lefti \opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}}]}
    {\left(\tforalll{\overline{s_i}}{\tappl{z}{\Sp'}}\right)}{\susp{\{ C^+ \}}}$
  \\ 
  \` (construction)
  \end{tabbing}

\item $\opbasic{{!}p^-_{ci},\ldots,{!}p^-_{cj},A^+_{j+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}}$
  \\
  $~ \qquad = \{ {\sf eval\_bi}\,({\sigma_{i-1}}t^{\it in}_i) 
                    \fuse  \ldots \fuse
                 {\sf eval\_bj}\,({\sigma_{i-1}}t^{\it in}_j)$
  \\
  $~ \qquad \qquad (\forall\overline{x_i}\ldots\forall\overline{x_j}.\, 
     {\sf retn\_bi}\,(\sigma_{i-1}{t^{\it out}_i})
     \fuse \ldots \fuse 
     {\sf retn\_bj}\,(\sigma_{i-1}{t^{\it out}_j})$
  \\
  $~ \qquad \qquad \quad
   \lefti \opbasic{A^+_{j+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}}) \}$\\
  {\it (where
   $p^-_{ck}$ is ${\sf bk}\,t^{\it in}_k\,t^{\it out}_k$ 
   and $FV(t_k^{\it in}) \notin (\overline{x_i} \cup \ldots \cup \overline{x_j})$ 
   for $i \leq k \leq j$)}

  \bigskip
  Let $\Sp = \tnil$. It then suffices to show that there is a trace
\begin{align*}
    &(\Psi, \opsubst{\Theta} \tackonstart
        x_i{:}\susp{{\sf eval\_bi}\,({\sigma_{i-1}}t^{\it in}_i)}, \ldots,
    x_j{:}\susp{{\sf eval\_bj}\,({\sigma_{i-1}}t^{\it in}_j)},
  \\
  & \qquad\qquad y_{ij}{:}(\forall\overline{x_i}\ldots\forall\overline{x_j}.\, 
     {\sf retn\_bi}\,(\sigma_{i-1}{t^{\it out}_i})
     \fuse \ldots \fuse 
     {\sf retn\_bj}\,(\sigma_{i-1}{t^{\it out}_j})\\
  & \qquad\qquad\qquad
      \lefti \opbasic{A^+_{j+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}})\,\mtrue
       \tackonstop)
  \\
  & \quad \leadsto^*_{\transop{\Sigma}} 
     (\Psi; \tackon{\opsubst{\Theta}}{y{:}\susp{{\sf retn\_a}\,s}})
\end{align*}

  \begin{tabbing}
  $\slst{\Sigma}{\Psi}{\Gamma}{\tbangr{N_k}}{[{!}{{\sf bk}\,({\sigma_k}t_k^{\it in})\,({\sigma_k}t_k^{\it out})}]}$ \quad $(i \leq k \leq j)$
  \` (given) 
  \\
  $\slst{\Sigma}{\Psi}{\Gamma}{\tbangr{N_k}}{[{!}{{\sf bk}\,({\sigma_{i-1}}t_k^{\it in})\,({\sigma_j}t_k^{\it out})}]}$ \quad $(i \leq k \leq j)$
  \` (condition on translation, defn. of $\sigma_k$)
  \\
  $T :: (\Psi$\=$, \opsubst{\Theta} \tackonstart
        x_i{:}\susp{{\sf eval\_bi}\,({\sigma_{i-1}}t^{\it in}_i)}, \ldots,
    x_j{:}\susp{{\sf eval\_bj}\,({\sigma_{i-1}}t^{\it in}_j)},$\\
  \>$~ \qquad y_{ij}{:}(\forall\overline{x_i}\ldots\forall\overline{x_j}.\, 
     {\sf retn\_bi}\,(\sigma_{i-1}{t^{\it out}_i})
     \fuse \ldots \fuse 
     {\sf retn\_bj}\,(\sigma_{i-1}{t^{\it out}_j})$\\
  \>$~ \qquad\qquad
      \lefti \opbasic{A^+_{j+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}})\,\mtrue
       \tackonstop)$\\
  $~ \qquad \leadsto^*_{\transop{\Sigma}} 
        (\Psi$\=$, \opsubst{\Theta} \tackonstart
        y_i{:}\susp{{\sf retn\_bi}\,({\sigma_{j}}t^{\it in}_i)}, \ldots,
    y_j{:}\susp{{\sf eval\_bj}\,({\sigma_{j}}t^{\it in}_j)},$\\
  \>$~ \qquad y_{ij}{:}(\forall\overline{x_i}\ldots\forall\overline{x_j}.\, 
     {\sf retn\_bi}\,(\sigma_{i-1}{t^{\it out}_i})
     \fuse \ldots \fuse 
     {\sf retn\_bj}\,(\sigma_{i-1}{t^{\it out}_j})$\\
  \>$~ \qquad\qquad
      \lefti \opbasic{A^+_{j+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}})\,\mtrue
       \tackonstop)$\\
  \` (by outer ind. hyp. (part 4) on each of the $N_k$ in turn)
  \\
  $\slst{\transop{\Sigma}}{\Psi}
     {\Gamma,[\opbasic{A^+_{j+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{j}}]}
     {\Sp'}{\susp{\{ C^+ \}}}$  \` (by inner ind. hyp.)
  \\
  $T' :: (\Psi, \tackon{\opsubst{\Theta}}{C^+}) \leadsto^*_{\transop{\Sigma}}
        (\Psi, \tackon{\opsubst{\Theta}}{y{:}\susp{{\sf retn\_a}\,s}})$ 
   \` (by inner ind. hyp.)
  \end{tabbing}

  The construction 
  $\left(T; \tstep{\mkpat{C^+}}{y_{ij}}{(\tforalll{\overline{s_i}\ldots\overline{s_j}}{\tappl{(\tfuser{y_i}{\tfuser{\ldots}{y_j}})}{\Sp'}})}; T'\right) $
  is then a trace of the correct type.
  \bigskip

\item $\opbasic{{!}G,A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma} = \forall
  \overline{x_i}.\, {!}\sigma\opsubst{G} \lefti
  \opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma}$

  \begin{tabbing}
  $\slst{\Sigma}{\Psi}{\Gamma}{\tbangr{N}}{[{!}\sigma_i{G}]}$
  \` (given) 
  \\
  $\slst{\transop{\Sigma}}{\Psi}{\opsubst{\Gamma}}{N'}{\sigma_i{\opsubst{G}}}$
  \` (by outer ind. hyp. (part 3)  on $N$) 
  \\
  $\sigma_i = (\sigma_{i-1}, \overline{s_i}/\overline{x_i})$.
  \` (definition of $\sigma_i$)
  \\
  $\slst{\transop{\Sigma}}{\Psi}{\Gamma, [\opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma_{i}}]}{\Sp'}{\susp{\{ C^+ \} }}$
  \` (by inner ind. hyp.)
  \\
  $T :: (\Psi; \tackon{\opsubst{\Theta}}{C^+}) \leadsto^*_{\transop{\Sigma}}
   (\Psi; \tackon{\opsubst{\Theta}}{y{:}{\sf retn\_a}\,s})$
  \` (by inner ind. hyp.)
  \\
  $\slst{\transop{\Sigma}}{\Psi}
    {\Gamma, [\forall \overline{x_i}.\, {!}(\sigma_{i-1}{G})
                \lefti \opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}}]}
    {\left(\tforalll{\overline{s_i}}{\tappl{\tbangr{N}}{\Sp'}}\right)}{\susp{\{ C^+ \}}}$
  \\ 
  \` (construction)
  \end{tabbing}
\end{itemize}

\noindent
This completes the inner induction in the fourth part, and hence
the proof.
\end{proof}

\section{Logical transformation: defunctionalization}
\label{sec:defunctionalization}

Defunctionalization is a procedure for turning higher-order
concurrent \sls~specifications into first-order concurrent
\sls~specifications. It is based on the following intuitions:
if $A^-$ is a closed negative proposition
of the form $\forall \overline{x}.\,A^+_1 \lefti \{ A^+_2 \}$
and we have a single-step transition 
$(\Psi; \tackon{\Theta}{y{:}\istrue{A^-}}) 
 \leadsto_{\Sigma} 
 (\Psi; \Delta')$
in an \sls~specification (witnessed by the step 
$(\tstep{\mkpat{A^+_2}}{y}{(\tforalll{\overline{t}}{\tappl{V}{\tnil}})})$), 
then we can define an augmented signature
\begin{align*}
\Sigma' = ~ & \Sigma, 
\\    ~~ & {\sf cont} : {\sf prop\,ord}, 
\\    ~~ & {\sf run\_cont} : \forall{\overline x}.\,p^+_\mtrue \fuse {\sf cont} \lefti \{ A^+ \}
\end{align*}
and it is the case that 
$(\Psi; \tackon{\Theta}{y{:}\susp{\sf cont}}) 
 \leadsto_{\Sigma'} 
 (\Psi; \Delta')$
as well; this new transition is witnessed by the step
$(\tstep{\mkpat{A^+}}{\sf run\_cont}{(\tappl{(\tfuser{V}{y})}{\tnil})})$.

More generally, if we are allowed to extend the signature and $A^-$
falls into the very specific form we have
specified,\footnote{Obviously, the restriction to propositions $A^-$
  of the form $\forall \overline{x}.\,p^+_\mtrue \lefti \{ A^+ \}$ is
  overly specific and designed to apply specifically to the output of
  operationalization, but we will not consider a generalization here.
  Conceptually, it is not complicated to consider a similar operation
  on other propositions, but it is difficult to elegantly describe the
  more general transformation due our use of ordered logic, and we do
  not currently need the more general transformation.}  we can create
a new ordered atomic proposition to do a negative proposition's
job. As long as $\Delta = \tackon{\Theta}{x{:}\susp{\sf cont}}$ and
${\sf cont}$ does not appear in $\Theta$, then $[{\downarrow}A^- /{\sf
  cont}]\Delta \leadsto_\Sigma [{\downarrow}A^- /{\sf cont}]\Delta'$
if and only if $\Delta \leadsto_{\Sigma'} \Delta'$. \footnote{Recall
  from Section~\ref{sec:framework-substprop} that we treat
  %
  $[{\downarrow}A^-/{\sf cont}](\tackon{\Theta}{z{:}\istrue{\susp{\sf cont}}})$
  %
  as being equal to the context in which we {\it first} perform the
  straightforward substitution, giving us
  $(\tackon{\Theta}{z{:}\istrue{{\downarrow}A^-}})$, and then {\it
    second} apply invertible rules, giving us
  $(\tackon{\Theta}{z'{:}\istrue{A^-}})$.} 

We need not restrict ${\sf cont}$ to just a single appearance
suspended in the process state, however. Multiple instances of ${\sf
  cont}$ can appear in the process state without a problem.  It is
similarly unproblematic for ${\sf cont}$ to appear in the monadic head
of some other rule in the process state, as the appearance of an
ordered atomic proposition in a monadic head will not effect the
existence of any transition, but may cause the ordered atomic
proposition to become a suspended ordered proposition in the process
state after the transition. 

By the same reasoning, it is similarly
unproblematic for ${\sf cont}$ to appear in the head of a rule in the
signature.  Therefore, we can replace propositions in the monadic heads
of rules in the signature, like this one:
\begin{align*}
& \Sigma, \\
& {\sf r} : {\sf a} \lefti \{ {\sf b} \fuse {\uparrow}({\sf c} \lefti \{ {\sf d} \fuse {\uparrow}({\sf e} \lefti \{ {\sf f} \}) \}) \}
\intertext{to produce a signature that looks like this:}
& \Sigma, \\
& {\sf cont1} : {\sf prop\,ord}, \\
& {\sf r1} : {\sf c} \fuse {\sf cont1} \lefti \{ {\sf d} \fuse {\uparrow}({\sf e} \lefti \{ {\sf f} \}) \}, \\
& {\sf r} : {\sf a} \lefti \{ {\sf b} \fuse {\sf cont1} \}
\intertext{and the process can be iterated to obtain 
a fully first-order signature:}
& \Sigma, \\
& {\sf cont2} : {\sf prop\,ord}, \\
& {\sf r2} : {\sf e} \fuse {\sf cont2} \lefti \{ {\sf f} \}, \\
& {\sf cont1} : {\sf prop\,ord}, \\
& {\sf r1} : {\sf c} \fuse {\sf cont1} \lefti \{ {\sf d} \fuse {\sf cont2} \}, \\
& {\sf r} : {\sf a} \lefti \{ {\sf b} \fuse {\sf cont1} \}
\end{align*}
This propositional transformation is similar to the one proposed by
Miller in~\cite{miller02higherorder}, where the new propositions were
introduced to hide the internal states of processes.

We can go further and allow $A^-$ to contain free variables if
$A^- = [t_1/y_1]\ldots[t_m/x_m]B^-$ where $B^- = \forall
\overline{x}.\,B_1^+ \lefti \{ B_2^+ \}$ has only the variables
$\overline{y} = y_1\ldots y_m$ free.  In this more general case, we
can revise the signature as follows:
\begin{align*}
\Sigma'' = ~ & \Sigma,
\\    ~~ & {\sf cont} : 
       \Pi x_y{:}\tau_1\ldots \Pi y_m{:}\tau_m.\, {\sf prop\,ord},
\\    ~~ & {\sf run\_cont} : \forall \overline{x}.\,\forall \overline{y}.\,
       p^+_\mtrue \fuse {\sf cont}\,\overline{y} \lefti \{ B^+ \}
\end{align*}
With this revision, we maintain that
%
$[{\downarrow}B^-/{\sf cont}\,\overline{x}]\Delta \leadsto_{\Sigma}
[{\downarrow}B^-/{\sf cont}\,\overline{x}]\Delta'$ if and only if
$\Delta \leadsto_{\Sigma''} \Delta'$ (as long as propositions of the
form ${\sf cont}\,\overline{t}$ only appear suspended in the process
state or in the monadic heads of rules that appear in the process
state).\issue{The process of proving this is mostly an issue of
  stating it precisely, which is a pain. I'd appreciate feedback as to
  whether this seems clear or whether I need to write out the detailed
  proof.}

The one twist we make to the defunctionalization transformation is
that, instead of introducing a new ordered atomic proposition ${\sf
  cont}\,\overline{t}$ for each iteration of the defunctionalization
procedure, we introduce a single type $({\sf frame} : {\sf type})$ and a
single atomic proposition $({\sf cont} : {\sf frame} \rightarrow {\sf
  prop\,ord})$. Then, each iteration of the defunctionalization
procedure produces a new constant with type $\Pi x_y{:}\tau_1\ldots
\Pi y_m{:}\tau_m.\, {\sf frame}$ instead of a new atomic proposition
with kind $\Pi x_y{:}\tau_1\ldots \Pi y_m{:}\tau_m.\, {\sf
  prop\,ord}$.  Operationally, these two approaches are equivalent,
though the approach using frames requires us to disallow variables
that can construct new terms of type ${\sf frame}$ from appearing in
the variable context $\Psi$.

\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/cbv-ev-ssos-fun.sls}
\caption{A first-order ordered abstract machine semantics for CBV
  evaluation.}
\label{fig:cbv-ev-ssos-fun}
\end{figure}

Using defunctionalization procedure outlined above, we obtain the
first-order specification in Figure~\ref{fig:cbv-ev-ssos-fun} from the
higher-order specification in Figure~\ref{fig:cbv-ev-ssos-tail}, which
was in turn derived from the natural semantics for CBV evaluation by
operationalization with tail-recursion optimization.

% As long as 
% $({\sf cont}\,t_1\ldots t_n)$ only appears in $\Delta$ as a 
% suspended atomic proposition, then it is the case that
% $[{\downarrow}(B^-\,x_1\ldots x_n)
%     /{\sf cont}\,x_1\ldots x_n]\Delta 
%  \leadsto_\Sigma
%  [{\downarrow}(B^-\,x_1\ldots x_n)
%     /{\sf cont}\,x_1\ldots\,x_n]\Delta'$ 
% if and only if 
% %
% $\Delta \leadsto_{\Sigma''} \Delta'$.\footnote{Recall from
%   Section~\ref{sec:framework-substprop} that
%   $[{\downarrow}(B^-\,x_1\ldots x_n)/({\sf cont}\,x_1\ldots
%   x_n)](z{:}\susp{{\sf cont}\,t_1\ldots t_n})$ as being equal to the
%   context in which we substitue and then apply invertible rules, i.e.
%   $z{:}\istrue{B^-\,t_1\ldots t_n}$}

% In addition to allowing these newly introduced 
% atomic propositions to appear suspended in the context, it is not 
% a problem to allow them to appear in the heads of monadic clauses. 
% This means that we can 

%  monadic
% clauses, there is no 

% The defunctionalization transformation then applies the same reasoning
% to signatures: if a proposition ${\downarrow}A^-$ appears in the monadic
% head of some rule, 

%  $A^- = B^-\,t_1\,t_2\,t_3$

% This is even
% true if $A^-$ has free variables: we can always define a closed
% $B^- : $




\section{Adequacy with abstract machines}
\label{sec:nat-ssos-adequacy}

I claim that the four-rule abstract machine specification given at the
beginning of this chapter is adequately represented by the derived
\sls~specification in Figure~\ref{fig:cbv-ev-ssos-fun}. For terms and
for deductive computations, adequacy is a well-understood concept: we
know what it means to define an adequate encoding function $\interp{e}
= t$ from ``on-paper'' terms $e$ with (potentially) variables
$x_1,\ldots,x_n$ free to LF terms $t$ where $x_1{:}{\sf
  exp},\ldots,x_n{:}{\sf exp} \vdash t : {\sf exp}$, and we know what
it means to adequately encode the judgment $e \Downarrow v$ as a
negative atomic \sls~proposition ${\sf ev}\,\interp{e}\,\interp{v}$
and to encode derivations of this judgment to \sls~terms $N$ where
$\slst{\Sigma}{\cdot}{\cdot}{N}{\susp{{\sf
      ev}\,\interp{e}\,\interp{v}}}$
\cite{harper93framework,harper07mechanizing}. What does it mean to
adequately represent machine states as process states (that is,
substructural contexts) and to encode a transition system as a 
concurrent \sls~specification? 

The answer given in the literature by Cervesato et
al.~\cite{cervesato02concurrent} and by
Schack-Nielsen~\cite{schacknielsen07induction} has three steps. The
first step is to, define an interpretation function from states $s$
and stacks $k$ to process states $\Delta$, so that, for example, the
state
\[
((\ldots({\sf halt}; \Box\,e_1)\ldots); (\lambda x.e_n)\,\Box) \lhd v
\]
is interpreted as the process state
\[
y{:}\susp{{\sf retn}\,\interp{v}}, ~~
x_n{:}\susp{{\sf cont}\,({\sf app2}\,\lambda x.\interp{e_n})}, ~~
\ldots, ~~
x_1{:}\susp{{\sf cont}\,({\sf app1}\,\interp{e_1})}, ~~
\]
The second step is to, prove a preservation-like adequacy theorem. Let
$\Sigma\ref{fig:cbv-ev-ssos-fun}$ be the signature from
Figure~\ref{fig:cbv-ev-ssos-fun}: we show that if state $s$ is
interpreted and $\Delta$ and $\Delta
\leadsto_{\Sigma\ref{fig:cbv-ev-ssos-fun}} \Delta'$, then there is a
state $s'$ such that $s'$ is interpreted as $\Delta'$. Then we can
prove the main adequacy result: that the interpretation of state $s$
steps to the interpretation of state $s'$ if and only if $s \mapsto
s'$.

I believe that the approach to adequacy given in previous work is
unsatisfactory because the interpretation of process states into
contexts is 1-to-1 but not onto (and therefore not invertible).  This
means that there is no {\it internal} notion of what it means for a
process state to encode a state $s$ or a stack $k$. By analogy,
``having type ${\sf exp}$'' captures what it means for an LF term
encode an expression and ``having type ${\sf
  ev}\,\interp{e}\,\interp{v}$'' captures what it means for an
\sls~term to encode a derivation of $e \Downarrow v$.

In this section, we will present a different three-part approach that
addresses this perceived deficiency. First, we create a signature
$\Sigma\sf gen$ that encodes well-formed states: the $\Delta$ such
that $x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf gen} \Delta$ and ${\sf
  gen} \notin \Delta$ are in a bijection with the states $s$
(Section~\ref{sec:nat-ssos-adequacy-gen}). This gives us the internal
notion of what it means to encode a process state, which is what we
were previously lacking. Second, we prove the preservation-like
property from before. The difference is that this can now be stated
formally as a property of \sls~specifications: if $x{:}\susp{\sf gen}
\leadsto^*_{\Sigma\sf gen} \Delta$ and $\Delta
\leadsto_{\Sigma\ref{fig:cbv-ev-ssos-fun}} \Delta'$, then $x{:}
\susp{\sf gen} \leadsto^*_{\Sigma\sf gen} \Delta'$
(Section~\ref{sec:nat-ssos-adequacy-pres}). The structure of this
theorem is critical, a point that we will consider in greater depth in
Part III of this thesis. Finally, the third step is the same as it was
in other approaches: we prove that the interpretation of state $s$
steps to the interpretation of state $s'$ if and only if $s \mapsto s'$.

\subsection{Adequacy of states}
\label{sec:nat-ssos-adequacy-gen}

Our first goal is to describe a signature $\Sigma\sf gen$ with the
property that if $x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf gen}
\Delta$ and ${\sf gen} \notin \Delta$ then $\Delta$. A well-formed
process state represting a state $k \rhd e$ has the form
\[
y{:}\susp{{\sf eval}\,\interp{e}}, ~~
x_n{:}\susp{{\sf cont}\,\interp{f_n}}, ~~
\ldots, ~~
x_1{:}\susp{{\sf cont}\,\interp{f_1}}
\]
where $\interp{\Box\,e_2} = {\sf app1}\,\interp{e_2}$ and
$\interp{(\lambda x.e)\,\Box} = {\sf app2}\,(\lambda.\interp{e})$. 
A well-formed process state representing a state $k \lhd e$ has 
the same form, but with a suspended ${\sf retn}\,\interp{v}$ instead
of ${\sf eval}\,\interp{e}$. 

The simplest \sls~signature that encodes this structure essentially
has the structure of a Chomsky normal form describing well-formed
contexts, with two unary productions and one unary production.

\smallskip
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/cbv-ev-ssos-gen.sls}
\smallskip

\noindent In addition to the four declarations above, the full
signature $\Sigma\sf gen$ includes all the type, proposition, and
constant declarations from Figure~\ref{fig:cbv-ev-ssos-fun}, but none
of the rules.

Note that this specification most definitely is {\it not} well-moded.
{\it Generative signatures} such as this one are not generally moded,
and we don't think about traces under these signatures as 
necessarily being concurrent computations in the same way we think
about ordered abstract machines being concurrent computations. That is,
rather than thinking of traces in these signatures being produced by
different computations, such as the computational content of the adequacy
theorem:

\bigskip
\begin{theorem}[Adequacy of states]~
\begin{itemize}
\item There is a bijection between contexts $\Delta$ such that
  $x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf gen} \Delta$ where ${\sf gen} \notin \Delta$
  and states $s$.
\item There is a bijection between frames $\Theta$ such that $x{:}\susp{\sf
    gen} \leadsto^*_{\Sigma\sf gen} \tackon{\Theta}{x'{:}{\sf gen}}$ and
  stacks $k$.
\end{itemize}
\end{theorem}

\begin{proof}
We will give only the two translation from the ``on paper''
semantic artifacts (states $s$ and stacks $k$) to traces:
\begin{itemize}
\item $\interp{s},$ which outputs
a trace $T$ with type $x{:}\susp{\sf gen} \leadsto_{\Sigma\sf gen} \Delta$
where ${\sf gen} \not\in \Delta$, and 
\item $\interp{k}$, which outputs
both a trace $T$ with type 
$x{:}\susp{\sf gen}
  \leadsto_{\Sigma\sf gen} \tackon{\Theta}{x'{:}\susp{\sf gen}}$ where
${\sf gen} \not\in \Delta$ and the variable name $x'$ of the resulting
${\sf gen}$ proposition (which may be the same as $x$). Rather than 
representing this output explicitly, we just assume it is always
named $x'$ in the definition below.  
\end{itemize}
Note that both functions builds contexts only indirectly by building 
traces; similarly, the inverses of these functions are defined by induction
on the structure of traces, not on the structure of contexts.
\begin{tabbing}
~~ \= \qquad\quad\qquad \= $~ :: ~$ \=\kill
\> $\interp{k \rhd e} = \interp{k}; 
     \tstep{z}{\sf gen/eval}{(\tforalll{\interp{e}}{(\tappl{x'}{\tnil})})}$
\\ \>\> $~ :: ~$ 
  \> $x{:}\susp{\sf gen} 
       \leadsto^*_{\Sigma\sf gen} \tackon{\Theta}{x'{:}\susp{\sf gen}} 
       \leadsto_{\Sigma\sf gen} 
          \tackon{\Theta}{z{:}\susp{{\sf eval}\,\interp{e}}}$
\\[4pt]
\> $\interp{k \lhd v} = \interp{k}; 
     \tstep{z}{\sf gen/retn}{(\tforalll{\interp{v}}{(\tappl{x'}{\tnil})})}$
\\ \>\> $~ :: ~$ 
  \> $x{:}{\sf gen} 
       \leadsto^*_{\Sigma\sf gen} \tackon{\Theta}{x'{:}\susp{\sf gen}} 
       \leadsto_{\Sigma\sf gen} 
          \tackon{\Theta}{z{:}\susp{{\sf retn}\,\interp{v}}}$
\\[4pt]
\> $\interp{{\sf halt}} = \emptytrace$
\> $~ :: ~$
  \> $x{:}\susp{\sf gen} \leadsto^* x{:}\susp{\sf gen}$
\\[4pt]
\> $\interp{k; \Box\,e_2} = \interp{k}; \tstep{z, x''}{\sf gen/cont}
     {(\tforalll{{\sf app1}\,\interp{e_2}}{(\tappl{x'}{\tnil})})}$
\\ \>\> $~ :: ~$
 \> $x{:}\susp{\sf gen}
       \leadsto^*_{\Sigma\sf gen} \tackon{\Theta}{x'{:}\susp{\sf gen}}
       \leadsto_{\Sigma\sf gen} \tackon{\Theta}
            {\mkconj
               {z{:}\susp{{\sf cont}\,({\sf app1}\,\interp{e_2})}}
               {x''{:}\susp{\sf gen}}}$
\\[4pt]
\> $\interp{k; (\lambda x.e)\,\Box} = \interp{k}; \tstep{z, x''}{\sf gen/cont}
     {(\tforalll{{\sf app1}\,\interp{e_2}}{(\tappl{x'}{\tnil})})}$
\\ \>\> $~ :: ~$
 \> $x{:}\susp{\sf gen}
       \leadsto^*_{\Sigma\sf gen} \tackon{\Theta}{x'{:}\susp{\sf gen}}
       \leadsto_{\Sigma\sf gen} \tackon{\Theta}
            {\mkconj
               {z{:}\susp{{\sf cont}\,({\sf app2}\,\lambda x.\interp{e})}}
               {x''{:}\susp{\sf gen}}}$
\end{tabbing}
To complete the theorem, it is necessary to show that the two encoding
functions are one-to-one and onto. This can be done by demonstrating
the existence of a function $\interp{T}^{-1}_s = s$ from traces $T$
with type $x{:}\susp{\sf gen} \leadsto_{\Sigma\sf gen} \Delta$ where
${\sf gen} \notin \Delta$ to states $s$ and a function
$\interp{T}^{-1}_k = k$ from traces $T$ with type $x{:}\susp{\sf gen}
\leadsto_{\Sigma\sf gen} \tackon{\Theta}{x'{:}{\sf gen}}$ (where $x$
and $x'$ may be the same) to stacks $k$ and then showing that
the functions compose to the identity in both directions. In this case,
that proof is tedious but straightforward.
\end{proof}

Note that two traces $T :: x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf
  gen} \Delta$ and $T' :: x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf
  gen} \Delta'$ are distinct if and only if the contexts $\Delta$ and
$\Delta'$ are distinct. Therefore, we can equivalently see adequacy as
a bijection between traces and abstract machine states $s$ or as a
bijection between contexts and abstract machine states $s$. In a
situation where this 1-to-1 correspondence between states and traces
did not exist (because two traces generated the same context), it is
not entirely clear whether it would be preferable to define adquecy in
terms of contexts or in terms of traces.

\subsection{Preservation}
\label{sec:nat-ssos-adequacy-pres}

Before we prove that the concurrent system from
Figure~\ref{fig:cbv-ev-ssos-fun} adequately represents the transition
system from the beginning of the chapter, we must show that our
criteria for context well-formedness is actually preserved by the
concurrent computations in Figure~\ref{fig:cbv-ev-ssos-fun}. This is
part of the adequacy argument, but because we state it in terms of the
generative signature $\Sigma\sf gen$, it is also a reasonable
standalone theorem entirely about of \sls~specifications. We will
return to theorems of this form in Part III of this thesis.

\bigskip
\begin{theorem}[Generation by $\Sigma\sf gen$ is invariant under
 $\Sigma\ref{fig:cbv-ev-ssos-fun}$]~\\
  If $x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf gen} \Delta$ and
  $\Delta \leadsto_{\Sigma\ref{fig:cbv-ev-ssos-fun}} \Delta'$, then
  $x{:} \susp{\sf gen} \leadsto^*_{\Sigma\sf gen} \Delta'$
\end{theorem}

\begin{proof}
  Primarily by enumeration of the possible synthetic transitions of
  $\Sigma\ref{fig:cbv-ev-ssos-fun}$ and secondarily by case analysis
  on the structure of the trace $T :: x{:}\susp{\sf gen}
  \leadsto^*_{\Sigma\sf gen} \Delta$.

  \begin{itemize}
  \item $\tstep{z}{\sf ev/lam}{(\tforalll{\lambda x.e\,x}
                                {(\tappl{y}{\tnil})})}$

    \qquad $:: \frameoff{\Theta}
                 {y{:}\susp{{\sf eval}\,({\sf lam}\,\lambda x.e\,x)}}
               \leadsto
               \tackon{\Theta}
                 {z{:}\susp{{\sf retn}\,({\sf lam}\,\lambda x.e\,x)}} $

    \medskip

    $T = T'; \tstep{y}{\sf gen/eval}{(\tforalll{{\sf lam}\,\lambda x.e\,x}{\tappl{x'}{\tnil}})}$,
    so we construct\\
    $T'; \tstep{z}{\sf gen/retn}{(\tforalll{{\sf lam}\,\lambda x.e\,x}{\tappl{x'}{\tnil}})}$

    \medskip

  \item $\tstep{z_1, z_2}{\sf ev/app}{(\tforalll{e_1}
                                {\tforalll{e_2}{(\tappl{y}{\tnil})}})}$

    \qquad $:: \frameoff{\Theta}
                 {y{:}\susp{{\sf eval}\,({\sf app}\,e_1\,e_2)}}
               \leadsto
               \tackon{\Theta}
                 {\mkconj
                  {z_1{:}\susp{{\sf eval}\,e_1}}
                  {z_2{:}\susp{{\sf cont}\,({\sf app1}\,e_2)}}} $

    \medskip

    $T = T'; \tstep{y}{\sf gen/eval}{(\tforalll{{\sf app}\,e_1\,e_2}{\tappl{x'}{\tnil}})}$,
    so we construct\\
    $T'; ~~
     \tstep{x_1, x_2}{\sf gen/cont}{(\tforalll{{\sf app1}\,e_2}{(\tappl{x'}{\tnil})})}; ~~
     \tstep{z}{\sf gen/retn}{(\tforalll{{\sf lam}\,\lambda x.e\,x}{\tappl{x'}{\tnil}})}$

    \medskip


  \item $\tstep{z_1, z_2}{\sf ev/app1}{(\tforalll{\lambda x.e\,x}
                       {\tforalll{e_2}{(\tappl{\tfuser{y_1}{y_2}}{\tnil})}})}$

    \qquad $:: \frameoff{\Theta}
                 {\matchconj
                  {y_1{:}\susp{{\sf retn}\,({\sf lam}\,\lambda x.e\,x)}}
                  {y_2{:}\susp{{\sf cont}\,({\sf app1}\,e_2)}}}$

    \qquad\qquad
               $\leadsto
               \tackon{\Theta}
                 {\mkconj
                  {z_1{:}\susp{{\sf eval}\,e_2}}
                  {z_2{:}\susp{{\sf cont}\,({\sf app2}\,(\lambda x.e\,x))}}} $

  \item $\tstep{z}{\sf ev/app2}{(\tforalll{v_2}
                       {\tforalll{\lambda x.e\,x}
                         {(\tappl{\tfuser{y_1}{y_2}}{\tnil})}})}$

    \qquad $:: \frameoff{\Theta}
                 {\matchconj
                  {y_1{:}\susp{{\sf retn}\,v_2}}
                  {y_2{:}\susp{{\sf cont}\,({\sf app2}\,\lambda x.e\,x)}}}
               \leadsto
               \tackon{\Theta}
                 {z{:}\susp{{\sf eval}\,(e\,v_2)}} $

  \end{itemize}
\end{proof}

\subsection{Adequacy of the transition system}
\label{sec:nat-ssos-adequacy-absmachine}


To make a carefully-chosen analogy, various on-paper styles of
specifying the operational semantics are well-known and visually
recognizable.  To anyone literate in the conventions of programming
languages researchers, a quick glance should be sufficient to classify
the following two rules as natural semantics (or big-step semantics)
for the call-by-value lambda calculus:


Natural semantics are a clean, high-level, and declarative way of
describing the semantics of a simple, pure programming languages, but
they do not scale particularly well with the addition of effects like
state and exceptions. Worse, natural semantics are mostly hopeless in
the face of languages features that incorporate nondeterminism or
advanced control (such as first-class continuations). 

Thus, a researcher interested in a simple, high level specification of
the core features of a functional programming language might
reasonably predict that natural semantics would be a good solution to
their problem; one example is Murphy VII, who used natural semantics
for the high-level formalization of Lambda 5 in his thesis
\cite{murphy08modal}.

Another style used to specify the operational semantics of programming
languages, he {\it abstract
  machine} semantics, is slightly less canonical but nevertheless has
an identifiable set of conventions. The following is an abstract 
machine semantics for our call-by-value lambda calculus:


 for programming languages, t, is slightly less canonical, but abstract machine
specifications nevertheless also have a set of common
conventions. There are two states in an abstract machine
specification, $k \rhd e$ (the expression $e$ is evaluating on top of
stack $k$) and $k \lhd v$ (the value $v$ is being returned to the top
of the stack $k$). The first rule says that, as a function $\lambda x.e$
is already a value, we proceed by returning it to the stack, whereas
for an application $e_1\,e_2$, 


Input


\begin{theorem}
If $x{:}\istrue{\sf gen} \leadsto \Delta$
\end{theorem}

There needs to be some structure and
at least


the design space
of 

 the general idea
of representing intermediate states of a computation as 
substructural process states 



\subsection{The functional correspondence}



\section{Exploring the richer fragment}
\label{sec:richer-ordered-abstract}

\subsection{Mutable storage}
\label{sec:mutable-storage}

No check for pointer inequality! This is a fundamental restriction of
the fact that we're using existential quantification rather than some
form of nominal quantification. (Hack due to Favonia and Bob, personal
communication, but dates back earlier - was it one of Karl's papers?
Cheney cites it in nominal abstraction.)

\subsection{Call-by-need evaluation}

\subsection{Recoverable failure}

\subsection{Environment semantics}

\subsection{Looking back at natural semantics}
\label{sec:enriching-natsem}

\section{Partial transformation}
\label{sec:othertransform}

\subsection{Evaluation contexts}
\label{sec:evaluationcontexts}

Thus far, we have considered big-step operational semantics and abstract
machines, neglecting the third great tradition of programming language
specification, {\it structural operational semantics}. Structural
operational semantics (SOS) define single-step evaluation inductively over
the structure of expressions; the SOS semantics for our running example
language is the following:
\[
\infer
{\lambda x.e\,{\sf value} \mathstrut}
{}
\quad
\infer
{e_1\,e_2 \mapsto e_1'\,e_2 \mathstrut}
{e_1 \mapsto e_1' \mathstrut}
\quad
\infer
{e_1\,e_2 \mapsto e_1\,e_2' \mathstrut}
{e_1\,{\sf value}
 &
 e_2 \mapsto e_2' \mathstrut}
\quad
\infer
{(\lambda x. e)v \mapsto [v/x]e \mathstrut}
{v\,{\sf value} \mathstrut}
\]
This inductive specification is adequately encoded on the left-hand
side of Figure~\ref{fig:cbv-sos}, along with the proposition \Verb|ev|
that describes a big-step operational semantics in terms of repeated
application of the small-step operational semantics.

\begin{figure}[tp]
\fvset{fontsize=\small,boxwidth=229pt}
\BVerbatimInput{sls/cbv-sos.sls}
\BVerbatimInput{sls/cbv-sos-eval.sls}
\caption{Small-step evaluation, and one corresponding abstract machine.}
\label{fig:cbv-sos}
\end{figure}

\fvset{fontsize=\small}

There are a couple of possibilities for how the 
One obvious way to proceed is to simply translate the big-step portion
of our semantics as encoded 


If we just translate the ${\sf ev}$ portion of the semantics (using
the tail-recursion optimizing translation), then we will get what is
probably fair to call the most boring possible substructural
operational semantics: 

\smallskip
\VerbatimInput{sls/cbv-sos-proc.sls}
\smallskip

\noindent
Under this semantics, the substructural context contains a single
resource, \Verb|eval-steps(E)|, which takes steps according to the
rules of the small-step structural operational semantics until a value
is reached, at which point the context contains \Verb|retn-steps(V)|.


\begin{figure}[t]
\VerbatimInput{sls/cbv-sos-defun.sls}
\caption{The defunctionalized abstract machine from Figure~\ref{fig:cbv-sos}.}
\label{fig:cbv-sos-defun}
\end{figure}

The interesting observations are to be had from the other direction: what if

\subsection{Temporal logic}

The natural semantics of \rowan~are not, on a superficial level,
significantly more complex than other natural semantics. However, it
turns out that the usual set of techniques for adding state to a
natural semantics break down, and discussing a \rowan-like logic with
state remained a challenge for many years.\robnote{Figure out from
  Rowan what the recent work he told you about was.} Through the
logical correspondence, it is easy to see why: the natural SSOS
specification of \rowan~integrates both concurrent and deductive
reasoning in an arbitrarily nested way. In fact, Figure XXX is the
only SLS specification in this thesis that exhibits this form of
recursive dependency between concurrent and deductive reasoning.  In
particular, the \rowan~specification is way out of the image of the
extended natural semantics we considered in
Section~\ref{sec:enriching-natsem}. The natural encoding in state lies
in the ambient substructural context of a concurrent computation, but
that ambient computation cannot properly enter into a deductive
sub-computation. If we tried to add state to \rowan~the same way we
added it in Section~\ref{sec:mutable-storage}, the entire store
would effectively leave scope whenever computation considered
the subterm $e$ of ${\sf next}(e)$. That consideration happens
as deductive reasoning, not as concurrent reasoning!

 it is the only we
will consider in this thesis that has with property.

It's hard to include state in temporal logic! But the logical correspondence
helps us understand why: the natural SSOS specification of 

