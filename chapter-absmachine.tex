\chapter{Ordered abstract machines}
\label{chapter-absmachine}

This chapter centers around two transformations on logical
specifications.  Taken together, the operationalization transformation
(Section~\ref{sec:operationalization}), and the defunctionalization
transformation (Section~\ref{sec:defunctionalization}) allow us to
establish the logical correspondence between the deductive SLS
specification of a natural semantics and the concurrent SLS
specification of an abstract machine.

Natural semantics specifications are common in the literature,
and are also easy to encode in either the deductive fragment of
\sls~or in a purely deductive logical framework like LF.  We will
continue to use the natural semantics specification of call-by-value
evaluation for the lambda calculus as a running example:
\[
\infer[{\sf ev/lam}]
{\lambda x. e \Downarrow \lambda x. e \mathstrut}
{}
\quad
\infer[{\sf ev/app}]
{e_1\,e_2 \Downarrow v \mathstrut}
{e_1 \Downarrow \lambda x.e
 &
 e_2 \Downarrow v_2
 &
 [v_2/x]e \Downarrow v \mathstrut}
\]

Abstract machine semantics are less prevalent than natural
semantics. The most well-known is almost certainly Landin's SECD
machine \cite{landin64mechanical}, though our abstract machine
presentation is much more similar to Danvy's SC machine from
\cite{danvy03rational} and Harper's $\mathcal K\{{\sf
  nat}{\rightharpoonup}\}$ system from \cite[Chapter
27]{harper12practical}.  This abstract machine semantics is defined in
terms of states $s$. The state $s = k \rhd e$ represents the
expression $e$ being evaluated on top of the stack $k$, and the state
$s = k \lhd v$ represents the value $v$ being returned to the stack
$k$. Stacks $k$ are sequences of frames $f_i$ with the form
$((\ldots({\sf halt}; f_1); \ldots); f_n)$, and each frame $f$ either
has the form $\Box\,e_2$ (an application frame waiting for an
evaluated function to be returned to it) or the form $(\lambda
x.e)\,\Box$ (an application frame with an evaluated function waiting
for an evaluated value to be returned to it). Given states, stacks,
and frames, we can define a ``classical'' abstract machine for
call-by-value evaluation of the lambda calculus as a transition system
with four transition rules:
\begin{align*}
{\sf absmachine/lam}{:} & ~~ k \rhd \lambda x.e ~ \mapsto ~ k \lhd \lambda x.e
\\
{\sf absmachine/app}{:} & ~~ k \rhd e_1\,e_2 ~ \mapsto ~ (k; \Box\,e_2) \rhd e_1
\\
{\sf absmachine/app1}{:} & ~~ 
  (k; \Box\,e_2) \lhd \lambda x.e ~ \mapsto ~ (k; (\lambda x.e)\,\Box) \rhd e_2
\\
{\sf absmachine/app2}{:} & ~~
  (k; (\lambda x.e)\,\Box) \lhd v_2 ~ \mapsto ~ k \rhd [v_2/x]e
\end{align*}

The operational intuition for these rules is precisely the same as the
operational intuition for the rewriting rules given in
Section~\ref{sec:intro-ssos}. This is not coincidental: the
\sls~specification from the introduction adequately encodes the
transition system $s \mapsto s'$ defined above, a point that we will
make precise in Section~\ref{sec:nat-ssos-adequacy}. The
\sls~specification from the introduction is {\it also} the result of
applying the operationalization and defunctionalization
transformations to the \sls~encoding of the natural semantics given
above, so the these two transformations combined with the adequacy
arguments at either end constitute a logical correspondence between
natural semantics and abstract machines. 

As discussed in Section~\ref{sec:the-point-is-modular-extension}, it
is interesting to put existing specification styles into logical
correspondence, but that is not our main reason for investigating
logical correspondence. Instead, we are primarily interested in
exploring the set of programming language features that can be
modularly integrated into a transformed \sls~specification that could
not be integrated into a natural semantics specification.  In
Section~\ref{sec:richer-ordered-abstract} we explore a selection of
these features, including mutable storage, call-by-need evaluation,
and recoverable failure.

\section{Logical transformation: operationalization}
\label{sec:operationalization}

The intuition behind operationalization is rather simple: we examine
the behavior of a deductive computation and then encode that
operational intuition as a concurrent computation.  Before presenting
the general transformation, we will motivate this transformation using
our natural semantics specification of call-by-value-evaluation. 

The definition of $e \Downarrow v$ is moded with $e$ as an input and
$v$ as an output, so it is meaningful to talk about being given $e$
and using deductive computation to search for a $v$ such that $e
\Downarrow v$ is derivable.  Consider a recursive search procedure
implementing this particular deductive computation:
\begin{itemize}
\item
      If $e = \lambda x. e'$, 
      it is possible to derive 
      $\lambda x. e' \Downarrow \lambda x. e'$
      with the rule ${\sf ev/lam}$.
\item
       If $e = e_1\,e_2$,
       attempt to derive 
       $e_1\,e_2 \Downarrow v$
       using the rule ${\sf ev/app}$ by doing the following:
    \begin{enumerate}
    \item Search for a $v_1$ such that 
          $e_1 \Downarrow v_1$ is derivable.
    \item Assert that $v_1 = \lambda x.e'$ for some
          $e'$; fail if it is not.
    \item Search for a $v_2$ such that 
          $e_2 \Downarrow v_2$ is derivable.
    \item Search for a $v$ such that 
          $[v_2/x]e \Downarrow v$ is derivable.
    \end{enumerate}
% \item
%       If $e = e_1 \arb e_2$,
%       attempt to derive $e_1 \arb e_2 \Downarrow v$ using
%       the rule ${\sf ev/choose1}$ by searching for a 
%       $v$ such that $e_1 \Downarrow v$ is derivable.
% \item
%       If $e = e_1 \arb e_2$,
%       attempt to derive $e_1 \arb e_2 \Downarrow v$ using 
%       the rule ${\sf ev/choose2}$ by searching for a 
%       $v$ such that $e_2 \Downarrow v$ is derivable.  
\end{itemize}
%
The goal of the operationalization transformation is to implement this
deductive computation as a concurrent computation. The first step in
doing so is to introduce two new ordered atomic propositions.  The
proposition ${\sf eval}\,\interp{e}$ is the starting point, indicating
that we want to search for a $v$ such that $e \Downarrow v$, and the
proposition ${\sf retn}\,\interp{v}$ indicates the successful
completion of this procedure. Therefore, searching for a $v$ such that
$e \Downarrow v$ is derivable will be analogous to building a trace $T
:: x_e{:}\susp{{\sf eval}\,\interp{e}} \leadsto^* x_v{:}\susp{{\sf
    retn}\,\interp{v}}$ with concurrent computation.

Representing the first case is straightforward: if we are evaluating
$\lambda x.e$, then we have succeeded and can return $\lambda x.e$. 
This is encoded as the following proposition:
\[
{\sf eval}\,({\sf lam}\,\lambda x.\,E\,x)
   \lefti \{ {\sf retn}\,({\sf lam}\,\lambda x.\,E\,x) \}
\]
Because the natural deduction rule ${\sf ev/app}$ 
involves both recursion and multiple subgoals,
we will generalize our picture of the process state to allow us to store a
stack of unfinished work in the ordered context, growing out to the
right. Our new understanding, then, is that contexts either have the
form $x{:}\susp{{\sf eval}\,\interp{e}}, \Delta$ or the form $x{:}\susp{{\sf
  retn}\,\interp{v}}, \Delta$. In the process of concurrently
computing a trace $x_e{:}\susp{{\sf eval}\,\interp{e_1\,e_2}}, \Delta
\leadsto^* x_v{:}\susp{{\sf retn}\,\interp{v}}, \Delta$, each of the
recursive calls to the search procedure will involve a sub-trace of the
form
%
\[x_e{:}\susp{{\sf eval}\,\interp{e'}}, y{:}\istrue{A^-}, \Delta
  \leadsto^*
  x_v{:}\susp{{\sf retn}\,\interp{v'}}, y{:}\istrue{A^-}, \Delta\]
%
  where $A^-$ is a negative proposition that is prepared to interact
  with the subgoal's final ${\sf retn}\,\interp{v'}$ proposition to
  kickstart the rest of the computation.

It's helpful to work backwards: in the fourth step, we have found
$\lambda x.\,E\,x = \lambda x.\interp{e}$ (where $e$ potentially has $x$ free) and $V_2 =
\interp{v_2}$. The recursive call is to ${\sf
  eval}\,\interp{[v_2/x]e}$, which is the same thing as ${\sf
  eval}\,(E\,V_2)$. If the recursive call successfully returns, the
context will contain a suspended atomic proposition of the form ${\sf
  retn}\,V$ where $V = \interp{v}$, and the search procedure as a
whole is complete: the answer is $v$.  Thus, the negative proposition
that implements the continuation can be written as $(\forall V.\,{\sf
  retn}\,V \lefti \{ {\sf retn}\,V \})$. The positive proposition that
will create this sub-computation can be written as follows:
\begin{align*}
{\it Step_4}(E,V_2) & \equiv {\sf eval}\,(E\,V_2) 
\fuse {\downarrow}(\forall V.\, {\sf retn}\,V \lefti \{ {\sf retn}\,V \})
%
\intertext{Moving backwards, in the third step we have a $E_2 =
  \interp{e_2}$ that we were given and $\lambda x.\,E\,x = \lambda x.\interp{e}$ that we
  have computed. The recursive call is to ${\sf
    eval}\,\interp{e_2}$, and assuming that it completes, we need
  to begin the fourth step. The positive proposition that will 
  create this sub-computation can be written as follows:}
%
{\it Step_3}(E_2,E) & \equiv {\sf eval}\,E_2 
\fuse {\downarrow}(\forall V_2.\,
  {\sf retn}\,V_2 \lefti \{ {\it Step_4}(E,V_2) \})
%
\intertext{Finally, the first two steps can be handled together. We have
$E_1 = \interp{e_1}$ and $E_2 = \interp{e_2}$; the recursive
call is to ${\sf eval}\,\interp{e_1}$. Once the
recursive call completes, we can enforce that the returned value has
the form $\interp{\lambda x.e}$ before proceeding
to the continuation.}
{\it Step_{1,2}}(E_1, E_2) & \equiv {\sf eval}\,E_1
\fuse {\downarrow}(\forall E.\, {\sf retn}\,({\sf lam}\,\lambda x.\,E\,x)
\lefti \{ {\it Step_3}(E_2, E)\})
\end{align*}
Thus, the rule implementing this entire portion of the search
procedure is 
\[
\forall E_1.\,\forall E_2.\,
{\sf eval}\,({\sf app}\,E_1\,E_2) \lefti \{ {\it
  Step_{1,2}}(E_1, E_2) \}
\]
The \sls~encoding of our example natural semantics is shown in
Figure~\ref{fig:example-transform-cbv} alongside the transformed
specification, which has the form of an ordered abstract machine
semantics, though it is different than the ordered abstract machine
semantics presented in the introduction. We say the specification
above is {\it higher-order}, as ${\sf ev/app}$ is a rule that, when it
participates in a transition, produces a new rule $(\forall E.\,{\sf
  retn}\,({\sf lam}\,\lambda x.\,E\,x) \lefti \{ \ldots \})$ that
lives in the context. (In contrast, the ordered abstract machine
semantics from the introduction was {\it first-order}.)  We discuss
the defunctionalization transformation, which allows us to derive
first-order specifications from specifications that are higher-order
in this way, in Section~\ref{sec:defunctionalization} below.

\begin{figure}
\begin{minipage}[b]{0.36\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-ev.sls}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.64\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-ev-ssos.sls}
\end{minipage}
\caption{A natural semantics for CBV (left) and the corresponding (higher-order)
  ordered abstract machine (right).}
\label{fig:example-transform-cbv}
\end{figure}

The intuitive connection between natural semantics specifications and
concurrent specifications has been explored previously and
independently in the context of CLF by Schack-Nielsen
\cite{schacknielsen07induction} and by Cruz and Hou
\cite{cruz12parallel}; Schack-Nielsen proves the equivalence of the
two specifications, whereas Cruz and Favonia used the connection
informally. The contribution of this section is to describe a general
transformation (of which Figure~\ref{fig:example-transform-cbv} is one
instance) and to prove the transformation correct in general. I have
implemented the operationalization transformation within my prototype
typechecker for \sls~specifications, whereas the defunctionalization 
transformation is not implemented and will be treated somewhat more
informally.

In Section~\ref{sec:trans-subset} we will present the subset of
specifications that our operationalization transformation handles, and
in Section~\ref{sec:trans-basic} we present the most basic form of the
transformation.  In
Sections~\ref{sec:trans-tail}~and~\ref{sec:trans-par} we extend the
basic transformation to be both tail-recursion optimizing and
parallelism-enabling. Finally, in
Section~\ref{sec:operationalization-correct}, we establish the
correctness of the overall transformation.

\subsection{Transformable signatures}
\label{sec:trans-subset}

The starting point for the operationalization transformation is a
deductive signature that is well-moded in the sense described in
Section~\ref{sec:framework-modes}. Every declared negative predicate
will either remain defined by deductive proofs (we write these
predicates as ${\sf ad}$ and write the atomic propositions built with
these predicates as $p_d^-$, $d$ for deductive) or will be transformed
so that it is concurrently defined (we write these predicates as ${\sf
  ac}$ and write the atomic propositions built with these predicates
as $p_c^-$, $c$ for concurrent).

For the purposes of describing and proving the correctness of the
operationalization transformation, we will assume that all transformed
atomic propositions $p_c^-$ have two arguments where the first
argument is moded as an input and the second is an output. That is,
they are declared as follows:
\begin{align*}
& {\sf \#mode~a~{+}~{-}}.\\
& {\sf ac} : \tau_1 \rightarrow \tau_2 \rightarrow {\sf prop}.
\end{align*}
Without dependency, two-place relations are sufficient for describing
$n$-place relations.\footnote{As an example, to handle addition on
  natural numbers, defined as a three-place relation ${\sf add} : {\sf
    nat} \rightarrow {\sf nat} \rightarrow {\sf nat} \rightarrow {\sf
    type}$ with its usual mode (${\sf add}~{+}~{+}~{-}$), we define a
  unique type ${\sf add\_in}$ with one binary constructor ${\sf
    add\_c} : {\sf nat} \rightarrow {\sf nat} \rightarrow {\sf
    add\_in}$. Then we can declare (${\sf add'} : {\sf add\_in}
  \rightarrow {\sf nat} \rightarrow {\sf type}$) with mode (${\sf
    add'}~{+}~{-}$).}  It should be possible to handle dependent
predicates (that is, those with declarations of the form ${\sf ac} :
\Pi x{:}\tau_1.\,\tau_2(x) \rightarrow {\sf type}$), but we will not do
so here.

The restriction on signatures furthermore enforces that all rules must
be of the form ${\sf r} : C$ or ${\sf r} : D$, where $C$ and $D$ are
refinements of the negative propositions of \sls~that are defined as
follows:
\begin{align*}
C & ::= p^-_{c} 
    \mid \forall x{:}\tau.\, C
    \mid p^+_\mpers \lefti C
    \mid {!}p^-_c \lefti C
    \mid {!}G \lefti C \\
D & ::= p^-_{d}
    \mid \forall x{:}\tau.\, D
    \mid p^+_\mpers \lefti D
    \mid {!}p^-_c \lefti D
    \mid {!}G \lefti C \\
G & ::= p^-_d 
    \mid \forall x{:}\tau.\, G
    \mid p^+_\mpers \lefti G
    \mid {!}D \lefti G
\end{align*}
If {\it all} propositions are to remain deductive, then the
propositions $p^-_c$ and $C$ are irrelevant, and this restriction
describes all persistent, deductive specifications -- essentially, any
signature that could be executed by the standard logic programming
interpretation of LF \cite{pfenning89elf}. On the other hand, if all
propositions are to be transformed, then the propositions $p^-_d$ and
$D$ are irrelevant and this restriction amounts to restricting
rules to the Horn fragment.

All propositions $C$ are equivalent (at the level of synthetic
inference rules) to propositions of the form $\forall
\overline{x_0}\ldots \forall \overline{x_n}.\,A^+_n \lefti \ldots
\lefti A^+_1 \lefti {\sf ac}\,t_{0}\,t_{n+1}$, where the $\forall
\overline{x_i}$ are shorthand for a series of universal quantifiers
$\forall {x_{i1}}{:}{\tau_{i1}} \ldots \forall {x_{\it
    ik}}{:}{\tau_{\it ik}}$ and where each variable in
$\overline{x_i}$ does not appear in $t_0$ (unless $i = 0$) nor in any
$A^+_j$ with $j < i$ but does appear in $A^+_i$ (or $t_0$ if $i =
0$). Therefore, when we consider moded proof search, the variables
bound in $\overline{x_0}$ are all fixed by the query and those bound
in the other $\overline{x_i}$ are all fixed by the output position of
the $i^{\rm th}$ premise.

\subsection{Basic transformation}
\label{sec:trans-basic}

The operationalization transformation $\transop{\Sigma}$
operates on SLS signatures $\Sigma$ that have the form described in the
previous section. We
will first give the transformation on signatures; the transformation
of rule declarations ${\sf r} : C$ is the key case.

Each two-place predicate ${\sf ac}$ gets turned into two one-place
predicates ${\sf eval\_a}$ and ${\sf retn\_a}$: both ${\sf
  eval\_a}\,t$ and ${\sf retn\_a}\,t$ are positive ordered atomic
propositions.  We will write $\opsubst{X}$ for the operation of
substituting all occurrences of $p^-_c = {\sf ac}\,t_1\,t_2$ with
$({\sf eval\_a}\,t_1 \lefti \{ {\sf retn\_a}\,t_2 \})$ in $X$. This
substitution operation is used on propositions, contexts, and frames;
it appears in the transformation of rules ${\sf r} : D$ below.

\begin{itemize}
\item $\transop{\cdot} = \cdot$
\item $\transop{\Sigma, {\sf ac} : \tau_1 \rightarrow \tau_2
    \rightarrow {\sf prop}} = \transop{\Sigma}, ~ {\sf eval\_a} :
  \tau_1 \rightarrow {\sf prop\,ord}, ~ {\sf retn\_a} : \tau_2
  \rightarrow {\sf prop\,ord}$ 
\item $\transop{\Sigma, {\sf b} : K} = \transop{\Sigma}, ~ {\sf b}
  : K$ {\it (if ${\sf b} \neq {\sf ac}$)}
\item $\transop{\Sigma, {\sf c} : \tau} = \transop{\Sigma}, ~ {\sf
    c} : \tau$ 
\item $\transop{\Sigma, {\sf r} : C} = \transop{\Sigma}, ~ {\sf r}
  : \forall \overline{x_0}.\, {\sf eval\_a}\,t_0 \lefti \llbracket A^+_1,
  \ldots, A^+_n \rrbracket (t_{n+1}, {\sf id})$ \\ {\it (where $C$ is
    equivalent to $\forall \overline{x_0}\ldots \forall
    \overline{x_n}.\, A^+_n \lefti \ldots \lefti A^+_1 \lefti {\sf
      ac}\,t_{0}\,t_{n+1}$)}
\item $\transop{\Sigma, {\sf r} : D} = \transop{\Sigma}, ~ {\sf r}
  : \opsubst{D}$
\end{itemize}

The transformation of a proposition $C = \forall
\overline{x_0}\ldots \forall \overline{x_n}.\,A^+_n \lefti \ldots
\lefti A^+_1 \lefti {\sf ac}\,t_{0}\,t_{n+1}$ involves the definition
$\opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma}$, where $\sigma$
substitutes only for variables in $\overline{x_j}$ where $j < i$. The
function is defined inductively on the length of the sequence
$A^+_i,\ldots,A^+_n$.

\begin{itemize}
\item $\opbasic{}{t_{n+1}}{\sigma} = \{ {\sf retn\_a}\,(\sigma{t_{n+1}}) \}$
\item $\opbasic{p^+_\mpers,A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma} 
  = \forall \overline{x_i}.\, (\sigma{p^+_\mpers}) \lefti \opbasic{A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma}$
\item $\opbasic{{!}p^-_c,A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma}$
  \\
  $~ \qquad = \{ {\sf eval\_b}\,({\sigma}t^{\it in}_i) \fuse
  (\forall\overline{x_i}.\, {\sf retn\_b}\,(\sigma{t^{\it out}_i})
  \lefti \opbasic{A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma}) \}$\\
  {\it (where $p^-_c$ is ${\sf bc}\,t^{\it in}_i\,t^{\it out}_i$)}
\item $\opbasic{{!}G,A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma} = \forall
  \overline{x_i}.\, {!}(\sigma\opsubst{G}) \lefti
  \opbasic{A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma}$
\end{itemize}

\noindent
This operation is slightly more general than it needs to be to
describe the transformation on signatures, where the substitution
$\sigma$ will always just be the identity substitution ${\sf id}$.
Non-identity substitutions arise during the proof of correctness, which
is why we introduce them here.

We have already given an example of the this basic operationalization
transformation, as Figure~\ref{fig:example-transform-cbv} is an
instance of this transformation.

\subsection{Tail-recursion}
\label{sec:trans-tail}

Consider again our motivating example, the procedure for that takes
expressions $e$ and searches for expressions $v$ such that $e
\Downarrow v$ is derivable. If we were to implement that procedure as
a functional program, the procedure would be {\it tail-recursive}. In
the procedure that handles the case when $e = e_1\,e_2$, the last step
is that the search procedure is invoked recursively. If and when that
callee returns $v$, then the caller will also return $v$.

Tail-recursion is significant in functional programming because
tail-recursive calls can be implemented without allocating a stack
frame: when a compiler makes this more efficient choice, we say it is
performing {\it tail-recursion optimization}.\footnote{Or {\it tail-call optimization}, as a tail-recursive function call is just a
  specific instance of a tail call.} An analogous opportunity for
tail-recursion optimization also arises in our logical compilation
procedure. In our motivating example, the last step in the $e_1\,e_2$
case was operationlized as a positive proposition of the form ${\sf
  eval}\,(E\, V) \fuse (\forall v.\,{\sf retn}\,v \lefti \{ {\sf
  retn}\,v \})$. In a successful search, the process state 
\[ x{:}{\sf
  eval}\,(E\, V), y{:}\istrue{(\forall v.\,{\sf retn}\,v \lefti \{
  {\sf retn}\,v \})}, \Delta\]
will concurrently compute until the
state 
\[ x'{:}{\sf retn}\,V, y{:}\istrue{(\forall v.\,{\sf retn}\,v \lefti
  \{ {\sf retn}\,v \})}, \Delta\] is reached, at which point the next
step \[y'{:}{\sf retn}\,V, \Delta\] is reached in one step by focusing
on $y$. 

If we operationalize the last step in the $e_1\,e_2$ case as ${\sf
  eval}\,(E\,V)$ instead of as ${\sf eval}\,(E\, V) \fuse (\forall
v.\,{\sf retn}\,v \lefti \{ {\sf retn}\,v \})$, we will reach the same
final state with one less transition. The tail-recursion optimizing
version of the operationalization transformation creates concurrent
computations that avoid these useless steps.

We cannot perform tail recursion in general because the output of the
last subgoal may be different from the output of the goal. For example,
the rule ${\sf r} : \forall{x}.\,\forall{y}.\,{!}{\sf a}\,x\,y \lefti
{\sf a}\,({\sf c}\,x)\,({\sf c}\,y)$ will translate to
\[ {\sf r} : \forall{x}.\,{\sf eval\_a}\,({\sf c}\,x) \lefti \{ {\sf
  eval\_a}\,x \fuse (\forall y.\, {\sf retn\_a}\,y \lefti \{ {\sf
  retn\_a}\,({\sf c}\,y) \} ) \} \] There is no opportunity for
tail-recursion optimization, because the output of the last search
procedure, $t^{\it out}_n = y$, is different than the value returned
down the stack, $t_{n+1} = {\sf c}\,y$. This case corresponds to
functional programs that cannot be tail-call optimized.

More subtly, we cannot even eliminate all cases where $t^{\it out}_n =
t_{n+1}$ unless these terms are {\it fully general}. We say that
$t_{n+1}$ with type $\tau$ is fully general if all of its free
variables are in $\overline{x_n}$ (and therefore not fixed by the
input of any other premise) and if, for any variable-free term $t'$ of
type $\tau$, there exists a substitution $\sigma$ such that $t =
{\sigma}t_{n+1}$. The simplest way to ensure this is to require
that $t_{n+1} = t^{\it out}_n = y$ where $y =
\overline{x_n}$.\footnote{It is also possible to have a fully general
  $t_{n+1} = {\sf c}\,y_1\,y_2$ if, for instance, ${\sf c}$ has type
  $\tau_1 \rightarrow \tau_2 \rightarrow {\sf foo}$ and there are no
  other constructors of type ${\sf foo}$. However, we also have to
  check that there are no other first-order variables in $\Psi$ with
  types like $\tau_3 \rightarrow {\sf foo}$ that could be used to make
  other terms of type ${\sf foo}$. The technology to handle this,
  worlds checking and subordination analysis, is well-understood and
  surveyed elsewhere \cite{harper07mechanizing}, but this is
  tangential to the current discussion.} This condition doesn't have
an analogue in functional programming, because it corresponds to the
possibility that moded deductive computation can perform pattern
matching on {\it outputs} and fail if the pattern match fails.

The tail-recursive procedure can be described by adding a new 
case to the definition of 
$\opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma}$:

\begin{itemize}
\item $\opbasic{{!}{\sf a}\,t^{\it in}_n\,t_{n+1}}{t_{n+1}}{\sigma} 
  = \{{\sf eval\_a}\,({\sigma}{t^{\it in}_n})\}$
\\
  {\it (where $t_{n+1}$ is fully general)}
\end{itemize}
This case overlaps with the third case of the definition given
in Section~\ref{sec:trans-basic}, which indicates that tail-recursion
optimization can be applied or not in a nondeterministic manner.

\begin{figure}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-ev-ssos-tail.sls}
\caption{Tail-recursion optimized semantics for the CBV
  evaluation.}
\label{fig:cbv-ev-ssos-tail}
\end{figure}

\subsubsection{Example}

Operationalizing the natural semantics from
\ref{fig:example-transform-cbv} with tail-recursion optimization gives
us the ordered abstract machine in Figure~\ref{fig:cbv-ev-ssos-tail}.
For a dramatic illustration of tail-call optimization, consider a
definition of
big-step evaluation that is based on a small-step structural
operational semantics (SOS) specification. In SOS specifications,
single-step evaluation is the two-place relation ${\sf step} : {\sf
  exp} \rightarrow {\sf exp} \rightarrow {\sf prop}$ (moded ${\sf
  exp}\,{+}\,{-}$) that makes use of the helper judgment ${\sf value}
: {\sf exp} \rightarrow {\sf prop}$ (moded ${\sf value}\,{+}$). We
will not define these propositions here, but we do so later on in
Section~\ref{sec:evaluationcontexts}.

Given the definition of ${\sf step}\,\interp{e}\,\interp{e'}$, it is
easy to define big-step evaluation ${\sf ev}\,\interp{e}\,\interp{v}$
as a series of small steps:

\smallskip
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-sos-steps.sls}
\smallskip

\begin{figure}
\begin{minipage}[b]{0.55\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-sos-proc2.sls}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-sos-proc.sls}
\end{minipage}
\caption{The transformation of a trivial big-step semantics, both
  without (left) and with (right) tail-recursion optimization.}
\label{fig:sos-tailrecursion}
\end{figure}

If we run this specification through the operationalization
transformation and only operationalize the ${\sf ev}$ predicate,
the result is
results in what I consider to be the most boring substructural
operational semantics specification. 
Figure~\ref{fig:sos-tailrecursion} presents the resulting specification
 both without the tail-recursion
optimization (left) and with the tail-recursion optimization (right).

\begin{figure}
\begin{align*}
& x_1{:}\susp{{\sf eval}\,\interp{e_1}} 
\\
\leadsto ~ & x_2{:}\susp{{\sf eval}\,\interp{e_2}}, 
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} 
\\
\leadsto ~ & x_3{:}\susp{{\sf eval}\,\interp{e_3}}, 
  y_2{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})}, 
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} 
\\
\leadsto ~ & \cdots
\\
\leadsto ~ & x_n{:}\susp{{\sf eval}\,\interp{v}}, 
  y_{n-1}{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})}, 
  \cdots,
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} 
\\
\leadsto ~ & z_n{:}\susp{{\sf retn}\,\interp{v}}, 
  y_{n-1}{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})}, 
  \cdots,
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} 
\\
\leadsto ~ & \cdots 
\\
\leadsto ~ & z_3{:}\susp{{\sf retn}\,\interp{v}}, 
  y_2{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})}, 
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} 
\\
\leadsto ~ & z_2{:}\susp{{\sf retn}\,\interp{v}}, 
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} \\
\leadsto ~ & z_1{:}\susp{{\sf retn}\,\interp{v}}
\end{align*}
\caption{Example trace with the non-tail-recursion-optimized
  semantics in Figure~\ref{fig:sos-tailrecursion}.}
\label{fig:example-proc-non-tail-recursive-trace}
\end{figure}

The tail-recursion optimized translation is definitely superior for
this example. Concurrent proofs for the non-tail-recursion-optimized
specification build up an enormous stack of useless copies of the
proposition $(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})$,
as shown in Figure~\ref{fig:example-proc-non-tail-recursive-trace}.
In contrast, the tail-recursion optimized version on the right hand
side of Figure~\ref{fig:sos-tailrecursion} takes half as many steps,
and each step is smaller, simpler, and the overall trace does a better
job of capturing the linear computation that is involved in performing
evaluation using a small-step structural operational semantics:
\[
x_1{:}\susp{{\sf eval}\,\interp{e_1}}
 ~\leadsto~
x_2{:}\susp{{\sf eval}\,\interp{e_2}}
 ~\leadsto~ \cdots ~\leadsto~
x_n{:}\susp{{\sf eval}\,\interp{v}}
 ~\leadsto~ 
z{:}\susp{{\sf retn}\,\interp{v}}
\]

\subsection{Parallelism}
\label{sec:trans-par}

Both the basic transformation and the tail-recursive transformation
are sequential: if $x{:}{\sf eval}\,\interp{e} \leadsto^* \Delta$,
then the process state $\Delta$ contains at most one proposition ${\sf
  eval}\,\interp{e'}$ or ${\sf retn}\,\interp{v}$ that can potentially
be a part of any further transition. Put differently, the first two
operationalization transformations express deductive computation as a
concurrent computation that does not exhibit concurrency (sequential
computation being a special case of concurrent computation).

Sometimes, this is what we want: in
Section~\ref{sec:nat-ssos-adequacy} we will see that the sequential
tail-recursion-optimized abstract machine 
adequately represents a traditional on-paper abstract machine for
the call-by-value lambda calculus. In general, however, when distinct
subgoals do not have input-output dependencies (that is, when none of
subgoal $i$'s outputs are inputs to subgoal $i+1$), deductive computation
can search for subgoal $i$ and $i+1$ simultaneously, and this can 
be represented in the operationalization transformation.

In the previous transformations, our process states were structured
such that every negative proposition $A^-$ was waiting on a single
${\sf retn}$ to be computed to its left; at that point, the negative
proposition could be focused on, effectively invoking the
continuation stored in that negative proposition. If we ignore the
first-order structure of the concurrent computation, these
intermediate states look like this:
\[
  (\mbox{subgoal 1}), y{:}\istrue{({\sf retn} \lefti {\it cont})}
\]
Note that $(\mbox{subgoal 1})$ is intended to represent some nonempty
sequence of ordered propositions, not a single proposition. With the
parallelism-enabling transformation, subgoal 1 can even be performing
parallel search for its own subgoals:
\[
 (\mbox{subgoal 1.1}), (\mbox{subgoal 1.2}), 
   y_1{:}\istrue{({\sf retn}_{1.1} \fuse {\sf retn}_{1.2} \lefti {\it cont}_1)}, 
   y{:}\istrue{({\sf retn} \lefti {\it cont})}
\]
The two subcomputations $(\mbox{subgoal 1.1})$ and $(\mbox{subgoal
  1.2})$ are next to one another in the ordered context, but the
structure of transformed specifications ensures that the only way they
can interact is if they both finish (becoming $z_{1.1}{:}\susp{{\sf
    retn}_{1.1}}$ and $z_{1.2}{:}\susp{{\sf retn}_{1.2}}$), which will
allow us to focus on $y_1$ and begin working on the continuation ${\it
  cont}_1$. The principle at work is the same one that ensures that postfix
notations like Reverse Polish notation are unambiguous: there's always
only one way to reconstruct the tree of subgoals. 

To allow for the transformed programs to have parallelism, we again
add a new case to the function that transforms propositions $C$ in the
signature.  In this case, the new case will subsume the old
case that dealt with sequences of the form ${!}p_c^-,
A^+_{i+1},\ldots,A^+_n$; that old case is now an instance of the
general case where $i = j$. 

\begin{itemize}
\item $\opbasic{{!}p^-_{ci},\ldots,{!}p^-_{cj},A^+_{j+1},\ldots,A^+_n}{t_{n+1}}{\sigma}$
  \\
  $~ \qquad = \{ {\sf eval\_bi}\,({\sigma}t^{\it in}_i) 
                    \fuse \ldots \fuse
                 {\sf eval\_bj}\,({\sigma}t^{\it in}_j) \fuse ~$
  \\
  $~ \qquad \qquad (\forall\overline{x_i}\ldots\forall\overline{x_j}.\, 
     {\sf retn\_bi}\,(\sigma{t^{\it out}_i})
     \fuse \ldots \fuse 
     {\sf retn\_bj}\,(\sigma{t^{\it out}_j})$
  \\
  $~ \qquad \qquad \quad
   \lefti \opbasic{A^+_{j+1},\ldots,A^+_n}{t_{n+1}}{\sigma}) \}$\\
  {\it (where
   $p^-_{ck}$ is ${\sf bk}\,t^{\it in}_k\,t^{\it out}_k$ 
   and $FV(t_k^{\it in}) \notin (\overline{x_i} \cup \ldots \cup \overline{x_j})$ 
   for $i \leq k \leq j$)}
\end{itemize}

\noindent
Note that the second side condition on the free variables of inputs is
necessary if the resulting term is to be well-scoped, and is trivially 
satisfied in the sequential case where $i = j$. 

\begin{figure}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-ev-ssos-par.sls}
\caption{The parallel, tail-recursion optimized ordered abstract machine for
 call-by-value evaluation.}
\label{fig:cbv-ev-ssos-par}
\end{figure}

The result of running the natural semantics from
Figure~\ref{fig:example-transform-cbv} through the parallel and
tail-recursion optimizing ordered abstract machine is shown in
Figure~\ref{fig:cbv-ev-ssos-par}; it shows that we can
search for the subgoals $e_1 \Downarrow \lambda x.e$ and
$e_2 \Downarrow v_2$ in parallel. We cannot, of course, run either
of these subgoals in parallel with the third subgoal 
$[v_2/x]e \Downarrow v$ because the input $[v_2/x]e$ mentions the outputs
of both of the previous subgoals. 

\subsection{Correctness}
\label{sec:operationalization-correct}

The correctness of the basic, tail-recursion-optimizing, and parallel
transformations follows from the correctness of the parallel
transformation; because the transformation is nondeterministic, the
previously presented transformations are just instances of this most
general one. 

Correctness is fundamentally the property that
we have $\slss{\Sigma}{\Psi}{\Gamma}{\susp{p^-_d}}$ if and only if 
$\slss{\transop{\Sigma}}{\Psi}{\opsubst{\Gamma}}{\susp{p^-_d}}$
and $\slss{\Sigma}{\Psi}{\Gamma}{\susp{{\sf ac}\,t_1\,t_2}}$ if and only if
$(\Psi; \mkconj{\Gamma}{{\sf eval\_a}\,t_1}) \leadsto^*_{\transop{\Sigma}} (\Psi; \mkconj{\Gamma}{{\sf retn}\,({\sf retn\_a}\,t_2)})$, we label the
forward direction ``completeness'' and the backward direction ``soundness'' but
that assignment is (as usuall) somewhat arbitrary. Soundness is a corollary
of Theorem~\ref{thm:opersound}, and completeness is a corollary of 
Theorem~\ref{thm:opercomp}. We use Theorem~\ref{thm:operlf} pervasively
and usually without mention. 

\bigskip
\begin{theorem}[No effect on the LF fragment]\label{thm:operlf}
  $\Psi \vdash_\Sigma t : \tau$ if and only if $\Psi
  \vdash_{\transop{\Sigma}} t : \tau$.
\end{theorem}

\begin{proof}
Straightforward induction in both directions; the transformation 
leaves the LF-relevant part of the signature unchanged.
\end{proof}

Soundness under the parallel translation requires that we be able
to manipulate traces into other (permutively equivalent) traces. 
\begin{align*}
R & ::= \forall \overline{x}.\,{\sf retn\_b1}\,t_1 \fuse \ldots \fuse {\sf retn\_bn}\,t_n \lefti S
\\
S & ::= \forall x{:}\tau.\,S 
   \mid p^+_\mpers \lefti S
   \mid {!}A^- \lefti S
   \mid \{ {\sf eval\_b1}\,t_1 \fuse \ldots \fuse {\sf eval\_bn}\,t_n 
           \fuse {\downarrow}R \}
   \mid \{ {\sf eval\_b}\,t \} 
\end{align*}

\noindent
Every concurrent rule in a transformed signature
$\transop{\Sigma}$ has the form ${\sf r} : \forall \overline{x}.\,
{\sf eval\_b}\,t \lefti S$. 

\bigskip
\begin{theorem}[Rearrangement]\label{thm:rearrangement}~
If $\Delta$ contains only atomic propositions, persistent propositions
of the form $D$, and ordered 
propositions of the form $R$,
then 
\begin{enumerate}
\item If $\Delta$ matches 
$\frameoff{\Theta}
 {\matchconj
  {x_1{:}\susp{{\sf retn\_b1}\,t_1}}
  {\matchconj
    {\ldots}
    {\matchconj 
      {x_n{:}\susp{{\sf retn\_bn}\,t_n}}
      {y{:}{(\forall \overline{x}.\,{\sf retn\_b1},s_1 \fuse \ldots \fuse {\sf retn\_bn}\,s_n \lefti S)}}}}}$ and 
$T :: (\Psi; \Delta) \leadsto^*_{\transop{\Sigma}}
(\Psi; z{:}\susp{{\sf retn\_z}\,t_z})$, then 
$T \equiv \tstep{p}{y}{(\tforalll{\overline{u}}{(\tappl{(\tfuser{x_1}{\tfuser{\ldots}{x_n}})}{\Sp})})}; T'$
where $(\overline{u}/\overline{x})s_i = t_i$ for $1 \leq i \leq n$.
\item If $\Delta$ matches
$\frameoff{\Theta}
 {y{:}\susp{{\sf eval\_b}\,t}}$ and 
$T :: (\Psi; \Delta) \leadsto^*_{\transop{\Sigma}}
(\Psi; z{:}\susp{{\sf retn\_z}\,t_z})$, 
then $T \equiv \tstep{p}{\sf r}{(\tforalll{\overline{u}}{(\tappl{y}{\Sp})})}; T'$
where 
${\sf r} : \forall \overline{x}.\,{\sf eval\_b}\,s \lefti S 
\in \transop{\Sigma}$.
\end{enumerate}
\end{theorem}

\begin{proof}
By induction over the structure of traces.

The transitions described {\it must} happen at some point: the
negative proposition associated with $y$ in part 1 and the suspended
${\sf eval}$ proposition in part 2 are not in the final state $(\Psi;
z{:}\susp{{\sf retn\_z}\,t_z})$, so the base cases where $T =
\emptytrace$ is vacuous. The actual base case is when first step 
focuses on $y$ (part 1) or consumes $y$ by focusing on some 
rule in the transformed signature (part 2), in which case the 
result follows immediately. 

The inductive case is when the first step occurs by focusing on 
some 
$y'{:}\istrue{R}$ in $\Delta$ that is distinct from $y$ (part 1)
or on some
transformed rule from the signature that does not first 
consume $x$ (part 2). It is necessary to show that this step will not
consume $y$ or, in part 1, any $x_i{:}\susp{{\sf retn\_bi}\,t_i}$ (part 1),
which allows us to invoke the induction hypothesis and permute the
step we're looking for to the beginning of the subtrace. We then have
to check that the first step doesn't introduce any variables or resources
that can be consumed by the step we're looking for (which is now the second
step); this is immediate from the structure of $R$, $S$, and transformed
signatures. 
\end{proof}

\begin{theorem}[Soundness of operationalization]\label{thm:opersound}
If all propositions in $\Gamma$ have the form 
$x{:}D$ or $z{:}\susp{p^+_{\mpers}}$, then
\begin{enumerate}
\item If $\slss{\transop{\Sigma}}{\Psi}{\opsubst{\Gamma}}{\susp{p^-_d}}$,
then $\slss{\Sigma}{\Psi}{\Gamma}{\susp{p^-_d}}$.
\item 
\item 
\item 
\end{enumerate}
\end{theorem}

\begin{proof}
XXX Proof
\end{proof}

\begin{theorem}[Completeness of operationalization]\label{thm:opercomp}
If all propositions in $\Gamma$ have the form 
$x{:}D$ or $z{:}\susp{p^+_{\mpers}}$, then
\begin{enumerate}
\item  
If $\slss{\Sigma}{\Psi}{\Gamma}{\susp{p^-_d}}$,
then $\slss{\transop{\Sigma}}{\Psi}{\opsubst{\Gamma}}{\susp{p^-_d}}$.
\item  
If $\slss{\Sigma}{\Psi}{\Gamma, [D]}{\susp{p^-_d}}$,
then $\slss{\transop{\Sigma}}{\Psi}{\opsubst{\Gamma}, [\opsubst{D}]}{\susp{p^-_d}}$.
\item  
If $\slss{\Sigma}{\Psi}{\Gamma}{G}$,
then $\slss{\transop{\Sigma}}{\Psi}{\opsubst{\Gamma}}{\opsubst{G}}$.
\item
If $\Delta$ matches $\frameoff{\Theta}{\Gamma}$ 
and $\slss{\Sigma}{\Psi}{\Gamma}{\susp{p^-_c}}$
(where $p^-_c = {\sf ac}\,t\,s$),\\
then
$(\Psi; \tackon{\opsubst{\Theta}}{x{:}\susp{{\sf eval\_a}\,t}}) 
  \leadsto^*_{\transop{\Sigma}}
 (\Psi; \tackon{\opsubst{\Theta}}{y{:}\susp{{\sf retn\_a}\,s}})$.
\end{enumerate}
\end{theorem}

\begin{proof}
Mutual induction on the size 
of the input derivation.

The first three parts are straightforward. In part 1, we have
$\slst{\Sigma}{\Psi}{\Gamma}{\tfocusl{h}{\Sp}}{\susp{p^-_d}}$ where
either $h = x$ and $x{:}D \in \Gamma$ or else $h = {\sf r}$ and ${\sf
  c}{:}D \in \Sigma$. In either case the necessary result is
$\tfocusl{h}{\Sp'}$, where we get $\Sp'$ from the induction hypothesis
(part 2) on $\Sp$.

In part 2, we proceed by case analysis on the proposition $D$ in focus. 
The only interesting case is where $D = {!}p^-_c \lefti D'$
\begin{itemize}
\item If $D = p_d^-$, then $\Sp = \tnil$ and $\tnil$ gives the desired result.

\item If $D = \forall x{:}\tau.\,D'$ or $D = p^+_{\sf
    pers} \lefti D'$, then $\Sp = (\tforalll{t}{\Sp'})$ 
  or $\Sp = (\tappl{z}{\Sp'})$ (respectively). The necessary result is
  $(\tforalll{t}{\Sp''})$ 
  or $(\tappl{z}{\Sp''})$ (respectively) where we get $\Sp''$ from the
  induction hypothesis (part 2) on $\Sp'$. 

\item If $D = {!}p^-_c \lefti D'$ and $p^-_c = {\sf ac}\,t_1\,t_2$, then 
  $\Sp = (\tappl{\tbangr{\tetan{N}}}{\Sp'})$
  and $\opsubst{D} = {!}({\sf eval\_a}\,t_1 \lefti \{ {\sf retn\_a}\,t_2 \}) \lefti \opsubst{D'}$.

  \begin{tabbing}
  $\slst{\Sigma}{\Psi}{\Gamma}{N}{\susp{{\sf ac}\,t_1\,t_2}}$
  \` (given)
  \\
  $\slst{\Sigma}{\Psi}{\Gamma, [D]}{\Sp'}{\susp{p^-_d}}$
  \` (given)
  \\
  $T :: (\Psi; \opsubst{\Gamma}, x{:}{\sf eval\_a}\,t_1)
    \leadsto^*_{\transop{\Sigma}} (\Psi; \opsubst{\Gamma}, y{:}{\sf retn\_a}\,t_2)$
  \` (ind. hyp. (part 4) on $N$)
  \\
  $\slst{\transop{\Sigma}}{\Psi}{\Gamma, [\opsubst{D'}]}{\Sp''}{\susp{p_d^-}}$
  \` (ind. hyp. (part 2) on $\Sp'$)
  \\
  $\slst{\transop{\Sigma}}{\Psi}{\opsubst{\Gamma}}
    {\tlaml{\tetap{x}{\,\tlet{T}{y}}}}
    {{\sf eval\_a}\,t_1 \lefti \{ {\sf retn\_a}\,t_2 \}}$
  \` (construction)
  \\
  $\slst{\transop{\Sigma}}{\Psi}{\opsubst{\Gamma}, [\opsubst{D}]}
    {\tappl{\tbangr{(\tlaml{\tetap{x}{\,\tlet{T}{y}}})}}{\Sp'}}
    {\susp{p^-_d}}$
  \` (construction)
  \end{tabbing}
\item If $D = {!}G \lefti D'$, then $\Sp =
  (\tappl{\tbangr{\tetan{N}}}{\Sp'})$. The necessary result is
  $(\tappl{\tbangr{\tetan{N'}}}{\Sp''})$; we get $N'$ from the
  induction hypothesis (part 3) on $N$ and get $\Sp''$ from the induction
  hypothesis (part 2) on $\Sp'$.
\end{itemize}

The cases of part 3 are straightforward invocations of the induction
hypothesis (part 1 or part 3). For instance, if $G = {!}D \lefti G'$
then we have a derivation of the form $\tlaml{\tbangl{x}{N}}$
where $\slst{\Sigma}{\Psi}{\Gamma, x{:}\ispers{D}}{N}{G'}$. By the
induction hypothesis (part 3) we have
$\slst{\transop{\Sigma}}{\Psi}{\mkconj{\opsubst{\Gamma}}
 {x{:}\ispers{\opsubst{D}}}}{N'}{\opsubst{G'}}$, and we conclude by
constructing $\tlaml{\tbangl{x}{N'}}$.

In part 4, we have $\slst{\Sigma}{\Psi}{\Gamma}{\tfocusl{\sf
    c}{\Sp}}{\susp{p^-_d}}$, where ${\sf r}{:}C \in \Sigma$ and the
proposition $C$ is equivalent to
$\forall{\overline{x_0}}\ldots\forall{\overline{x_n}}.\, A^+_n \lefti
\ldots \lefti A^+_1 \lefti {\sf ac}\,t_0\,t_{n+1}$ as described in
Section~\ref{sec:trans-basic}. This means that, for each $0 \leq
i \leq n$, we can decompose $\Sp$ to
get $\sigma_i = (\overline{s_0}/\overline{x_0},\ldots,
\overline{s_i}/\overline{x_i})$ (for some terms $\overline{s_0} \ldots
\overline{s_i}$ that correspond to the correct variables) and 
we have a value
$\slst{\Sigma}{\Psi}{\Gamma}{V_i}{[\sigma_i{A^+_i}]}$. 
We also have $t = \sigma_0{t_0}$ and $s = \sigma_n{t_{n+1}}$.

Because 
${\sf r}{:}\forall\overline{x_0}.\,{\sf eval\_a}\,t_0 \lefti \opbasic{A_1, \ldots, A_n}{t_{n+1}}{{\sf id}} \in \transop{\Sigma}$, by left-focusing
on that constant
it suffices to show that
there is a $\Sp'$ such that 
$\slst{\transop\Sigma}{\Psi}{\opsubst{\Gamma},[
\opbasic{A_1^+, \ldots, A_n^+}{t_{n+1}}{\sigma_0}
]}{\Sp'}{\susp{\{C^+\}}}$ and a trace of the form
$T :: (\Psi; \tackon{\opsubst{\Theta}}{C^+}) \leadsto^*_{\transop{\Sigma}}
 (\Psi; \tackon{\opsubst{\Theta}}{y{:}{\sf retn\_a}\,({\sigma_n}t_{n+1})})$. 
We will prove this
by induction; the general statement is that for any sequence $A_i,\ldots,A_n$ 
there is a $\Sp'$ such that 
$\slst{\transop\Sigma}{\Psi}{\Gamma,[
\opbasic{A_i^+, \ldots, A_n^+}{t_{n+1}}{\sigma_{i-1}}
]}{\Sp'}{\susp{\{C^+\}}}$ and a trace of the form
$T :: (\Psi; \tackon{\opsubst{\Theta}}{C^+}) \leadsto^*_{\transop{\Sigma}}
 (\Psi; \tackon{\opsubst{\Theta}}{y{:}{\sf retn\_a}\,({\sigma_n}t_{n+1})})$. We proceed
by case analysis on the definition of the operationalization transformation:
\begin{itemize}
\item $\opbasic{}{t_{n+1}}{\sigma_n} = \{ {\sf retn\_a}\,(\sigma_{n}{t_{n+1}}) \}$

  \bigskip
  This is a base case: 
  let $\Sp' = \tnil$. Because 
  $(\Psi; \tackon{\opsubst{\Theta}}{{\sf retn\_a}\,(\sigma_{n}{t_{n+1}})})$ decomposes
  to $(\Psi; 
  \tackon{\opsubst{\Theta}}{y{:}{\susp{{\sf retn\_a}\,(\sigma_{n}{t_{n+1}})}}})$,
  we are done.
  \bigskip

\item $\opbasic{{!}{\sf ac}\,t^{\it in}_n\,t_{n+1}}{t_{n+1}}{\sigma_{n-1}} 
  = \{{\sf eval\_a}\,(\sigma_{n-1}{t^{\it in}_n})\}$

  \bigskip
  We are given a value 
  $\slst{\Sigma}{\Psi}{\Gamma}{\tbangr{N}}
   {[{\bang}{\sf ac}\,\sigma_n{t^{\it in}_n}\,\sigma_n{t_{n+1}}]}$;
  observe that $\sigma_{n-1}{t_n^{\it in}} = \sigma_n{t_n^{\it in}}$.

  \smallskip
  This is also a base case: let $\Sp' = \tnil$. The process state
  $(\Psi; 
  \tackon{\opsubst{\Theta}}
  {{\sf eval\_a}\,(\sigma_{n}{t^{\it in}_n})})$
  decomposes to 
  $(\Psi; 
  \tackon{\opsubst{\Theta}}
  {x_{n}{:}{\susp{{\sf eval\_a}\,(\sigma_{n}{t^{\it in}_n})}}})$, so we must
  demonstrate a trace the rest of the way to
  $(\Psi; \tackon{\opsubst{\Theta}}{y{:}{\sf retn\_a}\,s})$. 
  This follows from the
  outer induction hypothesis (part 4) on $N$. 
  \bigskip

\item $\opbasic{p^+_\mpers,A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}} 
  = \forall \overline{x_i}.\, \sigma_{i-1}{p^+}_\mpers \lefti \opbasic{A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}}$

  \begin{tabbing}
  $\slst{\Sigma}{\Psi}{\Gamma}{z}{[\sigma_i{p^+_\mpers}]}$
  \` (given) 
  \\
  $\sigma_i = (\sigma_{i-1}, \overline{s_i}/\overline{x_i})$.
  \` (definition of $\sigma_i$)
  \\
  $\slst{\transop{\Sigma}}{\Psi}{\Gamma, [\opbasic{A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{i}}]}{\Sp'}{\susp{\{ C^+ \} }}$
  \` (by inner ind. hyp.)
  \\
  $T :: (\Psi; \tackon{\opsubst{\Theta}}{C^+}) \leadsto^*_{\transop{\Sigma}}
   (\Psi; \tackon{\opsubst{\Theta}}{y{:}{\sf retn\_a}\,({\sigma_n}t_{n+1})})$
  \` (by inner ind. hyp.)
  \\
  $\slst{\transop{\Sigma}}{\Psi}
    {\Gamma, [\forall \overline{x_i}.\, \sigma_{i-1}{p^+}_\mpers 
                \lefti \opbasic{A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}}]}
    {\left(\tforalll{\overline{s_i}}{\tappl{z}{\Sp'}}\right)}{\susp{\{ C^+ \}}}$
  \\ 
  \` (construction)
  \end{tabbing}

\item $\opbasic{{!}p^-_{ci},\ldots,{!}p^-_{cj},A^+_{j+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}}$
  \\
  $~ \qquad = \{ {\sf eval\_bi}\,({\sigma_{i-1}}t^{\it in}_i) 
                    \fuse  \ldots \fuse
                 {\sf eval\_bj}\,({\sigma_{i-1}}t^{\it in}_j) \fuse ~$
  \\
  $~ \qquad \qquad (\forall\overline{x_i}\ldots\forall\overline{x_j}.\, 
     {\sf retn\_bi}\,(\sigma_{i-1}{t^{\it out}_i})
     \fuse \ldots \fuse 
     {\sf retn\_bj}\,(\sigma_{i-1}{t^{\it out}_j})$
  \\
  $~ \qquad \qquad \quad
   \lefti \opbasic{A^+_{j+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}}) \}$\\
  {\it (where
   $p^-_{ck}$ is ${\sf bk}\,t^{\it in}_k\,t^{\it out}_k$ 
   and $FV(t_k^{\it in}) \notin (\overline{x_i} \cup \ldots \cup \overline{x_j})$ 
   for $i \leq k \leq j$)}

  \bigskip
  Let $\Sp = \tnil$. By composing the positive proposition, it 
  suffices to show that there is a trace
\begin{align*}
    &(\Psi, \opsubst{\Theta} \tackonstart
        x_i{:}\susp{{\sf eval\_bi}\,({\sigma_{i-1}}t^{\it in}_i)}, \ldots,
    x_j{:}\susp{{\sf eval\_bj}\,({\sigma_{i-1}}t^{\it in}_j)},
  \\
  & \qquad\qquad y_{ij}{:}(\forall\overline{x_i}\ldots\forall\overline{x_j}.\, 
     {\sf retn\_bi}\,(\sigma_{i-1}{t^{\it out}_i})
     \fuse \ldots \fuse 
     {\sf retn\_bj}\,(\sigma_{i-1}{t^{\it out}_j})\\
  & \qquad\qquad\qquad
      \lefti \opbasic{A^+_{j+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}})\,\mtrue
       \tackonstop)
  \\
  & \quad \leadsto^*_{\transop{\Sigma}} 
     (\Psi; \tackon{\opsubst{\Theta}}{y{:}\susp{{\sf retn\_a}\,({\sigma_n}t_{n+1})}})
\end{align*}

  \begin{tabbing}
  $\slst{\Sigma}{\Psi}{\Gamma}{\tbangr{N_k}}{[{!}{{\sf bk}\,({\sigma_k}t_k^{\it in})\,({\sigma_k}t_k^{\it out})}]}$ \quad $(i \leq k \leq j)$
  \` (given) 
  \\
  $\slst{\Sigma}{\Psi}{\Gamma}{\tbangr{N_k}}{[{!}{{\sf bk}\,({\sigma_{i-1}}t_k^{\it in})\,({\sigma_j}t_k^{\it out})}]}$ \quad $(i \leq k \leq j)$
  \` (condition on translation, defn. of $\sigma_k$)
  \\
  $T :: (\Psi$\=$, \opsubst{\Theta} \tackonstart
        x_i{:}\susp{{\sf eval\_bi}\,({\sigma_{i-1}}t^{\it in}_i)}, \ldots,
    x_j{:}\susp{{\sf eval\_bj}\,({\sigma_{i-1}}t^{\it in}_j)},$\\
  \>$~ \qquad y_{ij}{:}(\forall\overline{x_i}\ldots\forall\overline{x_j}.\, 
     {\sf retn\_bi}\,(\sigma_{i-1}{t^{\it out}_i})
     \fuse \ldots \fuse 
     {\sf retn\_bj}\,(\sigma_{i-1}{t^{\it out}_j})$\\
  \>$~ \qquad\qquad
      \lefti \opbasic{A^+_{j+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}})\,\mtrue
       \tackonstop)$\\
  $~ \qquad \leadsto^*_{\transop{\Sigma}} 
        (\Psi$\=$, \opsubst{\Theta} \tackonstart
        y_i{:}\susp{{\sf retn\_bi}\,({\sigma_{j}}t^{\it out}_i)}, \ldots,
    y_j{:}\susp{{\sf retn\_bj}\,({\sigma_{j}}t^{\it out}_j)},$\\
  \>$~ \qquad y_{ij}{:}(\forall\overline{x_i}\ldots\forall\overline{x_j}.\, 
     {\sf retn\_bi}\,(\sigma_{i-1}{t^{\it out}_i})
     \fuse \ldots \fuse 
     {\sf retn\_bj}\,(\sigma_{i-1}{t^{\it out}_j})$\\
  \>$~ \qquad\qquad
      \lefti \opbasic{A^+_{j+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}})\,\mtrue
       \tackonstop)$\\
  \` (by outer ind. hyp. (part 4) on each of the $N_k$ in turn)
  \\
  $\slst{\transop{\Sigma}}{\Psi}
     {\Gamma,[\opbasic{A^+_{j+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{j}}]}
     {\Sp'}{\susp{\{ C^+ \}}}$  \` (by inner ind. hyp.)
  \\
  $T' :: (\Psi, \tackon{\opsubst{\Theta}}{C^+}) \leadsto^*_{\transop{\Sigma}}
        (\Psi, \tackon{\opsubst{\Theta}}{y{:}\susp{{\sf retn\_a}\,s}})$ 
   \` (by inner ind. hyp.)
  \end{tabbing}

  The construction 
  $\left(T; \tstep{\mkpat{C^+}}{y_{ij}}{(\tforalll{\overline{s_i}\ldots\overline{s_j}}{\tappl{(\tfuser{y_i}{\tfuser{\ldots}{y_j}})}{\Sp'}})}; T'\right) $
  is then a trace of the correct type.
  \bigskip

\item $\opbasic{{!}G,A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}} = \forall
  \overline{x_i}.\, {!}\sigma_{i-1}\opsubst{G} \lefti
  \opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}}$

  \begin{tabbing}
  $\slst{\Sigma}{\Psi}{\Gamma}{\tbangr{N}}{[{!}\sigma_i{G}]}$
  \` (given) 
  \\
  $\slst{\transop{\Sigma}}{\Psi}{\opsubst{\Gamma}}{N'}{\sigma_i{\opsubst{G}}}$
  \` (by outer ind. hyp. (part 3)  on $N$) 
  \\
  $\sigma_i = (\sigma_{i-1}, \overline{s_i}/\overline{x_i})$.
  \` (definition of $\sigma_i$)
  \\
  $\slst{\transop{\Sigma}}{\Psi}{\Gamma, [\opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma_{i}}]}{\Sp'}{\susp{\{ C^+ \} }}$
  \` (by inner ind. hyp.)
  \\
  $T :: (\Psi; \tackon{\opsubst{\Theta}}{C^+}) \leadsto^*_{\transop{\Sigma}}
   (\Psi; \tackon{\opsubst{\Theta}}{y{:}{\sf retn\_a}\,s})$
  \` (by inner ind. hyp.)
  \\
  $\slst{\transop{\Sigma}}{\Psi}
    {\Gamma, [\forall \overline{x_i}.\, {!}(\sigma_{i-1}{G})
                \lefti \opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma_{i-1}}]}
    {\left(\tforalll{\overline{s_i}}{\tappl{\tbangr{N}}{\Sp'}}\right)}{\susp{\{ C^+ \}}}$
  \\ 
  \` (construction)
  \end{tabbing}
\end{itemize}

\noindent
This completes the inner induction in the fourth part, and hence
the proof.
\end{proof}

\section{Logical transformation: defunctionalization}
\label{sec:defunctionalization}

Defunctionalization is a procedure for turning higher-order
concurrent \sls~specifications into first-order concurrent
\sls~specifications. It is based on the following intuitions:
if $A^-$ is a closed negative proposition
of the form $\forall \overline{x}.\,A^+_1 \lefti \{ A^+_2 \}$
and we have a single-step transition 
$(\Psi; \tackon{\Theta}{y{:}\istrue{A^-}}) 
 \leadsto_{\Sigma} 
 (\Psi; \Delta')$
in an \sls~specification (witnessed by the step 
$(\tstep{\mkpat{A^+_2}}{y}{(\tforalll{\overline{t}}{\tappl{V}{\tnil}})})$), 
then we can define an augmented signature
\begin{align*}
\Sigma' = ~ & \Sigma, 
\\    ~~ & {\sf cont} : {\sf prop\,ord}, 
\\    ~~ & {\sf run\_cont} : \forall{\overline x}.\,p^+_\mtrue \fuse {\sf cont} \lefti \{ A^+ \}
\end{align*}
and it is the case that 
$(\Psi; \tackon{\Theta}{y{:}\susp{\sf cont}}) 
 \leadsto_{\Sigma'} 
 (\Psi; \Delta')$
as well; this new transition is witnessed by the step
$(\tstep{\mkpat{A^+}}{\sf run\_cont}{(\tappl{(\tfuser{V}{y})}{\tnil})})$.

More generally, if we are allowed to extend the signature and $A^-$
falls into the very specific form we have
specified,\footnote{Obviously, the restriction to propositions $A^-$
  of the form $\forall \overline{x}.\,A^+ \lefti \{ A^+ \}$ is
  overly specific and designed to apply specifically to the output of
  operationalization, but we will not consider a generalization here.}
%  Conceptually, it is not complicated to consider a similar operation
%  on other propositions, but it is difficult to elegantly describe the
%  more general transformation due our use of ordered logic.}  
we can create
a new ordered atomic proposition to do a negative proposition's
job. As long as $\Delta = \tackon{\Theta}{x{:}\susp{\sf cont}}$ and
${\sf cont}$ does not appear in $\Theta$, then $[{\downarrow}A^- /{\sf
  cont}]\Delta \leadsto_\Sigma [{\downarrow}A^- /{\sf cont}]\Delta'$
if and only if $\Delta \leadsto_{\Sigma'} \Delta'$. \footnote{Recall
  from Section~\ref{sec:framework-substprop} that we treat
  %
  $[{\downarrow}A^-/{\sf cont}](\tackon{\Theta}{z{:}\istrue{\susp{\sf cont}}})$
  %
  as being equal to the context in which we {\it first} perform the
  straightforward substitution, giving us
  $(\tackon{\Theta}{z{:}\istrue{{\downarrow}A^-}})$, and then {\it
    second} apply invertible rules, giving us
  $(\tackon{\Theta}{z'{:}\istrue{A^-}})$.} 

We need not restrict ${\sf cont}$ to just a single appearance
suspended in the process state.  It is
similarly unproblematic for ${\sf cont}$ to appear in the monadic head
of some other rule in the process state, as the appearance of an
ordered atomic proposition in a monadic head will not effect the
existence of any transition, but may cause the ordered atomic
proposition to become a suspended ordered proposition in the process
state after the transition. 

By the same reasoning, it is similarly
unproblematic for ${\sf cont}$ to appear in the head of a rule in the
signature.  Therefore, we can replace propositions in the monadic heads
of rules in the signature, like this one:
\begin{align*}
& \Sigma, \\
& {\sf r} : {\sf a} \lefti \{ {\sf b} \fuse {\uparrow}({\sf c} \lefti \{ {\sf d} \fuse {\uparrow}({\sf e} \lefti \{ {\sf f} \}) \}) \}
\intertext{to produce a signature that looks like this:}
& \Sigma, \\
& {\sf cont1} : {\sf prop\,ord}, \\
& {\sf r1} : {\sf c} \fuse {\sf cont1} \lefti \{ {\sf d} \fuse {\uparrow}({\sf e} \lefti \{ {\sf f} \}) \}, \\
& {\sf r} : {\sf a} \lefti \{ {\sf b} \fuse {\sf cont1} \}
\intertext{and the process can be iterated to obtain 
a fully first-order signature:}
& \Sigma, \\
& {\sf cont2} : {\sf prop\,ord}, \\
& {\sf r2} : {\sf e} \fuse {\sf cont2} \lefti \{ {\sf f} \}, \\
& {\sf cont1} : {\sf prop\,ord}, \\
& {\sf r1} : {\sf c} \fuse {\sf cont1} \lefti \{ {\sf d} \fuse {\sf cont2} \}, \\
& {\sf r} : {\sf a} \lefti \{ {\sf b} \fuse {\sf cont1} \}
\end{align*}
This transformation is similar to the one proposed by
Miller in~\cite{miller02higherorder}, where new propositions were
introduced and locally quantified to hide the internal states of processes.

We can go further and allow $A^-$ to contain free variables if
$A^- = [t_1/y_1]\ldots[t_m/x_m]B^-$ where $B^- = \forall
\overline{x}.\,B_1^+ \lefti \{ B_2^+ \}$ has only the variables
$\overline{y} = y_1\ldots y_m$ free.  In this more general case, we
can revise the signature as follows:
\begin{align*}
\Sigma'' = ~ & \Sigma,
\\    ~~ & {\sf cont} : 
       \Pi y_1{:}\tau_1\ldots \Pi y_m{:}\tau_m.\, {\sf prop\,ord},
\\    ~~ & {\sf run\_cont} : \forall \overline{x}.\,\forall \overline{y}.\,
       p^+_\mtrue \fuse {\sf cont}\,\overline{y} \lefti \{ B^+ \}
\end{align*}
With this revision, we maintain that
%
$[{\downarrow}B^-/{\sf cont}\,\overline{x}]\Delta \leadsto_{\Sigma}
[{\downarrow}B^-/{\sf cont}\,\overline{x}]\Delta'$ if and only if
$\Delta \leadsto_{\Sigma''} \Delta'$ as long as propositions of the
form ${\sf cont}\,\overline{t}$ only appear suspended in the process
state or in the monadic heads of rules that appear in the process
state.\robnote{The process of proving this is mostly an issue of
  stating it precisely, which is a pain. I'd appreciate feedback as to
  whether this seems clear or whether I need to write out the detailed
  proof.}

The one twist we make to the defunctionalization transformation is
that, instead of introducing a new ordered atomic proposition ${\sf
  cont}\,\overline{t}$ for each iteration of the defunctionalization
procedure, we introduce a single type $({\sf frame} : {\sf type})$ and a
single atomic proposition $({\sf cont} : {\sf frame} \rightarrow {\sf
  prop\,ord})$. Then, each iteration of the defunctionalization
procedure produces a new constant with type $\Pi y_1{:}\tau_1\ldots
\Pi y_m{:}\tau_m.\, {\sf frame}$ instead of a new atomic proposition
with kind $\Pi y_1{:}\tau_1\ldots \Pi y_m{:}\tau_m.\, {\sf
  prop\,ord}$.  Operationally, these two approaches are equivalent.

\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/cbv-ev-ssos-fun.sls}
\caption{A first-order ordered abstract machine semantics for CBV
  evaluation.}
\label{fig:cbv-ev-ssos-fun}
\end{figure}

Using the defunctionalization procedure outlined above, we obtain the
first-order specification in Figure~\ref{fig:cbv-ev-ssos-fun} from the
higher-order specification in Figure~\ref{fig:cbv-ev-ssos-tail}, which
was in turn derived from the natural semantics for CBV evaluation by
operationalization with tail-recursion optimization.

% As long as 
% $({\sf cont}\,t_1\ldots t_n)$ only appears in $\Delta$ as a 
% suspended atomic proposition, then it is the case that
% $[{\downarrow}(B^-\,x_1\ldots x_n)
%     /{\sf cont}\,x_1\ldots x_n]\Delta 
%  \leadsto_\Sigma
%  [{\downarrow}(B^-\,x_1\ldots x_n)
%     /{\sf cont}\,x_1\ldots\,x_n]\Delta'$ 
% if and only if 
% %
% $\Delta \leadsto_{\Sigma''} \Delta'$.\footnote{Recall from
%   Section~\ref{sec:framework-substprop} that
%   $[{\downarrow}(B^-\,x_1\ldots x_n)/({\sf cont}\,x_1\ldots
%   x_n)](z{:}\susp{{\sf cont}\,t_1\ldots t_n})$ as being equal to the
%   context in which we substitue and then apply invertible rules, i.e.
%   $z{:}\istrue{B^-\,t_1\ldots t_n}$}

% In addition to allowing these newly introduced 
% atomic propositions to appear suspended in the context, it is not 
% a problem to allow them to appear in the heads of monadic clauses. 
% This means that we can 

%  monadic
% clauses, there is no 

% The defunctionalization transformation then applies the same reasoning
% to signatures: if a proposition ${\downarrow}A^-$ appears in the monadic
% head of some rule, 

%  $A^- = B^-\,t_1\,t_2\,t_3$

% This is even
% true if $A^-$ has free variables: we can always define a closed
% $B^- : $




\section{Adequacy with abstract machines}
\label{sec:nat-ssos-adequacy}

I\robnote{Some of this discussion should be moved to Chapter 5 and rephrased
in terms of the PDA example.} 
claim that the four-rule abstract machine specification given at the
beginning of this chapter is adequately represented by the derived
\sls~specification in Figure~\ref{fig:cbv-ev-ssos-fun}. For terms and
for deductive computations, adequacy is a well-understood concept: we
know what it means to define an adequate encoding function $\interp{e}
= t$ from ``on-paper'' terms $e$ with (potentially) variables
$x_1,\ldots,x_n$ free to LF terms $t$ where $x_1{:}{\sf
  exp},\ldots,x_n{:}{\sf exp} \vdash t : {\sf exp}$, and we know what
it means to adequately encode the judgment $e \Downarrow v$ as a
negative atomic \sls~proposition ${\sf ev}\,\interp{e}\,\interp{v}$
and to encode derivations of this judgment to \sls~terms $N$ where
$\slst{\Sigma}{\cdot}{\cdot}{N}{\susp{{\sf
      ev}\,\interp{e}\,\interp{v}}}$
\cite{harper93framework,harper07mechanizing}. What does it mean to
adequately represent machine states as process states (that is,
substructural contexts) and to encode a transition system as a 
concurrent \sls~specification? 

The answer given in the literature by Cervesato et
al.~\cite{cervesato02concurrent} and by
Schack-Nielsen~\cite{schacknielsen07induction} has three steps. The
first step is to define an interpretation function from states $s$
and stacks $k$ to process states $\Delta$, so that, for example, the
state
\[
((\ldots({\sf halt}; \Box\,e_1)\ldots); (\lambda x.e_n)\,\Box) \lhd v
\]
is interpreted as the process state
\[
y{:}\susp{{\sf retn}\,\interp{v}}, ~~
x_n{:}\susp{{\sf cont}\,({\sf app2}\,\lambda x.\interp{e_n})}, ~~
\ldots, ~~
x_1{:}\susp{{\sf cont}\,({\sf app1}\,\interp{e_1})}, ~~
\]
The second step is to prove a preservation-like adequacy theorem. Let
$\Sigma\ref{fig:cbv-ev-ssos-fun}$ be the signature from
Figure~\ref{fig:cbv-ev-ssos-fun}: we show that if state $s$ is
interpreted and $\Delta$ and $\Delta
\leadsto_{\Sigma\ref{fig:cbv-ev-ssos-fun}} \Delta'$, then there is a
state $s'$ such that $s'$ is interpreted as $\Delta'$. Then we can
prove the main adequacy result: that the interpretation of state $s$
steps to the interpretation of state $s'$ if and only if $s \mapsto
s'$.

I believe that the approach to adequacy given in previous work is
unsatisfactory because the interpretation of process states into
contexts is 1-to-1 but not onto (and therefore not invertible).  This
means that there is no {\it internal} notion of what it means for a
process state to encode a state $s$ or a stack $k$. By analogy,
``having type ${\sf exp}$'' captures what it means for an LF term
encode an expression and ``having type ${\sf
  ev}\,\interp{e}\,\interp{v}$'' captures what it means for an
\sls~term to encode a derivation of $e \Downarrow v$.

In this section, we will present a different three-part approach that
addresses this perceived deficiency. First, we create a signature
$\Sigma\sf gen$ that encodes well-formed states: the $\Delta$ such
that $x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf gen} \Delta$ and ${\sf
  gen} \notin \Delta$ are in a bijection with the states $s$
(Section~\ref{sec:nat-ssos-adequacy-gen}). This gives us the internal
notion of what it means to encode a process state, which is what we
were previously lacking. Second, we prove the preservation-like
property from before. The difference is that this can now be stated
formally as a property of \sls~specifications: if $x{:}\susp{\sf gen}
\leadsto^*_{\Sigma\sf gen} \Delta$ and $\Delta
\leadsto_{\Sigma\ref{fig:cbv-ev-ssos-fun}} \Delta'$, then $x{:}
\susp{\sf gen} \leadsto^*_{\Sigma\sf gen} \Delta'$
(Section~\ref{sec:nat-ssos-adequacy-pres}). The structure of this
theorem is critical, a point that we will consider in greater depth in
Part III of this thesis. Finally, the third step is the same as it was
in other approaches: we prove that the interpretation of state $s$
steps to the interpretation of state $s'$ if and only if $s \mapsto s'$.

\subsection{Adequacy of states}
\label{sec:nat-ssos-adequacy-gen}

Our first goal is to describe a signature $\Sigma\sf gen$ with the
property that if $x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf gen}
\Delta$ and ${\sf gen} \notin \Delta$ then $\Delta$ encodes a state
$s$. A well-formed process state that represents an abstract machine
state $((\ldots({\sf halt}; f_1); \ldots); f_n) \rhd e$ has the form
\[
y{:}\susp{{\sf eval}\,\interp{e}}, ~~
x_n{:}\susp{{\sf cont}\,\interp{f_n}}, ~~
\ldots, ~~
x_1{:}\susp{{\sf cont}\,\interp{f_1}}
\]
where $\interp{\Box\,e_2} = {\sf app1}\,\interp{e_2}$ and
$\interp{(\lambda x.e)\,\Box} = {\sf app2}\,(\lambda x.\interp{e})$. 
A well-formed process state representing a state $k \lhd v$ has 
the same form, but with a suspended ${\sf retn}\,\interp{v}$ instead
of ${\sf eval}\,\interp{e}$. 

The simplest \sls~signature that encodes this structure essentially
has the structure of a CNF grammar for describing well-formed
contexts, with two unary productions ${\sf gen/eval}$ and ${\sf gen/retn}$
and one binary production ${\sf gen/cont}$.

\smallskip
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/cbv-ev-ssos-gen.sls}
\smallskip

\noindent In addition to the four declarations above, the full
signature $\Sigma\sf gen$ includes all the type, proposition, and
constant declarations from Figure~\ref{fig:cbv-ev-ssos-fun}, but none
of the rules.

Note that this specification is most definitely {\it not} well-moded.
{\it Generative signatures} such as this one are not generally moded,
and we don't think about traces under these signatures as 
necessarily being concurrent computations in the same way we think
about ordered abstract machines as encoding concurrent computations. 
Rather than traces in these signatures being produced by
concurrent computation, they are produced 
and manipulated by the constructive content of
theorems like the ones in this section.

\bigskip
\begin{theorem}[Adequacy of states]~
\label{thm:adequacy-states}
\begin{itemize}
\item There is a bijection (up to the renaming of variables in the context) 
  between states $s$ and contexts $\Delta$ such that
  $x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf gen} \Delta$ where 
  ${\sf gen} \notin \Delta$.
\item There is a bijection (up to the renaming of variables in the context) 
  between stacks $k$ and frames $\Theta$ such that $x{:}\susp{\sf
    gen} \leadsto^*_{\Sigma\sf gen} \tackon{\Theta}{x'{:}{\sf gen}}$.
\end{itemize}
\end{theorem}

\begin{proof}
We will give only the translation from the ``on paper''
semantic artifacts (states $s$ and stacks $k$) to traces:
\begin{itemize}
\item $\interp{s},$ which outputs 
a trace $T$ with type $x{:}\susp{\sf gen} \leadsto_{\Sigma\sf gen} \Delta$
where ${\sf gen} \not\in \Delta$, and 
\item $\interp{k}$, which outputs two things: first, a trace $T$ with
  type $x{:}\susp{\sf gen} \leadsto_{\Sigma\sf gen}
  \tackon{\Theta}{x'{:}\susp{\sf gen}}$ where ${\sf gen} \not\in
  \Delta$; and second, the variable name $x'$ of the resulting ${\sf
    gen}$ proposition (which may be the same as $x$). Rather than
  representing this output explicitly, we just assume it is always
  named $x'$ in the definition below.
\end{itemize}
Note that both functions build contexts only indirectly by building 
traces; similarly, the inverses of these functions are defined by induction
on the structure of traces, not on the structure of contexts.
\begin{tabbing}
~~ \= \qquad\quad\qquad \= $~ :: ~$ \=\kill
\> $\interp{k \rhd e} = \interp{k}; 
     \tstep{z}{\sf gen/eval}{(\tforalll{\interp{e}}{(\tappl{x'}{\tnil})})}$
\\ \>\> $~ :: ~$ 
  \> $x{:}\susp{\sf gen} 
       \leadsto^*_{\Sigma\sf gen} \tackon{\Theta}{x'{:}\susp{\sf gen}} 
       \leadsto_{\Sigma\sf gen} 
          \tackon{\Theta}{z{:}\susp{{\sf eval}\,\interp{e}}}$
\\[4pt]
\> $\interp{k \lhd v} = \interp{k}; 
     \tstep{z}{\sf gen/retn}{(\tforalll{\interp{v}}{(\tappl{x'}{\tnil})})}$
\\ \>\> $~ :: ~$ 
  \> $x{:}{\sf gen} 
       \leadsto^*_{\Sigma\sf gen} \tackon{\Theta}{x'{:}\susp{\sf gen}} 
       \leadsto_{\Sigma\sf gen} 
          \tackon{\Theta}{z{:}\susp{{\sf retn}\,\interp{v}}}$
\\[4pt]
\> $\interp{{\sf halt}} = \emptytrace$
\> $~ :: ~$
  \> $x{:}\susp{\sf gen} \leadsto^* x{:}\susp{\sf gen}$
\\[4pt]
\> $\interp{k; \Box\,e_2} = \interp{k}; \tstep{z, x''}{\sf gen/cont}
     {(\tforalll{{\sf app1}\,\interp{e_2}}{(\tappl{x'}{\tnil})})}$
\\ \>\> $~ :: ~$
 \> $x{:}\susp{\sf gen}
       \leadsto^*_{\Sigma\sf gen} \tackon{\Theta}{x'{:}\susp{\sf gen}}
       \leadsto_{\Sigma\sf gen} \tackon{\Theta}
            {\mkconj
               {z{:}\susp{{\sf cont}\,({\sf app1}\,\interp{e_2})}}
               {x''{:}\susp{\sf gen}}}$
\\[4pt]
\> $\interp{k; (\lambda x.e)\,\Box} = \interp{k}; \tstep{z, x''}{\sf gen/cont}
     {(\tforalll{{\sf app1}\,\interp{e_2}}{(\tappl{x'}{\tnil})})}$
\\ \>\> $~ :: ~$
 \> $x{:}\susp{\sf gen}
       \leadsto^*_{\Sigma\sf gen} \tackon{\Theta}{x'{:}\susp{\sf gen}}
       \leadsto_{\Sigma\sf gen} \tackon{\Theta}
            {\mkconj
               {z{:}\susp{{\sf cont}\,({\sf app2}\,\lambda x.\interp{e})}}
               {x''{:}\susp{\sf gen}}}$
\end{tabbing}
To complete the theorem, it is necessary to show that the two encoding
functions are one-to-one and onto. This can be done by demonstrating
the existence of a function $\interp{T}^{-1}_s = s$ from traces $T$
with type $x{:}\susp{\sf gen} \leadsto_{\Sigma\sf gen} \Delta$ where
${\sf gen} \notin \Delta$ to states $s$ and a function
$\interp{T}^{-1}_k = k$ from traces $T$ with type $x{:}\susp{\sf gen}
\leadsto_{\Sigma\sf gen} \tackon{\Theta}{x'{:}{\sf gen}}$ (where $x$
and $x'$ may be the same) to stacks $k$ and then showing that
the functions compose to the identity in both directions. 
That proof is tedious but straightforward.
\end{proof}

Note that two traces $T :: x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf
  gen} \Delta$ and $T' :: x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf
  gen} \Delta'$ are distinct if and only if the contexts $\Delta$ and
$\Delta'$ are distinct. Therefore, we can equivalently see adequacy as
a bijection between traces and abstract machine states $s$ or as a
bijection between contexts and abstract machine states $s$. In a
situation where this 1-to-1 correspondence between states and traces
did not exist (because two traces generate the same context), it is
not clear whether it would be preferable to define adequacy in
terms of contexts or in terms of traces.

\subsection{Preservation}
\label{sec:nat-ssos-adequacy-pres}

Before we prove that the concurrent system from
Figure~\ref{fig:cbv-ev-ssos-fun} adequately represents the transition
system from the beginning of the chapter, we must show that our
criteria for context well-formedness is actually preserved by the
concurrent computations in Figure~\ref{fig:cbv-ev-ssos-fun}. This is
part of the adequacy argument, but because we state it in terms of the
generative signature $\Sigma\sf gen$, it is also a 
standalone theorem entirely about of \sls~specifications. We will
return to theorems of this form in Part III of this thesis.

\bigskip
\begin{theorem}[Generation by $\Sigma\sf gen$ is invariant under
 $\Sigma\ref{fig:cbv-ev-ssos-fun}$]\label{thm:adequate-pres}~\\
  If $x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf gen} \Delta$ and
  $\Delta \leadsto_{\Sigma\ref{fig:cbv-ev-ssos-fun}} \Delta'$, then
  $x{:} \susp{\sf gen} \leadsto^*_{\Sigma\sf gen} \Delta'$
\end{theorem}

\begin{proof}
  Primarily by enumeration of the possible synthetic transitions of
  $\Sigma\ref{fig:cbv-ev-ssos-fun}$ and secondarily by case analysis
  on the structure of the trace $T :: x{:}\susp{\sf gen}
  \leadsto^*_{\Sigma\sf gen} \Delta$.

  \begin{itemize}
  \item $\tstep{z}{\sf ev/lam}{(\tforalll{\lambda x.e\,x}
                                {(\tappl{y}{\tnil})})}$

    \qquad $:: \frameoff{\Theta}
                 {y{:}\susp{{\sf eval}\,({\sf lam}\,\lambda x.e\,x)}}
               \leadsto
               \tackon{\Theta}
                 {z{:}\susp{{\sf retn}\,({\sf lam}\,\lambda x.e\,x)}} $

    \medskip

    $T = T'; \tstep{y}{\sf gen/eval}{(\tforalll{{\sf lam}\,\lambda x.e\,x}{\tappl{x'}{\tnil}})}$,\\
    so we construct\\
    $T'; \tstep{z}{\sf gen/retn}{(\tforalll{{\sf lam}\,\lambda x.e\,x}{\tappl{x'}{\tnil}})}$

    \medskip

  \item $\tstep{z_1, z_2}{\sf ev/app}{(\tforalll{e_1}
                                {\tforalll{e_2}{(\tappl{y}{\tnil})}})}$

    \qquad $:: \frameoff{\Theta}
                 {y{:}\susp{{\sf eval}\,({\sf app}\,e_1\,e_2)}}
               \leadsto
               \tackon{\Theta}
                 {\mkconj
                  {z_1{:}\susp{{\sf eval}\,e_1}}
                  {z_2{:}\susp{{\sf cont}\,({\sf app1}\,e_2)}}} $

    \medskip

    $T = T'; \tstep{y}{\sf gen/eval}{(\tforalll{{\sf app}\,e_1\,e_2}{\tappl{x'}{\tnil}})}$,\\
    so we construct\\
    $T'; 
     \tstep{z', z_2}{\sf gen/cont}{(\tforalll{{\sf app1}\,e_2}{(\tappl{x'}{\tnil})})};
     \tstep{z_1}{\sf gen/eval}{(\tforalll{e_1}{\tappl{z'}{\tnil}})}$

    \medskip


  \item $\tstep{z_1, z_2}{\sf ev/app1}{(\tforalll{\lambda x.e\,x}
                       {\tforalll{e_2}{(\tappl{\tfuser{y_1}{y_2}}{\tnil})}})}$

    \qquad $:: \frameoff{\Theta}
                 {\matchconj
                  {y_1{:}\susp{{\sf retn}\,({\sf lam}\,\lambda x.e\,x)}}
                  {y_2{:}\susp{{\sf cont}\,({\sf app1}\,e_2)}}}$

    \qquad\qquad
               $\leadsto
               \tackon{\Theta}
                 {\mkconj
                  {z_1{:}\susp{{\sf eval}\,e_2}}
                  {z_2{:}\susp{{\sf cont}\,({\sf app2}\,(\lambda x.e\,x))}}} $

    \medskip

    $T =
     T'; 
     \tstep{y', y_2}{\sf gen/cont}{(\tforalll{{\sf app1}\,e_2}{\tappl{x'}{\tnil}})};
     \tstep{y_1}{\sf gen/retn}{(\tforalll{{\sf lam}\,\lambda x.e\,x}{(\tappl{y'}{\tnil})})}$,\\
    so we construct\\
    $T'; 
     \tstep{z', z_2}{\sf gen/cont}{(\tforalll{{\sf app2}\,\lambda x.e\,x}{(\tappl{x'}{\tnil})})};
     \tstep{z_1}{\sf gen/eval}{(\tforalll{e_2}{\tappl{z'}{\tnil}})}$

    \medskip

  \item $\tstep{z}{\sf ev/app2}{(\tforalll{v_2}
                       {\tforalll{\lambda x.e\,x}
                         {(\tappl{\tfuser{y_1}{y_2}}{\tnil})}})}$

    \qquad $:: \frameoff{\Theta}
                 {\matchconj
                  {y_1{:}\susp{{\sf retn}\,v_2}}
                  {y_2{:}\susp{{\sf cont}\,({\sf app2}\,\lambda x.e\,x)}}}
               \leadsto
               \tackon{\Theta}
                 {z{:}\susp{{\sf eval}\,(e\,v_2)}} $

    \medskip

    $T =
     T'; 
     \tstep{y', y_2}{\sf gen/cont}{(\tforalll{{\sf app2}\,\lambda x.e\,x}{\tappl{x'}{\tnil}})};
     \tstep{y_1}{\sf gen/retn}{(\tforalll{v_2}{(\tappl{y'}{\tnil})})}$,\\
    so we construct\\
    $T'; 
     \tstep{z}{\sf gen/eval}{(\tforalll{e\,v_2}{\tappl{x'}{\tnil}})}$

    \medskip

  \end{itemize}

\noindent
This completes the proof. 
\end{proof}

The primary case analysis is just an enumeration of the possible
synthetic transitions, whereas the secondary case analyses on
generative traces, which allows us to say that only one generative
trace is possible in each case, requires more justification. We will
postpone a further discussion of this for now, however.\robnote{Fill
  in a forward reference above with a concrete reference when one
  exists}.

\subsection{Adequacy of the transition system}
\label{sec:nat-ssos-adequacy-absmachine}

The most interesting part of the adequacy proof was showing that
formation by generative signature $\Sigma\sf gen$ was an invariant of
$\Sigma\ref{fig:cbv-ev-ssos-fun}$. With that property established, the
final step is as straightforward as 

\bigskip
\begin{theorem}[Adequacy of the transition system]
$s \mapsto s'$ if and only if there exist $\Delta$ and $\Delta'$
such that
$\Delta \leadsto_{\Sigma\ref{fig:cbv-ev-ssos-fun}} \Delta'$,
$\interp{s} :: x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf gen} \Delta$, and
$\interp{s'} :: x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf gen} \Delta'$. 
\end{theorem}

\begin{proof} The proof is by straightforward case analysis and
  construction; we will give the case associated with ${\sf ev/app}$
  in both directions.

  The forward direction proceeds by case analysis over the definition
  of the transition system from the beginning of the chapter.  For
  instance, if $k \rhd e_1\,e_2 \mapsto (k; \Box\,e_2) \rhd e_1$ by
  rule ${\sf absmachine/app}$ then we can form (by
  Theorem~\ref{thm:adequacy-states}) the following traces:
  \begin{align*}
  \interp{k \rhd e_1\,e_2} 
  & :: x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf gen}
       \tackon{\Theta}
        {y{:}\susp{{\sf eval}\,({\sf app}\,\interp{e_1}\,\interp{e_2})}}
\\
  \interp{(k; \Box\,e_2) \rhd e_1} 
  & :: x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf gen}
       \tackon{\Theta}
        {\mkconj{z_1{:}\susp{{\sf eval}\,\interp{e_1}}}
         {z_2{:}\susp{{\sf cont}\,({\sf app1}\,\interp{e_2})}}}
  \end{align*}
  It is then possible to construct the required step:
  \begin{align*} 
  &\tstep{z_1, z_2}{\sf ev/app}{(\tforalll{\interp{e_1}}{\tforalll{\interp{e_2}}{(\tappl{y}{\tnil})}})}
  \\
  &\qquad\qquad :: \tackon{\Theta}
        {y{:}\susp{{\sf eval}\,({\sf app}\,\interp{e_1}\,\interp{e_2})}}
     \leadsto_{\Sigma\ref{fig:cbv-ev-ssos-fun}}
     \tackon{\Theta}
        {\mkconj{z_1{:}\susp{{\sf eval}\,\interp{e_1}}}
         {z_2{:}\susp{{\sf cont}\,({\sf app1}\,\interp{e_2})}}}
  \end{align*}

  In the backward direction, we are given a step in the dynamic 
  semantics, such as the one above, as well as the two traces 
  \begin{align*}
  T_1
  & :: x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf gen}
       \tackon{\Theta}
        {y{:}\susp{{\sf eval}\,({\sf app}\,\interp{e_1}\,\interp{e_2})}}
\\
  T_2
  & :: x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf gen}
       \tackon{\Theta}
        {\mkconj{z_1{:}\susp{{\sf eval}\,\interp{e_1}}}
         {z_2{:}\susp{{\sf cont}\,({\sf app1}\,\interp{e_2})}}}
  \end{align*}
  By the same case analysis on the structure of the trace that we performed
  in the preservation theorem (Theorem~\ref{thm:adequate-pres}) and
  , we 
  need to establish that \medskip \\
  $T_1 = T'; \tstep{y}{\sf gen/eval}{(\tforalll{\interp{e_1}}{\tforalll{\interp{e_2}}{(\tappl{x'}{\tnil})}})}$ and \\
  $T_2 = T'; \tstep{z', z_2}{\sf gen/cont}{(\tforalll{{\sf app1}\,\interp{e_2}}{(\tappl{x'}{\tnil})})}; \tstep{z_1}{\sf gen/eval}{(\tforalll{\interp{e_1}}{(\tappl{z'}{\tnil})})}$ \medskip\\
  % 
  where $T' :: x{:}\susp{\sf gen} \leadsto^*_{\Sigma\sf gen}
  \tackon{\Theta}{x'{:}{\sf gen}}$ in both cases. Therefore,
  Theorem~\ref{thm:adequacy-states} there is a stack $k$ such that
  $\interp{k} = T'$, $\interp{k \rhd e_1\,e_2} = T_1$, and
  $\interp{(k; \Box\,e_2) \rhd e_1} = T_2$.
  We conclude, then, by observing that 
  $k \rhd e_1\,e_2 \mapsto (k; \Box\,e_2) \rhd e_1$ by rule 
  ${\sf absmachine/app}$.
\end{proof}



\begin{figure}[t]
\begin{minipage}[b]{0.2\linewidth}
\[
\infer
{{\sf fix}\,x.e \Downarrow v}
{[{\sf fix}\,x.e/x]e \Downarrow v}
\]
\[
\infer
{\langle\rangle \Downarrow \langle\rangle}
{}
\]
~
\[
\infer
{\langle e_1, e_2 \rangle \Downarrow \langle v_1, v_2 \rangle}
{e_1 \Downarrow v_1 & e_2 \Downarrow v_2}
\]

\[
\infer
{e.1 \Downarrow v_1}
{e \Downarrow \langle v_1, v_2 \rangle}
\]\[
\infer
{e.2 \Downarrow v_2}
{e \Downarrow \langle v_1, v_2 \rangle}
\]
~
\[
\infer
{{\sf z} \Downarrow {\sf z}}
{}
\]
\[
\infer
{{\sf s}\,e \Downarrow {\sf s}\,v}
{e \Downarrow v}
\]
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.8\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/ssos-minml-core.sls}
\end{minipage}
\caption{Semantics of some pure functional features.}
\label{fig:ssos-minml-core}
\end{figure}

\section{Exploring the image of operationalization}
\label{sec:absmachine-nondeterminism}

The examples given in the previous section all deal
with call-by-value semantics for the untyped lambda calculus, which
has the property that any expression will either evaluate forever or
will eventually evaluate to a value $\lambda x. e$. We now want to
discuss ordered abstract machines with traces
that might get {\it stuck}. One way to raise the possibility of stuck
states is to add values besides $\lambda x.e$. In
Figure~\ref{fig:ssos-minml-core} we present an extension to
Figure~\ref{fig:cbv-ev-ssos-fun} with some of the features of a pure
``Mini-ML'' functional programming language: fixed-point recursion
($\interp{{\sf fix}\,x.e} = {\sf fix}\,\lambda x.\interp{e}$),
units and pairs ($\interp{\langle\rangle} = {\sf unit}$,
$\interp{\langle e_1, e_2 \rangle} = {\sf
  pair}\,\interp{e_1}\,\interp{e_2}$), projections ($\interp{e.1} =
{\sf fst}\,\interp{e})$, $\interp{e.2} = {\sf snd}\,\interp{e}$), and
natural numbers ($\interp{{\sf z}} = {\sf zero}$, $\interp{{\sf s}\,e}
= {\sf succ}\,\interp{e}$.  The natural semantics is given on the
left-hand side of that figure, and the operationalized and
defunctionalized ordered abstract machine that arises from (an
\sls~encoding of) that natural semantics is given on the right.


Note that, facilitated by the nondeterminism inherent in
operationalization, we chose parallel evaluation of pairs even though
the execution of functions is sequential in
Figure~\ref{fig:cbv-ev-ssos-fun}.  Traditional abstract machine
semantics are syntactic and do not handle parallel evaluation;
therefore, it is not possible to show that this ordered abstract
machine adequately encodes a traditional abstract machine presentation
of Mini-ML.

In an inductively-defined natural semantics, it is not possible to
distinguish a non-terminating term like ${\sf fix}\,x.x$ from a stuck
term like ${\sf z}.1$.  This is one of the problems that Leroy and
Grall sought to overcome in their presentation of coinductive big-step
operational semantics \cite{leroy09coinductive}. They defined the
judgment $e \Uparrow^\infty$ coinductively; therefore it is easy to
express the difference between non-terminating terms
(${\sf fix}\,x.x \Uparrow^\infty$) and stuck ones
(there is no $v$ such that ${\sf z}.1 \Downarrow v$ or
${\sf z}.1 \Uparrow^\infty$).

The translation of natural semantics into ordered abstract machines
also allows us to distinguish ${\sf fix}\,x.x$ from ${\sf z}.1$.
The former expression generates a trace that can always be extended:
\[ x_1{:}\susp{{\sf eval}\,({\sf fix}\,\lambda x.x)} \leadsto
   x_2{:}\susp{{\sf eval}\,({\sf fix}\,\lambda x.x)} \leadsto
   x_3{:}\susp{{\sf eval}\,({\sf fix}\,\lambda x.x)} \leadsto \ldots 
\]whereas the latter gets stuck and can make no more transitions:\[ 
  x_1{:}\susp{{\sf eval}\,({\sf fst}\,{\sf zero})} \leadsto
  x_2{:}\susp{{\sf eval}\,{\sf zero}}, y{:}\susp{{\sf cont}\,{\sf fst1}} \leadsto
  x_3{:}\susp{{\sf retn}\,{\sf zero}}, y{:}\susp{{\sf cont}\,{\sf fst1}} 
  \not\leadsto
\]
Thus, for deterministic semantics, both coinductive big-step
operational semantics and the operationalization transformation
represent ways of reasoning about the difference between
non-termination and failure in a natural semantics. Our approach has
the advantage of being automatic rather than requiring the definition
of a new coinductive relation $e \Uparrow^\infty$, though it would
presumably be possible to consider synthesizing the definition of $e
\Uparrow^\infty$ from the definition of $e \Downarrow v$ by an
analogue of our operationalization transformation.

In Section~\ref{sec:choicefail}, we discuss the advantages that the
operationalization approach has in dealing with nondeterministic
language features. These advantages do come with a cost when 
we consider natural semantics specifications that make deterministic
choices, which we discuss in Section~\ref{sec:choicecase}.

\subsection{Arbitrary choice and failure}
\label{sec:choicefail}

For the purposes of illustration, we will extend the language of
expressions with a nondeterministic choice operator $\interp{e_1 \arb
  e_2} = {\sf choose}\,\interp{e_1}\,\interp{e_2}$).  The two
(\sls-encoded) natural semantics rules for this extension and their
(tail-recursion optimized) operationalization are shown in
Figure~\ref{fig:ns-arb}.

\begin{figure}[t]
\begin{minipage}[b]{0.45\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-arb.sls}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.55\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-arb-ssos.sls}
\end{minipage}
\caption{Semantics of nondeterministic choice.}
\label{fig:ns-arb}
\end{figure}

We need to think about the desired semantics of 
expressions like $(\lambda y.\,y)
\arb {\sf z}.1$ -- if the first subexpression $(\lambda y.\,y)$ is
chosen for evaluation, then the expression evaluates to a value, but
if the second subexpression ${\sf z}.1$ is chosen, then the evaluation
gets stuck. Small-step
intuitions about language safety say that this is a possibly we should
be able to express, if only to exclude it with an appropriately
designed type system and type safety proof. The ordered abstract
machine semantics allows us to produce traces where
%
$x{:}\susp{\interp{(\lambda y.\,y) \arb {\sf z}.1}} \leadsto^* 
 y{:}\susp{{\sf lam}\,\lambda y.y}$
%
and where
%
$x{:}\susp{{\sf eval}\,\interp{(\lambda y.\,y) \arb {\sf z}.1}} \leadsto^* 
 (x'{:}\susp{{\sf retn}\,{\sf zero}}, y{:}\susp{{\sf cont}\,{\sf fst1}})
 \not\leadsto$
%
as we would hope. Natural semantics specifications (including
coinductive big step operational semantics) merely conclude
that $(\lambda y.\,y\,y) \arb {\sf z}.1 \Downarrow (\lambda
y.\,y\,y)$. Capturing the stuck behavior in this situation 
would require defining an extra inductive judgment capturing 
all the situations where $e$ can get stuck, which is undesirable.

Our ability to reason about stuck concurrent computations is an
artifact the fact that \sls~allows us to talk about traces $T$ in
addition to talking about complete proofs.  The operationalization
transformation allows us to transfer this concept to natural semantics
specifications by way of translation.

\subsection{Conditionals and factoring}
\label{sec:choicecase}

It is great that we're able to reason about nondeterministic
specifications in the output of the operationalization transformation!
But we run into a problem if we try to to encode a Mini-ML feature
that was conspicuously missing from Figure~\ref{fig:ssos-minml-core}:
the elimination form for natural numbers $\interp{{\sf case}\,e\,{\sf
    of}\,{\sf z} \Rightarrow e_z \mid {\sf s}\,x \Rightarrow e_s} =
{\sf case}\,\interp{e}\,\interp{e_z}\,(\lambda x.\interp{e_s})$.  The
usual natural semantics for case analysis look like this: 
\[
\infer[{\sf ev/casez}]
{\left({\sf case}\,e\,{\sf of}\,
   {\sf z} \Rightarrow e_z \mid {\sf s}\,x \Rightarrow e_s\right) \Downarrow v}
{e \Downarrow {\sf z}
 &
 e_z \Downarrow v}
\quad
\infer[{\sf ev/cases}]
{\left({\sf case}\,e\,{\sf of}\,
   {\sf z} \Rightarrow e_z \mid {\sf s}\,x \Rightarrow e_s\right) \Downarrow v}
{e \Downarrow {\sf s}\,v'
 &
 [v'/x]e_s \Downarrow v}
\]

\begin{figure}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/ssos-minml-case-bad.sls}
\caption{Problematic semantics of case analysis (not defunctionalized).}
\label{fig:ssos-minml-case-bad}
\bigskip
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/ssos-minml-case-bad-defun.sls}
\caption{Problematic semantics of case analysis (defunctionalized).}
\label{fig:ssos-minml-case-bad-defun}
\end{figure}

If we operationalize this specification directly, we get an
ordered abstract machine shown in Figure~\ref{fig:ssos-minml-case-bad}
before defunctionalization and in
Figure~\ref{fig:ssos-minml-case-bad-defun} after defunctionalization.
This operational semantics is nondeterministic much as the semantics of
$e_1 \arb e_2$ were, in that we can evaluate a case expression either
with rule ${\sf ev/casez}$, which effectively predicts that the answer
will be zero, or with rule ${\sf ev/cases}$, which effectively predicts
that the answer will be the successor of some value. But this means that
it is possible to get stuck while executing 
an intuitively type-safe expression if we predict the wrong branch:
\begin{align*}
 & x_1{:}\susp{{\sf eval}\,({\sf case}\,{\sf zero}\,e_z\,\lambda x.\,e_s)}
\\
\leadsto ~ & 
x_2{:}\susp{{\sf eval}\,{\sf zero}},
y_2{:}\susp{{\sf cont}\,({\sf cases}\,\lambda x.\,e_s)}
\\
\leadsto ~ & 
x_3{:}\susp{{\sf retn}\,{\sf zero}},
y_2{:}\susp{{\sf cont}\,({\sf cases}\,\lambda x.\,e_s)}
\\
\not\leadsto ~ & (!!!)
\end{align*}

This is actually a special case of a problem that appears in the logic
programming literature: interpreted as a deductive computation, the
natural semantics rules ${\sf ev/casez}$ and ${\sf ev/cases}$ may
require backtracking: if we try to evaluate $e$ using one of the rules
and fail, a backtracking semantics means that we will apply the other
rule, {\it re-evaluating} the scrutinee $e$ to a value. 

It is possible to avoid that backtracking by a transformation called
{\it factoring}. While factoring has been expressed by Polakow as a
transformation on functional programs in a variant of the Delphin
programming language \cite{poswolsky03factoring}, I am unaware of any
treatment of the factoring transformation as a generally-correct {\it
  logical} transformation on Prolog, $\lambda$Prolog, or Twelf
specifications. I will not do so here, but in this particular instance
there are two ways to factor the natural semantics to avoid
backtracking.  Both of them are provably equivalent to the original
natural semantics (in terms of the judgments $e \Downarrow v$ that are
derivable), and it is possible to use the existing metatheoretic
machinery of Twelf to verify this fact. Furthermore, both of them can
be operationalized into ordered abstract machine semantics that are
free of the unnecessary stuck stakes described above. 

One option is to create a new judgment ${\sf casen}\,v'\,e_z\,(x.e_s)\,v$
that is mutually recursive with the definition of $e \Downarrow v$. 
\[
\infer[{\sf ev/case}]
{\left({\sf case}\,e\,{\sf of}\,
   {\sf z} \Rightarrow e_z \mid {\sf s}\,x \Rightarrow e_s\right) \Downarrow v}
{e \Downarrow v'
 &
 {\sf casen}\,v'\,e_z\,(x.e_s)\,v}
\]
\[
\infer[{\sf casen/z}]
{{\sf casen}\,{\sf z}\,e_z\,(x.e_s)\,v}
{e_z \Downarrow v}
\quad
\infer[{\sf casen/s}]
{{\sf casen}\,({\sf s}\,v')\,e_z\,(x.e_s)\,v}
{[v'/x]e_s \Downarrow v}
\]
This fixes the problem with the original semantics, but the
transformation of this specification needs to operationalize both the
judgment $e \Downarrow v$ and the judgment ${\sf
  casen}\,v'\,e_z\,(x.e_s)\,v$. The resulting specification includes
what is essentially a tail-call that is missed by our somewhat
simplistic tail-recursion optimization; this makes the resulting
semantics unwieldy. 

It would be possible to generalize the tail-recursion optimization to
handle the specification above,\footnote{The operationalization
  transformation we presented creates two predicates ${\sf eval\_a}$
  and ${\sf retn\_a}$ for every predicate ${\sf ac} : \tau_1
  \rightarrow \tau_2 \rightarrow {\sf prop}$. A modification that was
  able to perform the desired tail-call optimization on the ${\sf
    casen}$ specification would need to create one ${\sf eval\_a}$ for
  every operationalized predicate and one ${\sf retn}$ for every {\it
    return type} $\tau_2$.  This would allow ${\sf ev}$ and ${\sf
    casen}$ to share the same return predicate, facilitating the use
  of tail-call optimization in the operationalization of ${\sf
    ev/case}$, ${\sf casen/z}$, and ${\sf casen/s}$. \label{returntypefootnote}} but there is another 
elegant option that uses the operationalization transformation
we have already described: we define a similar predicate ${\sf selectn}$
(mode ${\sf selectn} + + + -$) that merely performs the action of 
picking the $e_z$ or $e_s$ branch when given a value of the form
${\sf z}$ or ${\sf s}\,v$. 
\[
\infer[{\sf selectn/z}]
{{\sf selectn}\,{\sf z}\,e_z\,(x.e_s)\,e_z}
{}
\quad
\infer[{\sf selectn/s}]
{{\sf selectn}\,({\sf s}\,v')\,e_z\,(x.e_s)\,([v'/x]e_s)}
{}
\]
With this ${\sf select}$ function, we can write ${\sf ev/case}$ that
selects the next step $e'$ and then evaluates $e'$ to $v$.
\[
\infer[{\sf ev/case}]
{\left({\sf case}\,e\,{\sf of}\,
   {\sf z} \Rightarrow e_z \mid {\sf s}\,x \Rightarrow e_s\right) \Downarrow v}
{e \Downarrow v'
 &
 {\sf selectn}\,v'\,e_z\,(x.e_s)\,e'
 &
 e' \Downarrow v}
\]
The operationalization of this specification
(Figure~\ref{fig:ssos-minml-case-good}) is not quite in the form that
we discussed defunctionalizing, but we can defunctionalize an
equivalent specification, resulting in the semantics in
Figure~\ref{fig:ssos-minml-case-good-defun}.

\begin{figure}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/ssos-minml-case-good.sls}
\caption{Semantics of case analysis (not defunctionalized).}
\label{fig:ssos-minml-case-good}
\bigskip
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/ssos-minml-case-good-defun.sls}
\caption{Semantics of case analysis (defunctionalized).}
\label{fig:ssos-minml-case-good-defun}
\end{figure}


\subsection{Flat resolution for deductive computation}
\label{sec:flatresolution}

When we described the semantics of nondeterministic choice $e_1 \arb
e_2$, our operational intuition was to search either for a value such
that $e_1 \Downarrow v$ or a value such that $e_2 \Downarrow v$. We do
not want the operational semantics to backtrack and reconsider that
decision if the initial choice results in a stuck state. This
non-backtracking behavior is made explicit by the operationalization
transformation when we interpret the concurrent specification as a
committed-choice forward chaining logic program.

Maintaining this non-backtracking intuition means that some natural
semantics specifications, such as those for case analysis, need to be
treated carefully in order to have the correct behavior. Care is
needed in part because the standard
interpretation of deductive computation is as backtracking search.
The discussion above clarifies that, {\it in the context of natural
  semantics specifications}, we are interested in a (nonstandard) 
interpretation of deductive computation that is more like deterministic
or committed choice search; A{\"i}t-Kaci calls it {\it flat resolution}
 in \cite{aitkaci99warrens}. It is natural, then, to represent this
computation as a concurrent computation, for which is default
computational interpretation is committed choice.

This works both ways: in Section~\ref{sec:evaluationcontexts}
we transform a small-step structural operational semantics to obtain
an SLS analogue of evaluation contexts. As a deductive computation,
SOS specifications are naturally backtracking, so the concurrent
specifications we derive also need to be interpreted with backtracking
search if we want to give the resulting encoding a proper computational
interpretation.


\section{Exploring the richer fragment}
\label{sec:richer-ordered-abstract}

Work in the functional correspondence is generally concerned with
exploring tight correspondences between different styles of
specification. However, as we discussed in
Section~\ref{sec:the-point-is-modular-extension}, one of the main
reasons the logical correspondence in \sls~is interesting is because,
once we translate from a less expressive style (natural
semantics) to a more expressive style (ordered abstract machine
semantics), we can consider new modular extensions in the more
expressive style that were not possible in the less expressive
style. The opportunities for modular extension are part of what
distinguishes the logical correspondence we have presented from the
work by Hannan and Miller \cite{hannan92operational} and Ager
\cite{ager04natural}. Both of those papers translated natural
semantics into a syntactic specification of abstract machines; such
specifications are not modularly extensible to the degree that
concurrent \sls~specifications are.

The ordered abstract machine style of specification facilitates
modular extension with features that involve {\it state} and {\it
  parallel evaluation}. We have already seem examples of the latter:
the operationalization translation (as extended in
Section~\ref{sec:trans-par}) can put a natural semantics specification
into logical correspondence with either a sequential ordered abstract
machine semantics or a parallel ordered abstract machine semantics,
and our running example evaluates pairs in parallel. In this section,
we will consider some other extensions, focusing on stateful features
like mutable storage (Section~\ref{sec:mutable-storage}) and
call-by-need evaluation (Section~\ref{sec:call-by-need}). We will also
discuss the semantics of recoverable failure in
Section~\ref{sec:failure}. The presentation of recoverable failure
will lead us to consider a point of non-modularity: if we want to
extend our language flexibly with non-local control features like
recoverable failure, the parallel operationalization translation will
make this difficult or impossible.  A more modular semantics of
parallel evaluation will be presented in
Section~\ref{sec:modular-parallelism}.

This section will present extensions to the sequential, first-order
abstract machine for parallel evaluation presented in
Figure~\ref{fig:cbv-ev-ssos-fun}. Most of the specifications in
this section were first introduced in by Pfenning and I in
\cite{pfenning09substructural}.

\begin{figure}[t]
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/ssos-mutable.sls}
\caption{SSOS semantics of mutable storage.}
\label{fig:ssos-mutable}
\end{figure}

\subsection{Mutable storage}
\label{sec:mutable-storage}

The classic stateful programming language feature is mutable storage,
which forms the basis of imperative algorithms. We will consider
ML-style references, which add four new syntax forms to the language.
The first three create ($\interp{{\sf ref}\,e} = {\sf
  ref}\,\interp{e}$), dereference ($\interp{{!}e} = {\sf
  get}\,\interp{e}$), and update ($\interp{e_1 := e_2} = {\sf
  set}\,\interp{e_1}\,\interp{e_2}$) locations in the mutable
store. The fourth, ${\sf loc}\,l$, is a value that represents pointers
to allocated memory. The term $l$ is of a type ${\sf mutable\_loc}$
that has no constructors; locations $l$ can only be allocated at
runtime.

We also introduce a new ${\it linear}$ atomic proposition ${\sf
  cell}\,l\,v$ representing a piece of allocated memory (at location
$l$) and its contents (the value $v$). Recall that, without mutable
state or parallel computation, we maintain the invariant that the
process state $\Delta$ is made up of either a suspended ${\sf eval}\,e$
proposition or a suspended ${\sf retn}\,v$ proposition to the left of
some number of suspended ${\sf cont}\,f$ propositions. Once we add
mutable state, the LF context becomes non-empty, and we maintain the
invariant that the context has one mutable location for each allocated
cell:
\[
(l_1{:}{\sf mutable\_loc}, \ldots, l_n{:}{\sf mutable\_loc}; ~~
 x_1{:}\susp{{\sf cell}\,l_1\,v_1}, \ldots, 
 x_n{:}\susp{{\sf cell}\,l_n\,v_n}, 
 \Delta)
\]
where $\Delta$ has the form described before. Because the ${\sf
  cell}\,l_i\,v_i$ propositions are linear, the order of cells and
their placement relative to $\Delta$ are irrelevant.

Rule ${\sf ev/get}$ takes an expression ${\sf get}\,e$ and evaluates 
$e$ to a value of the form ${\sf loc}\,l$. After that, rule
${\sf ev/get1}$ takes the (unique, by invariant) cell associated
with that location, reads its value, and restores the cell to the
context. The synthetic transition associated with ${\sf ev/get1}$ is 
as follows:
\begin{align*}
(\Psi&;  \frameoff{\Theta}{x{:}\susp{{\sf retn}\,({\sf loc}\,l)}, y{:}\susp{{\sf cont}\,{\sf get1}}, z{:}\susp{{\sf cell}\,l\,v}})
\leadsto 
(\Psi; \tackon{\Theta}{w{:}\susp{{\sf retn}\,v},
z'{:}\susp{{\sf cell}\,l\,v}})
\end{align*}
Again, it is critical, when reading this transition, to account for the fact
that ${\sf retn}$ and ${\sf cont}$ are ordered predicates but ${\sf
  cell}$ is a linear predicate.

The ${\sf set}$ rules are similar, except that we also evaluate a new value
$v_2$ and restore that 
value to the process state instead of the value previously
contained in the cell. We mention ${\sf cell}\,l\,\_$ in the
premise of ${\sf ev/set2}$ only to consume the old cell associated with $l$
before we replace it with something new.

Finally, the ${\sf ref}$ rules evaluate the subexpression to a value
$v$ and then, in rule ${\sf ev/ref1}$, allocate a new cell to hold
that value. This new cell, according to our context invariant, needs
to be associated with a new variable $l$, which we generate with
existential quantification in the head of the ${\sf ev/ref1}$ rule.
The synthetic transition associated with ${\sf ev/ref1}$ therefore has
an extended LF context after the transition:
\begin{align*}
(\Psi;  \frameoff{\Theta}{x{:}\susp{{\sf retn}\,v}, y{:}\susp{{\sf cont}\,{\sf ref1}}})
\leadsto 
(\Psi, l{:}{\sf mutable\_loc}; \tackon{\Theta}{w{:}\susp{{\sf retn}\,({\sf loc}\,l)},
z{:}\susp{{\sf cell}\,l\,v}})
\end{align*}

\subsubsection{Existential angst} 

Our semantics of mutable storage uses existential quantification 
as a symbol generator to conjure up new locations. However, it is important
to remember that LF variables in $\Psi$ are defined by substitution,
so if there is a step 
$(\Psi, l_1{:}{\sf loc}, l_2{:}{\sf loc}; \Delta)
  \leadsto 
 (\Psi, l_1{:}{\sf loc}, l_2{:}{\sf loc}; \Delta')$,
it must also be the case that 
$(\Psi, l_1{:}{\sf loc}; [l_1/l_2]\Delta)
  \leadsto 
 (\Psi, l_1{:}{\sf loc}; [l_1/l_2]\Delta')$. 
%
 Because any transition must still be possible if we unify two
 locations, \sls~must not provide any way to write a rule that only
 fires if two variables are {\it distinct}. This, in turn, means our
 specification of Mini-ML with mutable references {\it cannot} be
 further extended to include the tests for reference equality that
 languages like Standard ML or Ocaml have.

This discussions suggests that a 
genuine logical facility for creating true names that can be compared
for inequality would be helpful; 
existential quantification doesn't fully give us
that facility. Instead, these existentially quantified locations, as
variables of a type with no constructors, are 
like stand-ins 
for the ``true'' locations, the locations that can be compared for
inequality and that \sls~is unable to represent.  If we wanted to
represent Standard ML's pointer inequality in \sls, we would have
to hack-up symbol generation manually, perhaps by tagging each memory cell
with a unique natural number timestamp, as natural numbers certainly can be
compared for inequality.

I believe that a substructural treatment of nominal quantification
could be incorporated into \sls~and would allow for locations to be
handled in a more satisfying way along the lines of proposals by
Cheney and Harper \cite{cheney12dependent,harper12practical}. This
extension to the \sls~framework is beyond the scope of this thesis,
however. Luckily, aside from being unable to elegantly represent tests
for pointer inequality or the entirety of Harper's Modernized Algol
\cite[Chapter 35]{harper12practical}, we will not miss name generation
facilities too much in the context of this thesis. One of the most important
use cases of name generation and nominal abstraction is in reasoning
{\it about} logical specifications within a uniform logic
\cite{gacek11nominal}, and this thesis does not consider a uniform {\it
  meta}logic for \sls~specifications.

\subsection{Call-by-need evaluation}
\label{sec:call-by-need}

Mutable references were
an obvious use of ambient state; another use is to
represent {\it call-by-need} evaluation. The basic idea in
call-by-need evaluation is that an expression is not evaluated
eagerly; rather, instead, it is stored until the value of that
expression is suspended. Once a value is
needed, it is computed and the value of that computation is memoized;
therefore, a suspended expression will be computed at most once.

I give two presentations of by-need evaluation, which correspond to
the two versions of lazy evaluation discussed by Harper in
\cite[Chapter 37]{harper12practical}.



\begin{figure}[t]
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/ssos-cbneed.sls}
\caption{SSOS semantics of call-by-need recursive suspensions.}
\label{fig:ssos-cbneed}
\end{figure}

\subsubsection{Recursive suspensions}


Recursive suspensions (Figure~\ref{fig:ssos-cbneed}) replace the
fixed-point operator ${\sf fix}\,x.e$ with a thunked expression
$\interp{{\sf thunk}\,x.e} = {\sf thunk}\,(\lambda
x.\,\interp{e})$. Whereas the fixed-point operator returns a value (or
fails to terminate), thunked expressions always immediately return a
value ${\sf issusp}\,l$, where $l$ is a location of type ${\sf
  bind\_loc}$. This location is initially associated with a linear
atomic proposition ${\sf susp}\,l\,(\lambda x.\interp{e})$ (rule ${\sf
  ev/thunk}$).

When we apply the ${\sf force}$ operator to an expression that returns
${\sf issusp}\,l$ for the first time, the location $l$ stops
being associated with a linear atomic proposition of the form ${\sf
  susp}\,l\,(\lambda x.\interp{e})$ and becomes associated with a
linear atomic proposition of the form ${\sf blackhole}\,l$ (rule ${\sf
  ev/force1a}$). This ${\sf blackhole}\,l$ proposition represents that
an expression that needs to directly reference its own value in the
process of computing to a value will never evaluate to a value; in
this example, such a computation will end up stuck, but a rule with the
premise ${\sf retn}\,({\sf issusp}\,l) \fuse {\sf cont}\,{\sf force1}
\fuse {\sf blackhole}\,l$ could instead be used to loop endlessly 
or signal failure. 

Once a suspended expression has been fully evaluated (rule ${\sf ev/force2b}$),
the black hole is removed and the location $l$ is permanently associated
with the value $v$; future attempts to force the same suspended
expression will trigger rule ${\sf ev/force1b}$ instead of 
${\sf ev/force1a}$ and will immediately return the memoized value.

\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/ssos-cbneed-refun.sls}
\caption{SSOS semantics of call-by-need recursive suspensions, refunctionalized.}
\label{fig:ssos-cbneed-refun}
\end{figure}

The last four rules in Figure~\ref{fig:ssos-cbneed} (and the one
commented-out pseudo-rule) are all part of one multi-stage protocol.
It may be enlightening to consider the {\it re}functionalization of
Figure~\ref{fig:ssos-cbneed} presented in
Figure~\ref{fig:ssos-cbneed-refun}. This rule has a conjunctive
continuation, with one conjunct for two of the three atomic propositions a
${\sf bind\_loc}$ can be associated with: the linear proposition
${\sf susp}\,l\,(\lambda x.e)$, the linear proposition 
${\sf blackhole}\,l$ (which cannot be handled by the continuation and 
so can result in a stuck state), and the persistent proposition
${\sf bind}\,l\,v$. 

\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/ssos-by-need.sls}
\caption{SSOS semantics of lazy call-by-need functions.}
\label{fig:ssos-by-need}
\end{figure}


\subsubsection{Lazy evaluation}

An alternative to the explicit suspensions above, which must be forced
explicitly, is a semantics of lazy evaluation, which better matches
the semantics of popular call-by-need languages like Haskell. For this
semantics, we will not create a new abstract location type like ${\sf
  mutable\_loc}$ or ${\sf bind\_loc}$; instead, we will associate 
suspended expressions (and black holes and memoized values) with
free expression variables of type ${\sf exp}$.

We can treat lazy call-by-need functions $({\sf lazylam}\,\lambda
x.e)$ as an extension to the language that already includes
call-by-value functions $({\sf lam}\,\lambda x.e)$ and
application. Lazy functions are values (rule ${\sf ev/lazylam}$), but
when a lazy function is returned to a frame $\interp{\Box\,e_2} = {\sf
  app1}\,\interp{e_2}$, we do not evaluate $e_2$ to a value
immediately. Instead, we create a free variable $x$ of type ${\sf
  exp}$ and substitute that into the lazy function.

Free variables $x$ can now be part of the language of expressions that
get evaluated, though they are not in the language of values that get
returned. Therefore, we need some way of evaluating free
variables. This is handled by the three rules ${\sf ev/susp'}$, ${\sf
  ev/susp1'}$, and ${\sf ev/bind'}$ as before. Each free expression
variable $x$ is either associated with a unique linear atomic
proposition ${\sf susp'}\,x\,e_2$, a black hole ${\sf blackhole'}\,x$,
or a persistent binding ${\sf bind'}\,x\,v$.

\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/ssos-by-env.sls}
\caption{SSOS environment semantics for call-by-value functions.}
\label{fig:ssos-by-env}
\end{figure}


\subsection{Environment semantics}

Toninho, Caires, and Pfenning have observed that call-by-need and
call-by-value are closely related as sequential variants of
speculative execution (or call-by-future) in which the evaluation of
arguments is scheduled either as early as possible (call-by-value) or
as late as possible (call-by-need) \cite{toninho12functions}. The
relationship between the lazy call-by-need semantics from
Figure~\ref{fig:ssos-by-need} and call-by-value is better presented by
giving a variant of what Pfenning and I called an {\it environment
  semantics} in \cite{pfenning09substructural}.


As with the lazy call-by-name semantics, we introduce the environment
semantics by creating a new function value ${\sf evlam}\,(\lambda
x.\,e)$ instead of reinterpreting the existing function value ${\sf
  lam}\,(\lambda x.\,e)$. When a value of the form ${\sf
  lazylam}\,(\lambda x.\,e)$ is returned to a frame ${\sf app1}\,e_2$
in rule ${\sf ev/applazy}$ from Figure~\ref{fig:ssos-by-need}, we
immediately create the new expression variable $x$, suspend the
argument $e_2$, and schedule the function body for evaluation. When a
value of the form ${\sf evlam}\,(\lambda x.\,e)$ is returned to a
frame ${\sf app1}\,e_2$ frame in rule ${\sf ev/appev1}$ in
Figure~\ref{fig:ssos-by-env}, we likewise create the new expression
variable $x$, but we suspend the {\it function body} in a frame ${\sf
  app2'}\,x\,e$ that also records the new expression variable $x$ and
schedule the {\it argument} for evaluation. Then, when the evaluated
function argument $v_2$ is returned to that frame (rule ${\sf
  ev/appev2}$), we create the same persistent binding ${\sf
  bind}\,x\,v_2$ that was generated by rule ${\sf ev/susp1'}$ in
Figure~\ref{fig:ssos-by-need} and proceed to evaluate the function
body. Upon encountering the free variable $x$ in the course of
evaluation, the same rule ${\sf ev/bind'}$ from
Figure~\ref{fig:ssos-by-need} will return the right value.

This presentation of the environment semantics is designed to look
like call-by-need, and so it creates the free variable $x$ early, in
rule ${\sf ev/appev1}$.  It would be equally reasonable to create the
free variable $x$ later, in rule ${\sf ev/appev2}$, which would result
in a specification that resembles
Figure~\ref{fig:cbv-ev-ssos-fun} more closely; this is what
was done in \cite{pfenning09substructural} and
\cite{simmons11logical}.


\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/ssos-fail.sls}
\caption{SSOS semantics of recoverable failure.}
\label{fig:ssos-fail}
\end{figure}


\subsection{Recoverable failure}
\label{sec:failure}

In a standard abstract machine presentation, recoverable failure can 
be introduced by adding a new state $s = k {\blacktriangleleft}$
to the existing two ($s = k \rhd e$ and $s = k \lhd v$)
\cite[Chapter 28]{harper12practical}. Whereas $k
\lhd v$ represents a value being returned to the stack, $k
{\blacktriangleleft}$ represents {\it failure} being returned to the
stack; failure is signaled by the expression ${\sf fail}$\footnote{Failure 
could also be introduced by actions like as dividing
by zero or encountering the black hole in in the commented-out case of
Figure~\ref{fig:ssos-cbneed}.} and
can be handled by the expression $\interp{{\sf try}\,e_1\,{\sf ow}\,e_2} = 
{\sf catch}\,e_1\,e_2$. 

We can extend {\it sequential} ordered abstract machines with
exceptions in a modular way, as shown in
Figure~\ref{fig:ssos-fail}. Recall that a sequential ordered abstract
machine specifications is one where there is only one ordered ${\sf
  eval}\,e$ or ${\sf retn}\,v$ proposition in the process state to the
right of a series of ordered ${\sf cont}\,f$ propositions -- process
states with ${\sf eval}\,\interp{e}$ correspond to states $k \rhd e$
and process states with ${\sf retn}\,\interp{v}$ correspond to states
$k \lhd v$. 

We introduce two new ordered atomic propositions. The first, ${\sf
  error}$, is introduced by rule ${\sf ev/fail}$.
 A state with an ${\sf error}$
proposition corresponds to a state $k{\blacktriangleleft}$ in
traditional ordered abstract machine specifications. Errors eat away
at any ${\sf cont}\,f$ propositions to their right (rule ${\sf
  ev/error}$). The only thing that stops the inexorable march of an
${\sf error}$ is the special ordered atomic proposition ${\sf
  handle}\,e$ that is introduced in rule ${\sf ev/catch}$ when we
evaluate an exception handler.

This is one case where the use of defunctionalized specifications --
and, in particular, our decision to defunctionalize with a single
${\sf cont}\,f$ proposition instead of inventing a new ordered atomic
proposition at every step -- gives us a lot of expressive power. If we
wanted to add exceptions to the higher-order specification of Mini-ML,
we would have to include the possibility of an exceptional outcome in
every individual rule. For instance, this would be the 
rule for evaluating $\interp{{\sf s}\,e} = {\sf succ}\,\interp{e}$:

\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/ssos-fail-s-refun.sls}

\noindent
Instead, this case is handled generically by rule ${\sf ev/error}$ in
Figure~\ref{fig:ssos-fail}, though the above specification is what we
would get if we were to refunctionalize the defunctionalized
specification of core Mini-ML from Figure~\ref{fig:ssos-minml-core}
extended with rule ${\sf ev/error}$.

\subsubsection{Failures and the parallel translation}

Our semantics of recoverable failure composes reasonably well with
stateful features, though arguably in call-by-need
evaluation it is undesirable that forcing a thunk can lead to errors
being raised in a non-local fashion. However,
recoverable failure does not compose well with parallel semantics as
we have described them. 

We assume, in rule ${\sf ev/error}$, that we can blindly eliminate
${\sf cont}\,f$ frames with the ${\sf ev/error}$ rule.  If we
eliminate the ${\sf cont}\,{\sf pair1}$ frame from
Figure~\ref{fig:ssos-minml-core} in this way, it breaks the invariant
that the ordered propositions represent a branching tree written down
in postfix. Recall that, without exceptions, a piece of process state
in the process of evaluating $\langle e_1\,e_2\rangle$ has the
following form:
\[
\mbox{(subgoal: compute $e_1$)},
\mbox{(subgoal: compute $e_2$)},
y{:}\susp{{\sf cont}\,{\sf pair1}} 
\]
If the second subgoal for $e_2$ signals an error, that error will
immediately propagate to the right, orphaning the first subgoal.
Conversely, if the first subgoal signals an error, that error will
have to wait until the first subgoal completes: \sls~specifications
are local, and there is no local way for the first subgoal to talk
about its continuation $y{:}\susp{{\sf cont}\,{\sf pair1}}$ because an
arbitrary amount of stuff (the representation of the second subgoal)
is in the way. This seems to force us into treating parallel
evaluations asymmetrically: if $e_{\it raise}$ signals failure and
$e_{\it loop}$ loops forever, then the two Mini-ML pair expressions
$\langle e_{\it raise}, e_{\it loop} \rangle$ and $\langle e_{\it
  loop}, e_{\it raise} \rangle$ are observably different. That is bad.

In my thesis proposal, I suggested a fix: we can defunctionalize ${\it
  sequential}$ propositions of the form $\forall \overline{x}.\,{\sf
  retn}\,v \lefti \{\ldots \}$ using one ordered atomic proposition
${\sf cont}\,f$ and defunctionalize ${\it parallel}$ propositions of
the form $\forall \overline{x}.\,{\sf retn}\,v_1 \fuse {\sf retn}\,v_2
\lefti \{\ldots \}$ using a different ordered atomic proposition ${\sf
  cont2}\,f$. This lets us write rules that treat parallel continuations
generically and that only return errors when {\it both} sub-computations
have completed and at least one has signaled an error: 

\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/ssos-fail-binary.sls}

\noindent
This is a big improvement, because parallel pairs are again treated
symmetrically. But it's not the way we necessarily wanted to restore
symmetry: the evaluation $\langle e_{\it raise}, e_{\it loop} \rangle$
and $\langle e_{\it loop}, e_{\it raise} \rangle$ will both loop
forever, but we might wish for both of them to signal failure. The
latter alternative is not expressible in an ordered abstract machine
specification.

Part of the problem is that recoverable failure is fundamentally a
{\it control} feature and not a stateful or parallel programming
language feature. As a result, it is not easy to handle at the level
of ordered abstract machines, because ordered abstract machines do not
give the specification author enough access to the control
structure. The destination-passing style we consider in the next
chapter, on the other hand, 
will give us sufficient access to control structure.


\subsection{Looking back at natural semantics}
\label{sec:enriching-natsem}

Mutable storage, call-by-need evaluation, and the environment
semantics are all modular extensions to the call-by-value specification in
Figure~\ref{fig:cbv-ev-ssos-fun}.  The
extensions are modular because they make essential use of the ambient
context available to concurrent \sls~specifications, introducing new
linear and persistent ordered atomic propositions that can be added to
to the context (and, in the linear case, removed as well).

For extensions to sequential ordered abstract machines that are only
based on extending the state, we can consider what it would mean to
reverse-engineer a natural semantics formalism that is as extensible
as the resulting ordered abstract machine. The primary judgment of
such a specification is not $e \Downarrow v$ as before; rather, the
primary judgment becomes $\{ e \| \mu \}_\Psi \Downarrow \{ v \| \mu
\}_{\Psi'}$.\footnote{The notation $\{ e \| \mu \}_\Psi$ is intended
  to invoke Harper's notation $\nu \Sigma \{ e \| \mu \}$, which is
  used to describe mutable references and lazy evaluation in Chapters
  36 and 37 of \cite{harper12practical}. The critical semantic
  distinction is that our $\Psi$ contains variables whereas $\Sigma$
  contains proper symbols that are not available in \sls, as discussed
  in Section~\ref{sec:mutable-storage}.} The variable contexts $\Psi$
and $\Psi'$ are the same variable contexts that appear in our process
states $(\Psi; \Delta)$ and our specifications are expected to
maintain the invariant that $\Psi \subseteq \Psi'$. The objects $e$
and $v$ remain syntactic objects adequately representable in LF, as
before, whereas $\mu$ is an extensible bag of judgments $\mu = J_1
\otimes \ldots \otimes J_n$ that correspond to the propositions in our
linear and persistent context; we treat $\otimes$ as an associative
and commutative operator (just like conjunction
of linear logic contexts).  A
new judgment in an \sls~specification 
can be treated as a new member of the syntactic class $J$.  For
instance, lazy call-by-need functions as defined in
Figure~\ref{fig:ssos-by-need} uses three judgments:
$x{\hookrightarrow}e$ (corresponding to ${\sf susp'}\,x\,e$),
$x{\hookrightarrow}{\fuse}$ (corresponding to ${\sf blackhole'}\,x$),
and $x{\rightarrowtriangle}v$ (corresponding to ${\sf bind'}\,x\,v$).
We can give a statefully-modular natural semantics for call-by-need
lazy functions as follows:

\[
\infer
{\{ \lambda x.e \| \mu \}_\Psi
  \Downarrow 
 \{ \lambda x.e \| \mu \}_\Psi \mathstrut}
{}
\]
\[
\infer
{\{ e_1\,e_2 \| \mu \}_\Psi
  \Downarrow 
 \{ v \| \mu'' \}_{\Psi''} \mathstrut}
{\{ e_1 \| \mu \}_\Psi
  \Downarrow 
 \{ \lambda x. e \| \mu' \}_{\Psi'}
 &
 \{ e \| x {\hookrightarrow} e_1 \otimes \mu' \}_{\Psi', x}
  \Downarrow 
 \{ v \| \mu'' \}_{\Psi''} \mathstrut}
\]
\[
\infer
{\{ x \| x {\hookrightarrow} e \otimes \mu \}_{\Psi, x} 
  \Downarrow
 \{ v \| x{\rightarrowtriangle}v \otimes \mu' \}_{\Psi', x} \mathstrut}
{\{ e \| x {\hookrightarrow}{\fuse} \otimes \mu \}_{\Psi, x}
  \Downarrow
 \{ v \| x {\hookrightarrow}{\fuse} \otimes \mu' \}_{\Psi', x} \mathstrut}
\]
\[
\infer
{\{ x \| x {\rightarrowtriangle}v \otimes \mu \}_{\Psi, x}
  \Downarrow 
 \{ v \| (x {\rightarrowtriangle}v \otimes \mu \}_{\Psi, x} }
{}
\]

While the definition of $e \Downarrow v$ could be directly encoded
as a deductive \sls~specification, the definition of
$\{ e \| \mu \}_{\Psi} \Downarrow \{ v \| \mu' \}_{\Psi'}$ cannot.
Nevertheless, the example above suggests
that a carefully-defined formalism for statefully-modular natural
semantics specifications could be similarly compiled into (or, indeed,
{\it defined in terms of}) the operationalization of specifications
into \sls.

There is a great deal of work on special-purpose formalisms for the 
specification and modular extension of operational semantics. 
The specification above follows Harper's development
in \cite{harper12practical}, and Mosses's Modular Structural
Operational Semantics (MSOS) is a similar development 
\cite{mosses04modular}. Previous work is primarily interested in the
modular extension of small-step structural operational semantics
specifications rather than big-step natural semantics, though Mosses
does discuss the latter. The operationalization transformation applies
to SOS specifications (as discussed below in
Section~\ref{sec:evaluationcontexts}), but the result is something
besides an ordered abstract machine semantics.

The functional correspondence connects structural operational
semantics and abstract machines \cite{danvy08defunctionalized}. A
logical correspondence between SOS specifications and ordered abstract
machines in \sls~might give us insight into a modular formalism for
SOS that is defined in terms of concurrent \sls~specifications, but
this is left for future work.

% \begin{figure}
% \begin{minipage}[b]{0.43\linewidth}
% \fvset{fontsize=\small,boxwidth=auto}
% \VerbatimInput{sls/nat.sls}
% \end{minipage}
% \hspace{0.5cm}
% \begin{minipage}[b]{0.57\linewidth}
% \fvset{fontsize=\small,boxwidth=auto}
% \VerbatimInput{sls/nat-ssos.sls}
% \end{minipage}
% \caption{Two versions of binary addition.}
% \label{fig:nat}
% \end{figure}

\section{Partial transformation}
\label{sec:othertransform}

Thus far, we have only discussed the application of the
operationalization and defunctionalization transformations to natural
semantics specifications. However, both transformations are general
and can be applied to many different specifications. 

In this section, we will consider the meaning of
operationalization on two other types of deductive
\sls~specifications: small-step structural operational semantics
specifications and the natural semantics of Davies' staged computation
language \rowan. Both are also interesting examples of partial
transformation where we use the generality of operationalization
to transform only some of the predicates in a program.


% As one example, consider Pfenning's course notes. 
% For instance, in course notes, Pfenning describes
% addition of binary numbers\footnote{Bits are encoded as a type ${\sf
%     bit}$ with two elements ${\sf b0}$ and ${\sf b1}$, and binary
%   natural numbers have the form $n ::= \epsilon \mid n{\sf 0} \mid n{\sf 1}$
%   and are encoded in a type ${\sf bin}$ as follows: $\interp{\epsilon}
%   = {\sf e}$, $\interp{n{\sf 0}} = {\sf d}\,\interp{n}\,{\sf b0}$, and
%   $\interp{n{\sf 1}} = {\sf d}\,\interp{n}\,{\sf b1}$.}
% both as an ordered abstract machine and a deductive specification; 
% we can 

% both as an ordered abstract machine and a deductive 

% a type of binary numbers that can be encoded in the LF type
% ${\sf bin}$ as follows: $\interp{\epsilon} = {\sf e}$, 
% $\interp{n{\sf 0}} = {\sf b}\,\interp{n}\,{\sf b0}$, and
% $\interp{n{\sf 1}} = {\sf b}\,\interp{n}\,{\sf b1}$.
% a concurrent specification of binary addition 

% (as alluded to in Footnote~\ref{returntypefootnote}
% above).

\begin{figure}[tp]
\begin{minipage}[b]{0.450\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-sos.sls}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.50\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-sos-eval.sls}
\end{minipage}
\caption{Small-step evaluation, and one corresponding abstract machine.}
\label{fig:cbv-sos}
\end{figure}

\subsection{Evaluation contexts}
\label{sec:evaluationcontexts}


We have thus far considered big-step operational semantics and
abstract machines, mostly neglecting the third great tradition of
programming language specification, {\it structural operational
  semantics} or SOS specifications. SOS specifications are small-step
like abstract machine specifications, but whereas abstract machine
specifications are ``flat'' and defined only by pattern-matching on
syntax, SOS specifications are defined inductively on the structure
of syntax. This is the SOS specification for 
call-by-value evaluation of the untyped lambda calculus:
\[
\infer
{\lambda x.e\,{\sf value} \mathstrut}
{}
\quad
\infer
{e_1\,e_2 \mapsto e_1'\,e_2 \mathstrut}
{e_1 \mapsto e_1' \mathstrut}
\quad
\infer
{e_1\,e_2 \mapsto e_1\,e_2' \mathstrut}
{e_1\,{\sf value}
 &
 e_2 \mapsto e_2' \mathstrut}
\quad
\infer
{(\lambda x. e)v \mapsto [v/x]e \mathstrut}
{v\,{\sf value} \mathstrut}
\]
This inductive specification is adequately encoded on the left-hand
side of Figure~\ref{fig:cbv-sos}, along with the proposition ${\sf
  ev}\,e\,v$ that describes a big-step operational semantics in terms
of repeated application of the small-step operational semantics. This
is the complete specification that we referred to in
Section~\ref{sec:trans-tail} when we operationalized only the ${\sf
  ev}$ predicate to motivate tail recursion. On the right-hand side of
Figure~\ref{fig:cbv-sos}, we show the specification that results if we
instead operationalize and defunctionalize only the 
${\sf step}$ predicate. Instead of ${\sf eval}$ we have 
${\sf decomp}$, since the relevant action is to decompose the
expression looking for an applicable $\beta$-reduction, and instead of 
${\sf retn}$ we have ${\sf plug}$, since the relevant action is 
to plug the reduced expression back into the larger term.

When we operationalized natural semantics, the structure of the
suspended ${\sf cont}\,f$ propositions was analogous to the control
stacks $k$ of abstract machine specifications. In our operationalized
SOS specification, the structure of the ${\sf cont}\,f$ propositions
is analogous to the {\it evaluation contexts}, often written as 
$E[]$.
\[
E[] ::= E[]\,e \mid v\,E[] \mid []
\]
The names {\it decompose} and {\it plug} are taken from the treatment
of evaluation contexts in the functional correspondence
\cite{danvy08defunctionalized}.

As we
foreshadowed in Section~\ref{sec:flatresolution}, the right
computational interpretation of Figure~\ref{fig:cbv-sos} is {\it not}
committed-choice forward chaining. Consider terms of type ${\sf
  eval}\,\interp{(\lambda x.x)\,e} \lefti \{ {\sf
  retn}\,\interp{(\lambda x.x)\,e'} \}$ where $e \mapsto e'$ and
consequently $\tackon{\Theta}{x{:}\susp{{\sf eval}\,\interp{e}}}
\leadsto^* \tackon{\Theta}{y{:}\susp{{\sf retn}\,\interp{e'}}}$.
It is entirely possible to use rule ${\sf step/app1}$ to
derive the following:
\[
  x_1{:}\susp{{\sf eval}\,\interp{(\lambda x.x)\,e}}
   \leadsto
  x_2{:}\susp{{\sf eval}\,\interp{\lambda x.x},
  y_2{:}\susp{{\sf cont}\,({\sf ap1}\,\interp{e})}}
   \not\leadsto
\]
While stuck states in abstract machines raised alarm bells about
language safety, the stuck state above is not a concern -- we merely
should have applied rule ${\sf step/app2}$ to $x_1$ instead. This
corresponds to the fact that both small-step SOS specifications and
specifications that use evaluation context 
map most naturally to the backtracking
search behavior more frequently associated with deductive computation. 

The agnostic attitude that logical specifications take towards
backtracking can be helpful, but actually treating the
operationalization of an SOS specification as an effective language
interpreter presents some interesting challenges, particularly if we
consider operationalization of {\it both} the ${\sf step}$ and ${\sf
  ev}$ predicates. The desired interpretation of the resulting
specification would have phases of backtracking search as we sought to
find a series of transitions corresponding to a small step $e \mapsto
e'$, but these phases would exist within a larger concurrent
derivation representing the sequence $e \mapsto e' \mapsto e'' \mapsto
\ldots$ that is interpreted as committed-choice forward chaining.

\subsection{Temporal logic}

As a final example, we present two \sls~specifications of Davies'
\rowan, a logically-motivated type system and natural
semantics for partial evaluation \cite{davies96temporal}.  Partial
evaluation is not a modular language extension, either on paper or in
\sls. On paper, we have to generalize the judgment $e \Downarrow v$ to
have free variables; we write $e \Downarrow_\Psi v$ where $\Psi$
contains free expression variables. In \sls, this does not actually
require us to change the judgment ${\sf ev}\,e\,v$ from
Figure~\ref{fig:example-transform-cbv}, since the specification itself
does not specify the context of LF. However, \rowan~also requires a
separate judgment $e \Downarrow^n_{\Psi} e'$ for reducing expressions
that will be fully evaluated not now but $n$ partial evaluation stages
in the future. This is the full \rowan~specification for call-by-value
functions:
\[
\infer
{\lambda x. e \Downarrow_{\Psi} \lambda x. e \mathstrut}
{}
\quad
\infer
{e_1\,e_2 \Downarrow_{\Psi} v \mathstrut}
{e_1 \Downarrow_{\Psi} \lambda x.e
 &
 e_2 \Downarrow_{\Psi} v_2
 & 
 [v_2/x]e \Downarrow_{\Psi} v \mathstrut}
\quad
\infer
{\lambda x. e \Downarrow^n_{\Psi} \lambda x.e' \mathstrut}
{e \Downarrow^n_{\Psi,x} e' \mathstrut}
\quad
\infer
{e_1\,e_2 \Downarrow^n_{\Psi} e_1'\, e_2' \mathstrut}
{e_1 \Downarrow^n_{\Psi} e_1'
 &
 e_2 \Downarrow^n_{\Psi} e_2' \mathstrut}
\]
Note that the partial evaluation rule for $\lambda x.e$ extends
the variable context $\Psi$. The \sls~encoding of the judgment $e
\Downarrow^n_{\Psi} e'$ is given in
Figure~\ref{fig:lc-ev2}, which also introduces an auxiliary ${\sf var}$
judgment that tracks all the variables in $\Psi$. 

\begin{figure}[tp]
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/lc-ev2.sls}
\caption{Partial evaluation for \rowan~(lambda calculus fragment).}
\label{fig:lc-ev2}
\end{figure}

\rowan~gets interesting when we consider the temporal fragment, which
mediates between the $e \Downarrow_\Psi v$ and $e \Downarrow^n_\Psi
e'$ judgments with two expressions. The first, ${\sf next}\,e$, says
that the enclosed expression $e$ should be evaluated one time step
later than the surrounding expression.  The second, ${\sf prev}\,e$,
says that the enclosed expression should be evaluated one time step
{\it before} the surrounding expression. There's no way to evaluate
${\sf prev}\,e \Downarrow_{\Psi} v$ at time 0, so when we evaluate
${\sf prev}\,e$ at time 1 it is necessary that $e$ evaluates
to ${\sf next}\,e'$, as ${\sf prev}\,({\sf next}\,e)$ at time step 1 
will reduce to $e$.
\[
\infer
{{\sf next}\,e \Downarrow_\Psi {\sf next}\,e'}
{e \Downarrow^1_{\Psi} e'}
\quad
\infer
{{\sf next}\,e \Downarrow^n_\Psi {\sf next}\,e'}
{e \Downarrow^{n+1}_{\Psi} e'}
\quad
\infer
{{\sf prev}\,e \Downarrow^1_{\Psi} e'}
{e \Downarrow_{\Psi} {\sf next}\, e'}
\quad
\infer
{{\sf prev}\,e \Downarrow^{n+2}_{\Psi} {\sf prev}\,e'}
{{\sf prev}\,e \Downarrow^{n+1}_{\Psi} {\sf prev}\,e'}
\]

\begin{figure}[tp]
\begin{minipage}[b]{0.450\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/lc-ev3.sls}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.50\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/lc-ssos-3.sls}
\end{minipage}
\caption{Semantics for \rowan~(temporal fragment).}
\label{fig:lc}
\end{figure}

This natural semantics specification is represented on the left-hand
side of Figure~\ref{fig:lc}. Due to the structure of the ${\sf
  evn/lam}$ rule, we {\it cannot} operationalize the ${\sf evn}$
predicate: it does not have the structure of a $C$ proposition as
described in Section~\ref{sec:trans-subset}. Rule ${\sf evn/lam}$ does
have the structure of a $D$ proposition, so it is possible to
operationalize the ${\sf ev}$ predicate without operationalizing the
${\sf evn}$ predicate. This leaves the rules in
Figure~\ref{fig:lc-ev2} completely unchanged; the right-hand side of
Figure~\ref{fig-lc} contains the transformed temporal fragment,
where ${\sf evn/next}$ and ${\sf evn/prev}$ rules are similarly unchanged. 
The
${\sf ev/next}$ rule, however,
contains a subgoal ${!}{\sf evn}\,({\sf s}\,{\sf
  z})\,e\,v$ which uses a deductive derivation to build a concurrent
step. Conversely, the ${\sf ev/prev}$ rule contains a subgoal of ${\sf
  eval}\,e \lefti \{ {\sf retn}\,({\sf next}\,v) \}$ that uses
a concurrent derivation to create a deductive derivation.  This makes the
right-hand side of Figure~\ref{fig:lc} the only \sls~specification in
this thesis that exhibits an arbitrarily nested dependency between
concurrent and deductive reasoning.

The natural semantics of \rowan~are not, on a superficial level,
significantly more complex than other natural semantics.   It
turns out that the usual set of techniques for adding state
to a natural semantics break down for \rowan. Discussing a \rowan-like
logic with state remained a challenge for many years, though full
solution has been given by Kameyama et al.~using delimited control
operators \cite{kameyama11shifting}. Our discussion of
operationalization gives a perspective on why this is difficult, as the
specification is far outside of the image of the extended natural
semantics we considered in Section~\ref{sec:enriching-natsem}.  We
normally add state to ordered abstract machine specifications by
fiddling with the set of ambient linear and persistent resources.  If
we tried to add state to \rowan~the same way we added it in
Section~\ref{sec:mutable-storage}, the entire store would effectively
leave scope whenever computation considered the subterm $e$ of ${\sf
  next}(e)$.

I conjecture that the nominal generalization of ordered linear lax
logic alluded to in the discussion of locations and existential name
generation (Section~\ref{sec:mutable-storage}) could support
operationalizing predicates like ${\sf evn}\,n\,e\,e'$. This might, in
turn, make it possible to add state to an SSOS specification of
\rowan, but that is left for future work.
