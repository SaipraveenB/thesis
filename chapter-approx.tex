\chapter{Linear logical approximation}
\label{chapter-approx}

The abstract interpretation of programs relates the exact semantics of
a programming language to a finite approximation of the semantics. A
general recipe for constructing a sound program analysis is to (1)
specify the operational semantics of the underlying programming
language via an interpreter, and (2) specify a terminating
approximation of the interpreter itself. This is the basic idea behind
{\it abstract interpretation} \cite{cousot77abstract}, which provides
techniques for constructing approximations (for example, by exhibiting
a Galois connection between concrete and abstract domains). The
correctness proof must establish the appropriate relationship between
the concrete and abstract computations and show that the abstract
computation terminates. We need to vary both the specification of
the operational semantics and the form of the approximation in order
to obtain various kinds of program analyses, sometimes with
considerable ingenuity.

In this chapter, which is mostly derived from \cite{simmons11logical},
we consider a new class of instances in the general schema of abstract
interpretation that is based on the approximation of SSOS
specifications in \sls. We apply logically justified techniques for
manipulating and approximating SSOS specifications to yield
approximations that are correct by construction. The resulting
persistent logical specifications can be interpreted and executed as
saturating logic programs, which means that derived specifications
are executable program analyses.
%, generalizing proposals of McAllester and Ganzinger
%\cite{mcallester02complexity,ganzinger02logical} to specifications
%that use higher-order abstract syntax.

The process described in this chapter does not claim to capture or be
able to derive all possible interesting program analyses. For
instance, the methodology we describe only derives over-approximations
(or {\it may-} analyses) that ensure all possible behaviors will be
reported by the analysis. There is a whole separate class of program
under-approximations (or {\it must-} analyses) that ensure that if a
behavior is reported by the analysis it is possible; these are more
closely associated with bug-finding and we will not consider them here
\cite{godefroid10compositional}. Instead, we argue for the utility of
our methodology by deriving two fundamental and rather different
over-approximation-based analyses: a context-insensitive control flow
analysis (Section~\ref{sec:0cfa}) and an alias analysis
(Section~\ref{sec:aliasanalysis}). Might and Van Horn's closely
related ``abstracting abstract machines'' methodology, described in
Section~\ref{sec:approximately-related} along with other related work,
suggests many more examples.

\section{Saturating logic programming}

Concurrent \sls~specifications where all positive atomic propositions
are persistent (and where all inclusions of negative propositions in
positive propositions -- if there are any -- have the form ${!}A^-$, not
${\downarrow}A^-$ or ${\gnab}A^-$) have a distinct logical and
operational character. Logically, by the discussion in
Section~\ref{sec:perm-fragments} we are justified in reading such
specifications as specifications in persistent intuitionistic logic or
persistent lax logic. Operationally, while persistent specifications
have an interpretation as transition systems, that interpretation is
not very useful. This is because if we can take a transition once --
for instance, using the rule ${\sf a} \lefti \{ {\sf b} \}$ to derive
the persistent atomic proposition ${\sf b}$ from the persistent atomic
proposition ${\sf a}$ -- none of the facts that enabled that
transition can be consumed, as all facts are persistent. Therefore, we
can continue to make the same transition indefinitely; in the
above-mentioned example, such transitions will derive multiple
redundant copies of ${\sf b}$.

The way we will understand the meaning of persistent and concurrent
\sls~specifications is in terms of {\it saturation}. A process state
$(\Psi; \Delta)$ is saturated relative to the signature $\Sigma$ if,
for any step $(\Psi; \Delta) \leadsto_\Sigma (\Psi'; \Delta')$, it is
the case that $\Psi$ and $\Psi'$ are the same (the step unified no
distinct variables and introduced no new variables),
$x{:}\susp{p^+_\mpers} \in \Delta'$ implies $x{:}\susp{p^+_\mpers} \in
\Delta$, and $x{:}\ispers{A^-} \in \Delta'$ implies $x{:}\ispers{A^-}
\in \Delta$. This means that a signature with a rule that produces new
variables by existential quantification, like ${\sf a} \lefti \{
\exists \lf{x}. {\sf b}\lf{(x)} \}$ has no saturated process states
where ${\sf a}$ is present. We will cope with rules of this form by
turning them into rules of the form ${\sf a} \lefti \{ \exists
\lf{x}. {\sf b}\lf{(x)} \fuse \lf{x} \doteq \lf{t} \}$ for some
$\lf{t}$, neutralizing the free existential variable as a notational
definition. Notions of saturation that can cope with {\it free}
existentially generated variables are interesting, but are beyond the
scope of this thesis.

A {\it minimal} saturated process state is one with no duplicated
propositions; we can compute a minimal process state from any
saturated process state by removing duplicates. For purely persistent
specifications and process states, minimal saturated process states
are unique when they exist: if $(\Psi; \Delta) \leadsto^*_\Sigma
(\Psi_1; \Delta_1)$ and $(\Psi; \Delta) \leadsto^*_\Sigma (\Psi_2;
\Delta_2)$ and both $(\Psi_1; \Delta_1)$ and $(\Psi_2; \Delta_2)$ are
saturated, then $(\Psi_1; \Delta_1)$ and $(\Psi_2; \Delta_2)$ have
minimal process states that differ only in the names of variables.

Furthermore, if a saturated process state exists for a given initial
process state, the minimal saturated process state can be computed by
the usual forward-chaining semantics where only transitions that
derive ${\it new}$ persistent atomic propositions or equalities
$\lf{t} \doteq \lf{s}$ are allowed. This forward-chaining logic
programming interpretation of persistent logic is extremely common,
usually associated with the logic programming language
Datalog.  A generalization of Datalog formed the basis of McAllester
and Ganzinger's meta-complexity results: they gave a cost semantics to
their logic programming language, and then they used that cost
semantics to argue that many program analyses could be {\it
  efficiently} implemented as logic programs
\cite{mcallester02complexity,ganzinger02logical}. Persistent
\sls~specifications can be seen as an extension of McAllester and
Ganzinger's language (and, transitively, as a generalization of
Datalog). We will not deal with cost semantics or efficiency, however,
as our use of higher-order abstract syntax complicates McAllester and
Ganzinger's cost semantics substantially.

Just as the term {\it persistent logic} was introduced in 
Chapter~\ref{chapter-foc} to
distinguish what is traditionally referred to as intuitionistic logic
from intuitionistic ordered and linear logic, we will use the term
{\it saturating logic programming} to distinguish what is
traditionally referred to as forward-chaining logic programming from
the forward-chaining logic programming interpretation that makes sense
for ordered and linear logical specifications. There is a useful
variant of substructural forward chaining, forward chaining with {\it
  quiescence} \cite{lopez05monadic}, that acts like saturating logic
programming on purely-persistent specifications and like simple
committed-choice forward chaining on specifications with no persistent
propositions. We refined this interpretation and gave it a cost
semantics in \cite{simmons08linear}, but this more sophisticated
interpretation is not relevant to the examples in this thesis.

\section{Using approximation}
\label{sec:pda-approxme}


The meta-approximation theorem that we present in the next section
gives us a way of building abstractions from specifications and
initial process states: we interpret the approximate version of the
program as a saturating logic program over that initial state. If we
can obtain a saturated process state using the logic programming
interpretation, it is an abstraction of the initial process state. It
is not always possible to obtain a saturated process state using the
logic programming interpretation, however: rules like $\forall
\lf{x}.\,{\sf a}\lf{(x)} \lefti \{ {\sf a}\lf{({\sf s}(x))} \}$ and
$\forall \lf{x}.\,{\sf a}\lf{(x)} \lefti \{ \exists \lf{y}. {\sf
  a}\lf{(y)} \}$ lead to non-termination when interpreted as
saturating logic programs. Important classes of programs are known to
terminate in all cases, such as those in the Datalog fragment where
the only terms in the program are variables and constants. Structured
terms (like expressions encoded in the LF type ${\sf exp}$) fall
outside the Datalog fragment.

\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/pda-pers.sls}
\caption{Skolemized approximate version of the PDA specification from
Figure~\ref{fig:pda-lin}}
\label{fig:pda-pers}
\end{figure}

Consider the destination-passing PDA specification from
Figure~\ref{fig:pda-lin}. If we simply turn all linear predicates
persistent, the first step in the approximation methodology, then the
${\sf push}$ rule will lead to non-termination because the head
$\exists \lf{m}. {\sf stack}\,\lf{x}\,\lf{l}\,\lf{m} \fuse {\sf
  hd}\,\lf{m}\,\lf{r}$ introduces a new existential parameter
$\lf{m}$.  We can cope by adding a new conclusion $\lf{m} \doteq
\lf{t}$, as adding new conclusions is the second step in the
approximation methodology. This, however, means we have to pick a
$\lf{t}$.  The most general starting point for selecting a $\lf{t}$ is
to apply Skolemization to the rule. By moving the existential
quantifier for $\lf{m}$ in front of the implicitly quantified
$\lf{X}$, $\lf{L}$, $\lf{M}$, and $\lf{R}$, we get a Skolem function
$\lf{{\sf fm}\,X\,L\,M\,R}$ that takes four arguments. Letting $\lf{t}
= \lf{{\sf fm}\,X\,L\,M\,R}$ results in the \sls~specification/logic
program shown in Figure~\ref{fig:pda-pers}. (Remember that, because
the specification in Figure~\ref{fig:pda-pers} is purely persistent,
we will omit the optional \verb|!| annotation described in
Section~\ref{sec:prototype}, writing \verb|hd L M| instead of
\verb|!hd L M| and so on.)

Notice that we have effectively taken a specification that freely
introduces existential quantification (and that therefore {\it
  definitely} will not terminate when interpreted as a saturating
logic program) and produced a specification that uses structured terms
$\lf{{\sf fm}\,X\,L\,R\,M}$. But the
introduction of structured terms takes us outside the Datalog fragment,
which may also lead to non-termination! This is not as bad as it may
seem: when we want to treat a specification with structured terms as a
saturating logic program, it is simply necessary to reason explicitly
about termination. Giving any finite upper bound on the number of
derivable facts is a simple and sufficient criteria for showing that a
saturating logic program terminates.

Skolem functions provide a natural starting point for approximations,
even though the Skolem constant that arises directly from
Skolemization is usually more precise than we want. From the starting
point in Figure~\ref{fig:pda-pers}, however, we can define
approximations simply by instantiating the Skolem constant.  For
instance, we can equate the existentially generated destination in the
conclusion with the one given in the premise (letting $\lf{\sf fm} =
\lf{\lambda X. \lambda L. \lambda M. \lambda R.\, M}$). The result is
equivalent to this specification:

\smallskip
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/pda-pers-precise.sls}
\smallskip

\noindent This substitution yields a precise approximation that
exactly captures the behavior of the original PDA as a saturating
logic program. 

To be concrete about what this means, let us recall how the PDA works
and what it means for it to accept a string.  To use the linear PDA
specification in Figure~\ref{fig:pda-lin}, we encode a string as a
sequence of linear atomic propositions ${\it ptok}_1\ldots {\it
  ptok}_n$, where each ${\it ptok}_i$ either has the form ${\sf
  left}\,\lf{tok_i}\,{\sf d_i}\,{\sf d_{i+1}}$ or the form ${\sf
  right}\,\lf{tok_i}\,{\sf d_i}\,{\sf d_{i+1}}$. The term $\lf{tok_i}$
that indicates whether we're talking about a left/right parenthesis,
curly brace, square brace, etc., and $\lf{{\sf d_0}\ldots{\sf
    d_{n+1}}}$ are $n+2$ constants of type {\it dest}.\footnote{We
  previously saw destinations as only inhabited by parameters, but the
  guarantees given by the meta-approximation theorem are clearer when
  the initial state contains destinations that are constants.}  Let
$\Delta = (h{:}\iseph{\susp{{\sf hd}\,\lf{\sf d_0}\,\lf{\sf d_1}}},
x_1{:}{\iseph{\susp{{\it ptok}_1}}}, \ldots,
x_n{:}{\iseph{\susp{{\it ptok}_n}}})$. The PDA
accepts the string encoded as ${\it ptok}_1\ldots {\it ptok}_n$ if and
only if there is a trace under the signature in
Figure~\ref{fig:pda-lin} where $\Delta \leadsto^* (\Psi';
x{:}\iseph{\susp{{{\sf hd}\,\lf{\sf d_0}\,\lf{\sf d_{n+1}}}}})$.

Now, say that we turn the predicates persistent and  run the program
described by the ${\sf push}$ and ${\sf pop}$ rules above as a
saturating logic program, obtaining a saturated process state
$\Delta_{\it sat}$ from the initial process state $(h{:}\ispers{\susp{{\sf
      hd}\,\lf{d_0}\,\lf{d_1}}}, x_1{:}\ispers{\susp{{\it ptok}_1}}, \ldots,
x_n{:}\ispers{\susp{{\it ptok}_n}})$. (We can see from the
structure of the program that LF context will remain empty.)  The
meta-approximation theorem {\it ensures} that, if the original PDA
accepted, then the proposition ${\sf hd}\,\lf{\sf d_0}\,\lf{\sf d_{n+1}}$ is
in $\Delta_{\it sat}$. It just so happens to be the case that the
converse is also true -- if ${\sf hd}\,\lf{\sf d_0}\,\lf{\sf d_{n+1}}$ is in
$\Delta_{\it sat}$, the original PDA specification accepts the string. 
That is why we say we have
a precise approximation.

On the other hand, if we set $\lf{m}$ equal to $\lf{l}$ 
(letting $\lf{\sf fm} = \lf{\lambda X. \lambda L. \lambda M. \lambda R.\, L}$), 
the result is equivalent to this specification:

\smallskip
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/pda-pers-approx.sls}
\smallskip

\noindent
If the initial process state contains a single atomic proposition
${\sf hd}\,\lf{\sf d_0}\,\lf{\sf d_1}$ in addition to all the ${\sf
  left}$ and ${\sf right}$ facts, then the two rules above maintain
the invariant that, as new facts are derived, the first argument of
${\sf hd}$ and the second and third arguments of ${\sf stack}$ will
{\it always} be $\lf{\sf d_0}$.  These arguments are therefore vestigial,
like the extra arguments to ${\sf eval}$ and ${\sf retn}$ discussed in
Section~\ref{sec:vestigial}, and we can remove them from the
approximate specification, resulting in the specification in
Figure~\ref{fig:pda-pers-approx2}.

\begin{figure}[ht]
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/pda-pers-approx2.sls}
\caption{Approximated PDA specification}
\label{fig:pda-pers-approx2}
\end{figure}

This logical approximation of the original PDA accepts if we run
saturating logic programming from the initial process state
$(h{:}\ispers{\susp{{\sf hd}\,\lf{\sf d_1}}}, x_1{:}\ispers{\susp{{\it
      ptok}_1}}, \ldots, x_n{:}\ispers{\susp{{\it ptok}_n}})$ and
${\sf hd}\,\lf{\sf d_{n+1}}$ appears in the saturated process
state. Again, the meta-approximation theorem ensures that any string
accepted by the original PDA will also be accepted by any
approximation. This approximation will additionally accept every
string where, for every form of bracket $\lf{tok}$, at least one left
$\lf{tok}$ appears before any of the right $\lf{tok}$. The string
\obj{\mbox{\sf [ ] ] ] ( ( )}} would be accepted by this approximated
PDA, but the string \obj{\mbox{\sf ( ) ] [ [ ]}} would not, as the
first right square bracket appears before any left square
bracket. 

\section{Logical transformation: approximation}
\label{sec:abstraction}

The approximation strategy demonstrated in the previous section is
quite simple: a signature in an ordered or linear logical
specification can be approximated by making all atomic propositions
persistent, and a flat rule $\forall \lf{\overline{x}}. \,A^+ \lefti
\{ B^+ \}$ containing only persistent atomic propositions can be
further approximated by removing premises from $A^+$ and adding
conclusions to $B^+$. Of particular practical importance are added
conclusions that neutralize an existential
quantification with a notational definition. The approximation procedure
doesn't force us to neutralize all such variables in this
way. However, as we explained above, failing to do so almost ensures
that the specification cannot be run as a saturating logic program,
and being able to interpret specifications as saturating logic
programs is a prerequisite for applying the meta-approximation theorem
(Theorem~\ref{thm:metapprox}).

First, we define what it means for a specification to be an approximate
version of another specification:

\bigskip
\begin{definition}\label{def:approxversion}
  A flat, concurrent, and persistent specification $\Sigma_a$ is an
  {\em approximate version} of another specification $\Sigma$ if every
  predicate ${\sf a} : \Pi \lf{x_1}{:}\tau_1 \ldots \Pi \lf{x_n}{:}\tau_n.\,
  {\sf prop}\,{\sf lvl}$ declared in $\Sigma$ has a corresponding
  predicate ${\sf a} : \Pi \lf{x_1}{:}\tau_1 \ldots \Pi \lf{x_n}{:}\tau_n.\,
  {\sf prop}\,{\sf pers}$ in $\Sigma_a$ and if for every rule ${\sf
    r} : \forall \overline{x}.\,A_1^+ \lefti \{ A_2^+ \}$ in $\Sigma$ there
  is a corresponding rule ${\sf r} : \forall \lf{\overline{x}}.\,B_1^+ \lefti
  \{ B_2^+ \}$ in $\Sigma_a$ such that:
  \begin{itemize}
  \item The existential quantifiers in $A_1^+$ and $A_2^+$ are
    identical to the existential quantifiers in $B_1^+$ and $B_2^+$
    (respectively),
  \item For each premise ($p^+_\mpers$ or $\lf{t} \doteq \lf{s}$) in $B^+_1$,
    the same premise appears in $A^+_1$, and 
  \item For each conclusion ($p^+_\mlvl$ or $\lf{t}
    \doteq \lf{s}$) in $A^+_2$, the same premise appears in $B^+_2$.
  \end{itemize}
\end{definition}
\bigskip

\noindent
While approximation is a program transformation, it is not a
deterministic one: Definition~\ref{def:approxversion} describes a
whole family of potential approximations. Even the nondeterministic
operationalization transformation was just a bit nondeterministic,
giving several options for operationalizing any given deductive
rule. The approximation transformation, in contrast, needs explicit
information from the user: which premises should be removed, and what
new conclusions should be introduced? While there is value in actually
implementing the operationalization, defunctionalization, and
destination-adding transformations, applying approximation requires
intelligence. Borrowing a phrase from Danvy, approximation is a
candidate for ``mechanization by graduate student'' rather than
mechanization by computer.

Next, we give a definition of what it means for a state to be an 
approximate version (we use the word ``generalization'') of another state
or a family of states. 

\bigskip
\begin{definition}
  The persistent process state $(\Psi_g; \Delta_g)$ is a {\em
    generalization} of the process state $(\Psi; \Delta)$ if there is
  a substitution $\Psi_g \vdash \lf{\sigma} : \Psi$ such that, for all
  atomic propositions $p^+_\mlvl = {\sf a}\,\lf{t_1}\ldots \lf{t_n}$
  in $\Delta$, there exists a persistent proposition
  $p^+_\mpers = {\sf a}\,\lf{(\sigma t_1)}\ldots\lf{(\sigma t_n)}$ in
  $\Delta_g$.
\end{definition}
\bigskip

One thing we might prove about the relationship between process states
and their generalizations is that generalizations can {\it simulate}
the process states they generalize: that is, if $(\Psi_g; \Delta_g)$
is a generalization of $(\Psi; \Delta)$ and $(\Psi; \Delta)
\leadsto_\Sigma (\Psi'; \Delta')$ then $(\Psi_g; \Delta_g)
\leadsto_{\Sigma_a} (\Psi'_g; \Delta'_g)$ where $(\Psi'_g; \Delta'_g)$
is a generalization of $(\Psi'; \Delta')$. This is true, and we will
prove it as a corollary on the way to the proof of
Theorem~\ref{thm:metapprox}.  However, we are not interested in
generalization per se; rather, we're interested in a stronger
property, {\em abstraction}, that is defined in terms of
generalization:

\bigskip
\begin{definition}
A process state $(\Psi_a; \Delta_a)$ is an {\em abstraction} of 
$(\Psi_0; \Delta_0)$ under the signature $\Sigma$ if, for any trace
$(\Psi_0; \Delta_0) \leadsto^*_\Sigma (\Psi_n; \Delta_n)$, 
$(\Psi_a; \Delta_a)$ is a generalization of $(\Psi_n; \Delta_n)$. 
\end{definition}
\bigskip

An abstraction of the process state $(\Psi_0; \Delta_0)$ is therefore
a single process state that captures {\it all possible future
  behaviors} of the state $(\Psi_0; \Delta_0)$ because, for any atomic
proposition $p^+_\mlvl = {\sf a}\,\lf{t_1}\ldots \lf{t_n}$ that may be derived
by evolving $(\Psi_0; \Delta_0)$, there is a substitution $\lf\sigma$
such that ${\sf a}\,\lf{(\sigma t_1)}\ldots\lf{(\sigma t_n)}$ is already present
in the abstraction. The meta-approximation theorem relates this definition
of abstraction to the concept of approximate versions of programs as
specified by Definition~\ref{def:approxversion}.  

\bigskip
\begin{theorem}[Meta-approximation]\label{thm:metapprox}
  If $\Sigma_a$ is an approximate version of $\Sigma$, and if there is a
  state $(\Psi_0; \Delta_0)$ well-formed according to $\Sigma$, and if
  for some $\Psi_0' \vdash \lf\sigma : \Psi_0$ there is a trace
  $(\Psi_0'; \lf\sigma\Delta_0) \leadsto^*_{\Sigma_a} (\Psi_a; \Delta_a)$
  such that $(\Psi_a; \Delta_a)$ is a saturated process state, then
  $(\Psi_a; \Delta_a)$ is an abstraction of $(\Psi_0; \Delta_0)$.
\end{theorem}

\begin{proof}
As in \cite[Theorem 3]{simmons11logical}.
\end{proof}

The meaning of the meta-approximation theorem is that if (1) we can
approximate a specification and an initial state and (2) we can obtain
a saturated process state from that approximate specification
and approximate initial state, then the saturated process state captures
all possible future behaviors of the (non-approximate) initial state. 






\section{Control flow analysis}
\label{sec:0cfa}

The initial process state for destination-passing SSOS specifications
generally has the form 
$(\lf{d}{:}{\sf dest}; x{:}\susp{{\sf eval}\,\lf{t}\,\lf{d}})$
for some program represented by the LF term $\lf{t} = \interp{e}$. 
This means that we
can use the meta-approximation result to derive abstractions from
initial expressions $\obj{e}$ using the saturating logic programming
interpretation of persistent SSOS specifications.

A control flow analysis is a fundamental analysis on functional
programs, attributed to Shivers \cite{shivers88control}. It is used
for taking an expression and ``determining for each subexpression a
hopefully small number of functions that it may evaluate to; thereby
it will determine where the flow of control may be transferred to in
the case where the subexpression is the operator of a function
application'' \cite[p. 142]{nielson05principles}. That is, we want to
take a program and find, for every subexpression $\obj{e}$ of that
unevaluated program, all the values $\obj{v}$ that the subexpression
may evaluate to over the course of evaluating the program to a
value. Because we are talking about subexpressions of the unevaluated
program, the answer might not be unique. Consider the evaluation of
$\obj{(\lambda f\ldots (f\,(\lambda y\ldots)) \ldots (f\,(\lambda
  z\ldots)) \ldots )\,(\lambda x.x)}$. The function $\obj{\lambda
  x.x}$ gets bound to $\obj{f}$ and therefore may get called twice,
once with the argument $\obj{(\lambda y\ldots)}$ and once with the
argument $\obj{(\lambda z\ldots)}$. The subexpression $\obj{x}$ of
$\obj{\lambda x.x}$ can therefore evaluate to $\obj{(\lambda
  y\ldots)}$ in the context of the call $\obj{f\,(\lambda y\ldots)}$
and to $\obj{(\lambda z\ldots)}$ in the context of the call
$\obj{f\,(\lambda z\ldots)}$. As a {\it may}-analysis, the output of a
control flow analysis is required to report both of these
possibilities.\footnote{This statement assumes that both of the
  calling sites $\obj{f\,(\lambda y\ldots)}$ and $\obj{f\,(\lambda
    z\ldots)}$ are reachable: the control flow analysis we derive
  performs some dead-code analysis, and it may not report that
  $\obj{x}$ evaluates to $\obj{(\lambda y\ldots)}$, for instance, if
  the call $\obj{f\,(\lambda y\ldots)}$ is certain to never occur.}

When we {\it use} a control flow analysis, it is relevant that the
calculation of which subexpressions evaluate to which values is done
in service of a different goal: namely, determining which functions
may be called from which calling sites. However, the primary goal of
control flow analysis is irrelevant to our discussion of deriving
control flow analyses from SSOS specifications, so we will concentrate
on the question of subexpressions evaluate to which values. Before we
begin, however, we will address the issue of what it even means to be
a (closed) subterm of an expression $\obj{e}$ that has been encoded
with higher-order abstract syntax into the canonical forms of LF.

\subsection{Subexpressions in higher-order abstract syntax}
\label{sec:hoas-subexpressions}

When given a term $\lf{{\sf a}\,({\sf b}\,{\sf c}\,{\sf c})}$, it is
clear that there are three distinct subterms: the entire term,
$\lf{{\sf b}\,{\sf c}\,{\sf c}}$, and $\lf{{\sf c}}$. Therefore, it is
meaningful to bound the size of a saturated process state using some
function that depends on the number of subterms of the original
term. But what are the subterms of $\lf{{\sf lam}\,(\lambda x.\,{\sf
    app}\,x\,x)}$, and how can we write a saturating logic program
that derives all those subterms? The rule for application is easy:
\begin{align*}
{\sf sub/app} &: ~~
  \forall \lf{e_1}{:}{\sf exp}.\, \forall \lf{e_2}{:}{\sf exp}.\,
    {\sf subterms}\lf{({\sf app}\,e_1\,e_2)} \lefti
      \{ {\sf subterms}\,\lf{e_1} \fuse {\sf subterms}\,\lf{e_2} \}
\intertext{What about the rule for lambda abstractions?
Experience with LF says that, when we
open up a binder, we should substitute a fresh variable into that
binder. This would correspond to the following rule:}
{\sf sub/lam/ohno} &: ~~
  \forall \lf{e}{:}{\sf exp} \rightarrow {\sf exp}.\,
    {\sf subterms}\lf{({\sf lam}(\lambda x.e\,x))} \lefti
      \{ \exists \lf{x}.\,{\sf subterms}\lf{(e\,x)} \}
\intertext{The rule 
${\sf sub/lam/ohno}$ 
%
will, as we have discussed, lead to nontermination when we interpret
the rules as a saturating logic program. The solution is to apply
Skolemization as described in Section~\ref{sec:pda-approxme}, which
introduces a new constant we will call ${\sf var}$. The rule ${\sf
  sub/lam/ohono}$ can then be approximated as a terminating rule:}
%
{\sf sub/lam} &: ~~ 
  \forall \lf{e}{:}{\sf exp} \rightarrow {\sf exp}.\,
    {\sf subterms}\lf{({\sf lam}(\lambda x.e\,x))} \lefti
      \{ {\sf subterms}\lf{(e({\sf var}(\lambda x.e\,x)))} \}
\end{align*}

The subterms of any closed term
$\lf{e}$ of LF type ${\sf exp}$ can then be enumerated by running this
saturating logic program starting with the fact ${\sf subterms}\lf{(e)}$,
where ${\sf subterms}$ is a persistent positive proposition.
%The last rule, ${\sf sub/var}$, is redundant: if we ever derive a fact
%of the form ${\sf subterms}\lf{({\sf var}(\lambda x.e\,x))}$, we know
%that we previously derived ${\sf subterms}\lf{({\sf lam}(\lambda
%  x.e\,x))}$ and therefore, by the first rule, we have already derived
%${\sf subterms}\lf{(e({\sf var}(\lambda x.e\,x))}$.  This means that
We start counting subterms from the outside, and stop 
%(because of saturation) 
when we reach a variable represented by a term $\lf{{\sf
    var}(\lambda x.e)}$.  The logic program and discussion above imply
that there are three distinct subterms of $\lf{{\sf lam}\,(\lambda
  x.\,{\sf app}\,x\,x)}$: the entire term, $\lf{{\sf app}\,({\sf
    var}(\lambda x.\,{\sf app}\,x\,x))\,({\sf var}(\lambda x.\,{\sf
    app}\,x\,x))}$, and $\lf{{\sf var}(\lambda x.\,{\sf app}\,x\,x)}$.

Another solution, discussed in the next
section, is to uniquely tag the lambda expression with a label. This
has the same effect of allowing us to associate the variable $x$
with an different concrete term, the tag, that represents the site
where $x$ was bound.

%Because we ensure that we {\it only} substitute terms
%of the form $\lf{{\sf var}(\lambda x.e\,x)}$ into functions of the form
%$\lf{\lambda x.e\,x}$ we can actually answer this question:





\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/dest-env.sls}
\caption{Alternative environment semantics for CBV evaluation}
\label{fig:dest-env}
\end{figure}

\subsection{Environment semantics}


The starting point for deriving a control flow analysis is the
environment semantics for call-by-value shown in
Figure~\ref{fig:dest-env}. It differs from the environment semantics
shown in Figure~\ref{fig:ssos-by-env} in three ways. First and
foremost, it is a destination-passing specification instead of an
ordered abstract machine specification, but that difference is
accounted for by the destination-adding transformation in 
Chapter~\ref{chapter-destinations}.
A second difference is that the existentially generated parameter
$\lf{x}$ associated with the persistent proposition ${\sf
  bind}\,\lf{x}\,\lf{v}$ is introduced as late as possible in the
multi-stage protocol for evaluating an application (rule ${\sf
  ev/app2}$ in Figure~\ref{fig:dest-env}), not as early as possible
(rule ${\sf ev/appenv1}$ in Figure~\ref{fig:ssos-by-env}). The third
difference is that there is an extra frame $\lf{\sf app3}$ and an
extra rule ${\sf ev/app3}$ that consumes such frames. The $\lf{\sf
  app3}$ frame is an important part of the control flow analysis we
derive, but in \cite{simmons11logical} the addition of these frames
was otherwise unmotivated. Based on our discussion of the logical
correspondence in 
Chapters~\ref{chapter-correspondence}~and~\ref{chapter-absmachine}, 
we now have a principled account for this
extra frame and rule: it is precisely the pattern we get from
operationalizing a natural semantics {\it without} tail-recursion
optimization and then applying defunctionalization.

\subsection{Approximation to 0CFA}

In order for us to approximate Figure~\ref{fig:dest-env} to derive a
finite control flow analysis, we turn all linear atomic propositions
persistent and then must deal with the variables introduced by
existential quantification. The variable $\lf{x}$ introduced in ${\sf
  ev/app2}$ will be equated with $\lf{{\sf var}(\lambda x.E\,x)}$,
which is consistent with making $\lf{E\,x}$ -- which is now equal to
$\lf{E({\sf var}(\lambda x.E\,x))}$ -- a subterm of $\lf{{\sf lam}(\lambda x.\, E\,x)}$.  The new
constructor $\lf{\sf var}$ is also a simplified Skolem
function for $\lf{x}$ that only mentions the LF term $\lf{E}$; the
most general Skolem function in this setting would have also been
dependent on $\lf{V}$, $\lf{D}$, and $\lf{D_2}$. The existentially
generated variable $\lf x$ was also the first argument to ${\sf bind}$,
so ${\sf bind}$, as a relation, will now associate binding sites
and values instead of unique variables and values. 

\begin{figure}[t]
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/dest-cfa-1.sls}
\caption{A control-flow analysis derived from Figure~\ref{fig:dest-env}}
\label{fig:dest-cfa-1}
\end{figure}

The discussion above pertains to the existentially generated variable
$\lf{x}$ in rule ${\sf ev/app2}$, but we still need some method for
handling destinations $\lf{d_1}$, $\lf{d_2}$, and $\lf{d_3}$ in 
${\sf ev/app}$, ${\sf
  ev/app1}$, and ${\sf ev/app2}$ (respectively). To
this end, we need recall the question that we intend to answer with
control flow analysis: what values may a given subexpression evaluate
to? A destination passing specification attempts to return a value to
a destination: we will instead return {\it to an expression} by
equating destinations $d$ with the expressions they represent. One way
to do this would be to introduce a new constructor $\lf{\sf d}: {\sf
  exp} \rightarrow {\sf dest}$, but we can equivalently conflate the
two types ${\sf exp}$ and ${\sf dest}$ to get the specification in
Figure~\ref{fig:dest-cfa-1}.


The specification in Figure~\ref{fig:dest-cfa-1} has a point of
redundancy along the lines of the redundancy in our second PDA
approximation: the rules maintain the invariants that the two
arguments to ${\sf eval}\,\lf{e}\,\lf{d}$ are always the same. Therefore, the
second argument to ${\sf eval}$ can be treated as vestigial; by
removing that argument, we get a specification equivalent to
Figure~\ref{fig:dest-cfa-2}. That figure includes another
simplifications as well: instead of introducing expressions $\lf{d_1}$,
$\lf{d_2}$, and $\lf{d_3}$ by existential quantification just to equate them
with expressions $\lf{e_1}$, $\lf{e_2}$, and $\lf{e}$, 
we substitute in the equated
expressions where the respective destinations appeared in
Figure~\ref{fig:dest-cfa-1}; this modification does not change
anything at the level of  synthetic inference rules.

\begin{figure}[t]
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/dest-cfa-2.sls}
\caption{Simplification of Figure~\ref{fig:dest-cfa-1} that
  eliminates the redundant argument to ${\sf eval}$}
\label{fig:dest-cfa-2}
\end{figure}

Let's consider the termination of specification in
Figure~\ref{fig:dest-cfa-2} interpreted as a saturating logic program.
Fundamentally, the terms in the heads of rules are all subterms (in
the generalized sense of Section~\ref{sec:hoas-subexpressions}), which
is a sufficient condition for the termination of a saturating logic
program. More specifically, consider that we start the database with a
single fact ${\sf eval}\,\interp{e}$, where $\interp{e}$ has $n$
subterms by the analysis in Section~\ref{sec:hoas-subexpressions}.  We
can only ever derive $n$ new facts ${\sf eval}\,\lf{e}$ -- one for
every subterm. If we deduced that every subexpression was a value that
could be returned at every subexpression, there would still be only
$n^2$ facts ${\sf retn}\,\lf{e}\,\lf{e'}$, and the same analysis holds
for facts of the form ${\sf cont}\,\lf{\sf app3}\,\lf{e}\,\lf{e'}$.  A
fact of the form ${\sf cont}\,\lf{({\sf
    app1}\,e_2)}\,\lf{e_1}\,\lf{e}$ will only be derived when $\lf{e}
= \lf{{\sf app}\,e_1\,e_2}$, so there are at most $n$ of these
facts. A fact of the form ${\sf cont}\,\lf{{\sf app2}\,\lambda
  x.\,e_0\,x}\,\lf{e_2}\,\lf{e}$ will only be derived when $\lf{e} =
\lf{{\sf app}\,\lf{e_1}\,\lf{e_2}}$ for some $\lf{e_1}$ that is also a
subterm, so there are most $n^2$ of these facts too. This means that
we can derive no more than $2n + 3n^2$ facts starting from a database
containing ${\sf eval}\,\interp{e}$, where $\obj{e}$ has $n$ subterms.
We could give a much more precise analysis than this, but this
imprecise analysis certainly bounds the size of the database, ensuring
termination, which was our goal.

There is one important caveat to this analysis. If for some value
$\obj{v}$ we consider the program $\interp{((\lambda x.x)\,(\lambda
  y.y))\,v}$, we might expect a reasonable control flow analysis to
notice that only $\interp{\lambda y.y}$ is passed to the function
$\interp{\lambda x.x}$ and that only $\obj{v}$ is passed to the
function $\interp{\lambda y.y}$. Because of our use of higher-order
abstract syntax, however, $\interp{\lambda y.y}$ and $\interp{\lambda
  x.x}$ are $\alpha$-equivalent and therefore equal in the eyes of the
logic programming interpreter. This is not a problem with correctness,
but it means that our analysis may be less precise than expected,
because it distinguishes only subterms, not {\it subterm
  occurrences}. One solution would be to add distinct labels to terms,
marking the $\alpha$-equivalent $\obj{\lambda x.x}$ and $\obj{\lambda
  y.y}$ with their distinct positions in the overall term. Adding a
label on the inside of every lambda-abstraction would seem to suffice,
and in any real example labels would already be present in the form of
source-code positions or line numbers. The alias analysis presented in
the next section demonstrates the use of such labels.

\subsection{Correctness}

The termination analysis for to the derived specification in
Figure~\ref{fig:dest-cfa-2}, together with the meta-approximation
theorem (Theorem~\ref{thm:metapprox}), ensures that we have derived
some sort of program analysis. How do we know that it is a control 
flow analysis? 

The easy option is to simply inspect the analysis and compare it to
the behavior of the SSOS semantics whose behavior the analysis is
approximating.  Note that the third argument $\lf{e}$ to ${\sf
  cont}\,\lf{f}\,\lf{e'}\,\lf{e}$ is always a term $\lf{{\sf
    app}\,e_1\,e_2}$ -- that is, a call site. The rule ${\sf ev/app2}$
starts evaluating the function $\lf{{\sf lam}(\lambda x.e_0\,x)}$ and
generates the fact ${\sf cont}\,\lf{\sf app3}\,\lf{(e({\sf
    var}(\lambda x.e_0\,x)))}\,\lf{e}$. This means that, in the course
of evaluating some initial expression $\lf{e_{\it init}}$, the
function $\lf{{\sf lam}(\lambda x.e_0\,x)}$ may be called from the
call site $\lf{e}$ only if ${\sf cont}\,\lf{\sf app3}\,\lf{(e({\sf
    var}(\lambda x.e_0\,x)))}\,\lf{e}$ appears in a saturated process
state that includes the persistent atomic proposition ${\sf
  eval}\lf{(e_{\it init})}$.


The analysis above is a bit informal, however. Following Nielson et
al., an {\it acceptable control flow analysis} takes the form of two
functions. The first, $\widehat{\sf C}$, is a function from
expressions $\obj{e}$ to sets of values $\{\obj{v_1},
\ldots,\obj{v_n}\}$, and the second, $\widehat{\rho}$, is a function
from variables $\obj{x}$ to sets of values $\{\obj{v_1},
\ldots,\obj{v_n}\}$. $\widehat{\sf C}$ and $\widehat{\rho}$ are said
to represent an acceptable control flow analysis for the expression
$\obj{e}$ if a coinductively defined judgment $(\widehat{\sf
  C},\widehat{\rho}) \models \obj{e}$ holds.

We would like to interpret a saturated program state $\Delta$ as a
(potentially acceptable) control flow analysis as follows (keeping in
mind that, given our current interpretation of subterms, $\interp{x} =
\lf{{\sf var}(\lambda x.\,E\,x)}$ for some $\lf{E}$):
%
\smallskip
%
\begin{itemize}
\item $\widehat{\sf C}(\obj{e}) = \{ \obj{v} \mid {\sf retn}\,\interp{v}\,\interp{e} \}$, and
\item $\widehat{\rho}(\obj{x}) = \{ \obj{v} \mid {\sf bind}\,\interp{x}\,\interp{v} \}$.
\end{itemize}
%
\smallskip
%
Directly adapting Nielson et al.'s definition of an acceptable control
flow analysis from \cite[Table 3.1]{nielson05principles} turns out not
to work. The control flow analysis we derived in
Figure~\ref{fig:dest-cfa-2} is rather sensitive to non-termination: if
we let $\obj{\omega} = \obj{(\lambda x.\,x\,x)\,(\lambda x.\,x\,x)}$,
then our derived control flow analysis will not analyze the argument
$\obj{e_2}$ in an expression $\obj{\omega\,e_2}$, nor will it analyze
the function body $\obj{e}$ in an expression $\obj{(\lambda
  x.e)\,\omega}$. Nielson et al.'s definition, on the other hand,
demands that both $\obj{e_2}$ in $\obj{\omega\,e_2}$ and $\obj{e}$ in
$\obj{(\lambda x.e)\,\omega}$ analyzed. In Nielson et al.'s Exercise
3.4, they then point out that a modified analysis, which takes order
of evaluation into account, is possible.

\begin{figure}
\begin{align*}
& [{\it var}] & (\widehat{\sf C}, \widehat{\rho}) & \models
  \obj{x} 
  \mbox{~~iff~~} \widehat{\rho}(\obj{x})
    \subseteq \widehat{\sf C}(\obj{x})
\\
& [{\it lam}] & (\widehat{\sf C}, \widehat{\rho}) & \models
  \obj{\lambda x.e}
  \mbox{~~iff~~} \{ \obj{(\lambda x.e)} \} 
    \subseteq \widehat{\sf C}(\obj{\lambda x.e})
\\
\qquad\qquad\qquad& [{\it app}] & (\widehat{\sf C}, \widehat{\rho}) & \models
  \obj{e_1\,e_2} \mbox{~~iff}
\\
& & & \qquad (\widehat{\sf C}, \widehat{\rho}) \models \obj{e_1} \wedge ~
\\
& & & \qquad (\forall(\obj{\lambda x.e_0}) \in \widehat{\sf C}(\obj{e_1}) : ~
\\
& & & \qquad\qquad (\widehat{\sf C}, \widehat{\rho}) \models \obj{e_2} \wedge ~
\\
& & & \qquad\qquad (\widehat{\sf C}(\obj{e_2}) \subseteq \widehat{\rho}(\obj{x})) \wedge ~
\\
& & & \qquad\qquad (\forall(\obj{v}) \in \widehat{\sf C}(\obj{e_2}) : 
\\
& & & \qquad\qquad\qquad (\widehat{\sf C}, \widehat{\rho}) \models \obj{e_0} \wedge ~
\\
& & & \qquad\qquad\qquad(\widehat{\sf C}(\obj{e_0}) \subseteq \widehat{\sf C}(\obj{e_1\,e_2}))))\qquad\qquad\qquad
\end{align*}
\caption{Coinductive definition of an acceptable control flow analysis}
\label{fig:acceptablecontrolflowanalysis}
\end{figure}

We can carry out Nielson et al.'s Exercise 3.4 to get the definition
of an acceptable control flow analysis given in
Figure~\ref{fig:acceptablecontrolflowanalysis}. Relative to this
definition, it is possible to prove that the abstractions computed by
the derived \sls~specification in Figure~\ref{fig:dest-cfa-2} are
acceptable control flow analyses.

\bigskip
\begin{theorem}\label{thm:acceptablecfa}
If $\Delta$ is a saturated process state that is well-formed according to
the signature in Figure~\ref{fig:dest-cfa-2}, and if $\widehat{\sf C}$
and $\widehat{\rho}$ are defined in terms of $\Delta$ as described above,
then ${\sf eval}\,\interp{e} \in \Delta$ implies
that $(\widehat{\sf C}, \widehat{\rho}) \models \obj{e}$. 
\end{theorem}

\begin{proof}
  By coinduction on the definition of acceptability in
  Figure~\ref{fig:acceptablecontrolflowanalysis}, and case analysis on 
  the form of $\obj{e}$. 

  \begin{itemize}
  \item[--] $\obj{e} = \obj{x}$, 
  so $\interp{e} = \interp{x} = \lf{{\sf var}(\lambda x.\,E_0\,x)}$

  We have to show $\widehat{\rho}(\obj{x}) \subseteq \widehat{\sf
    C}(\obj{x})$. In other words, if ${\sf
    bind}\,\interp{x}\,\interp{v} \in \Delta$, then ${\sf
    retn}\,\interp{v}\,\interp{x} \in \Delta$. Because ${\sf
    eval}\,\interp{e} \in \Delta$, this follows by the presence of
  rule ${\sf ev/bind}$ -- if ${\sf eval}\,\interp{e} \in \Delta$ and
  ${\sf bind}\,\interp{x}\,\interp{v} \in \Delta$, then ${\sf
    retn}\,\interp{v}\,\interp{x} \in \Delta$ as well; if it were not,
  the process state would not be saturated!

  \bigskip

  \item[--] $\obj e = \obj{\lambda x.e}$, so 
   $\interp{e} = \interp{\lambda x.e_0} = \lf{{\sf lam}(\lambda x.\,E_0\,x)}$

    We have to show $\{ \obj{(\lambda x.e)} \} \subseteq \widehat{\sf
      C}(\obj{\lambda x.e})$. In other words,
    ${\sf retn}\,\interp{\lambda x.e}\,\interp{\lambda x.e} \in
    \Delta$. This follows by rule ${\sf ev/lam}$ by the same reasoning given 
   in more detail above. 
  
  \bigskip  

  \item[--] $\obj{e} = \obj{e_1\,e_2}$, so 
  $\interp{e} = \interp{e_1\,e_2} = \lf{{\sf app}\,E_1\,E_2}$
 
  We have to show several things. The first, that $(\widehat{\sf C},
  \widehat{\rho}) \models \obj{e_1}$, follows from the coinduction
  hypothesis -- by rule ${\sf ev/app}$, ${\sf eval}\,\interp{e_1} \in
  \Delta$. That rule also allows us to conclude that ${\sf
    cont}\,\lf{{\sf app1}\,\interp{e_2}}\,\interp{e_1}\,\interp{e_1\,e_2} \in
  \Delta$.

  \medskip Second, given a $\obj{(\lambda x.e_0}) \in \widehat{\sf
    C}(\obj{e_1})$ (meaning ${\sf retn}\,\interp{\lambda
    x.e_0}\,\interp{e_1} \in \Delta$) we have to show that
  $(\widehat{\sf C}, \widehat{\rho}) \models \obj{e_2}$. This follows
  from the coinduction hypothesis: by rule ${\sf ev/app1}$, because
  ${\sf retn}\,\interp{\lambda x.e_0}\,\interp{e_1} \in \Delta$ and
  ${\sf cont}\,\lf{({\sf
      app1}\,\interp{e_2})}\,\interp{e_1}\,\interp{e_1\,e_2} \in \Delta$,
  ${\sf eval}\,\interp{e_2} \in \Delta$. This same reasoning allows us
  to conclude that ${\sf cont}\,\lf{({\sf app2}\,(\lf \lambda
    x.\,\interp{e_0}))}\,\interp{e_2}\,\interp{e_1\,e_2} \in \Delta$ given
  that $\obj{\lambda x.e_0}) \in \widehat{\sf
    C}(\obj{e_1})$. 

  \medskip Third, given a $\obj{(\lambda x.e_0}) \in \widehat{\sf
    C}(\obj{e_1})$, we have to show that $(\widehat{\sf C}(\obj{e_2})
  \subseteq \widehat{\rho}(\obj{x}))$: in other words, that ${\sf
    retn}\,\interp{v_2}\,\interp{e_2} \in \Delta$ implies ${\sf
    bind}\,\lf{({\sf var}(\lambda x.\,\interp{e_0}))}\,\interp{v_2}
  \in \Delta$.  Because we know by the reasoning above that ${\sf
    cont}\,\lf{({\sf app2}\,(\lf \lambda
    x.\,\interp{e_0}))}\,\interp{e_2}\,\interp{e_1\,e_2} \in \Delta$, this
  follows by rule ${\sf ev/app2}$. 

  \medskip The same reasoning allows us to conclude that $\obj{(\lambda
    x.e_0}) \in \widehat{\sf C}(\obj{e_1})$ and ${\sf
    retn}\,\interp{v_2}\,\interp{e_2} \in \Delta$ together imply ${\sf
    eval}\,\interp{e_0} \in \Delta$ (and therefore that $(\widehat{\sf
    C}, \widehat{\rho}) \models \obj{e_0}$ by the coinduction
  hypothesis, the fourth thing we needed to prove) and that ${\sf
    cont}\,\lf{\sf app3}\,\interp{e_0}\,\interp{e_1\,e_2} \in \Delta$ (which
  with ${\sf ev/app3}$ implies $\widehat{\sf C}(\obj{e_0}) \subseteq
  \widehat{\sf C}(\obj{e_1\,e_2})$, the last thing we needed to
  prove).
  \end{itemize}

\noindent
This completes the proof.
\end{proof}

We claim that, if we had started with an analysis that incorporated
both parallel evaluation of functions and arguments (in the style of
Figure~\ref{fig:dest-pair} from Section~\ref{sec:modular-parallelism})
and the call-by-future functions discussed in
Figure~\ref{fig:dest-futures} from Section~\ref{sec:dest-futures},
then the derived analysis would have satisfied a faithful
representation of Nielson et al.'s acceptability relation. The proof,
in this case, should the same lines as the proof of
Theorem~\ref{thm:acceptablecfa}.

%  which is given a coinductive
% definition in Figure~\ref{fig:acceptablecontrolflowanalysis}, holds.


% \smallskip This is the right interpretation, but the analysis in
% Figure~\ref{fig:dest-cfa-2} does not quite line up correctly with the
% definition in Figure~\ref{fig:acceptablecontrolflowanalysis}. It is
% illustrative see where the proof that ${\sf eval}\,\interp{e} \in
% \Delta$ implies $(\widehat{\sf C},\widehat{\rho}) \models \obj{e}$
% fails.

% The first two cases go correctly. If we have ${\sf eval}\,\interp{x}
% \in \Delta$, then rule ${\sf ev/bind}$ ensures that, if ${\sf
%   bind}\,\interp{x}\,\interp{v} \in \Delta$, then ${\sf
%   retn}\,\interp{v}\,\interp{x} \in \Delta$, which implies that
% $\widehat{\rho}(\obj{x}) \subseteq \widehat{\sf C}(\obj{x})$ as
% required.  Similarly, if we have ${\sf eval}\,\interp{\lambda x.e} \in
% \Delta$, then rule ${\sf ev/lam}$ means that ${\sf
%   retn}\,\interp{\lambda x.e}\,\interp{\lambda x.e}$, which is to say
% that $\{\obj{\lambda x.e} \} \subseteq \widehat{\sf C}(\obj{\lambda
%   x.e})$ as required. The third case is where trouble arises: if ${\sf
%   eval}\,\interp{e_1\,e_2} \in \Delta$, then by rule ${\sf ev/app}$ it
% must also be the case that ${\sf eval}\,\interp{e_1} \in \Delta$,
% allowing us to coinductively conclude that $(\widehat{\sf
%   C},\widehat{\rho}) \models \obj{e_1}$ as required, but if no values
% are returned from $\obj{e_1}$ (which can happen if, for example $\obj{e_1} =
% \obj{(\lambda x.\,x\,x)\,(\lambda x.\,x\,x)}$) then we will not
% necessarily have  ${\sf eval}\,\interp{e_1} \in \Delta$ by rule
% ${\sf ev/app2}$. This means that we cannot conclude that 
% $(\widehat{\sf
%   C},\widehat{\rho}) \models \obj{e_2}$ as required. 

% The methodology of logical approximation is not at fault here;
% instead, we can place blame either on the SSOS semantics in
% Figure~\ref{fig:dest-env} or on the specification in
% Figure~\ref{fig:acceptablecontrolflowanalysis}. To place blame on the
% specification, we can observe that the specific point of failure above
% is due to the definition of an acceptable analysis not taking
% left-to-right evaluation order into account; Exercise 3.4 in
% \cite{nielson05principles} is to make the definition of an acceptable
% analysis more precise so that ``there is no need to analyze the
% operand if the operator cannot produce any closures.'' Alternatively,
% we place blame on the form of the SSOS specification. If we had
% evaluated both arguments $\lf{E_1}$ and $\lf{E_2}$ in parallel in
% Figure~\ref{fig:dest-env} (in the style of Figure~\ref{fig:dest-pair}
% from Section~\ref{sec:modular-parallelism}), then the resulting
% analysis would have placed ${\sf eval}\,\interp{e_1}$ and ${\sf
%   eval}\,\interp{e_2}$ into the context simultaneously, addressing the
% point where our proof failed above. To really get the SSOS semantics
% that corresponds to Nielson et al.'s specification in
% Figure~\ref{fig:acceptablecontrolflowanalysis}, we would have needed
% to use a semantics that incorporated both parallel evaluation of
% functions and arguments and the call-by-future functions discussed
% in Figure~\ref{fig:dest-futures} from Section~\ref{sec:dest-futures}.

\section{Alias analysis}
\label{sec:aliasanalysis}

The control flow analysis above was derived from the SSOS
specification of a language that looked much like the Mini-ML-like
languages considered in
Chapters~\ref{chapter-absmachine}~and~\ref{chapter-destinations}, and
we described how to justify such an analysis in terms of coinductive
specifications of what comprises a well-designed control flow
analysis.

In this section, we work in the other direction: the starting point
for this specification was the interprocedural object-oriented alias
analysis presented as a saturating logic program in \cite[Chapter
12.4]{aho07compilers}. We then worked backwards to get a SSOS
semantics that allowed us to derive that logic program as closely as
possible; the result was a monadic SSOS semantics. There should
not, however, be any obstacle to deriving an alias analysis from a
semantics that looks more like the specifications elsewhere in this
thesis.

\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/ssos-monadic.sls}
\caption{Semantics of functions in the simple monadic language}
\label{fig:ssos-monadic}
\end{figure}

\subsection{Monadic language}

The language we consider differentiates atomic actions, which we will
call {\it expressions} (and encode in the LF type ${\sf exp}$) and
procedures or {\it commands} (which we encode in the LF type ${\sf
  cmd}$). There are only two commands $\obj{m}$ in our monadic language. The
first command, $\obj{{\sf ret}\,x}$, is a command that returns the value
bound to the variable $\obj{x}$ (rule ${\sf ev/ret}$ in
Figure~\ref{fig:ssos-monadic}). The second command, $\interp{{\sf
    bnd}^l\, x \leftarrow e \,{\sf in}\,m} = \lf{{\sf
  bnd}\,l\,\interp{e}\,\lambda x.\interp{m}}$, evaluates $\obj{e}$ to a
value, binds that value to the variable $\obj{x}$, and then evaluates
$\obj{m}$. Note the presence of $\obj{l}$ 
in the bind syntax; we will call it a
{\it label}, and we can think of it as a line number or source-code
position from the original program.

In the previous languages we have considered, values $\obj{v}$ were a
syntactic refinement of the expressions $\obj{e}$. In contrast, our monadic
language will differentiate the two: there are five expression forms
and three values that we will consider. An expression $\interp{\lambda
  x.e} = \lf{{\sf fun}\,\lambda x.\interp{e}}$ evaluates to a value
$\interp{\lambda^l x.e} = \lf{{\sf lam}\,l\,\lambda x.\interp{e}}$, where
the label $\obj{l}$ represents the source code position where the function
was bound. When we evaluate the command $\interp{{\sf
    bnd}^l \, y \leftarrow \lambda x.e \,{\sf in}\,m}$, the value
$\interp{\lambda^l x.e}$ gets bound to $\obj{y}$ in the body of the command
$\obj{m}$ (rule ${\sf ev/fun}$ in Figure~\ref{fig:ssos-monadic}).

The second expression form is a function call: $\interp{f\,x} = \lf{{\sf
  app}\,f\,x}$. To evaluate a function call, we expect a function value
(which is a command $\obj{m_0}$ with one free variable) to be bound to the
variable $\obj{f}$; we then store the rest of the current command on the
stack and evaluate the command $\obj{m_0}$ to a value. Note that the rule
${\sf ev/call}$ in Figure~\ref{fig:ssos-monadic} also stores the call
site's source-code location $\obj{l}$ on the stack frame. The reason for
storing a label here is that we need it for the alias
analysis. However, it is possible to independently motivate adding
these source-code positions to the operational semantics: for instance, it
would allow us to model the process of giving a stack trace when an
exception is raised. When the function we have called returns (rule
${\sf ev/call1}$ in Figure~\ref{fig:ssos-monadic}), we continue
evaluating the command that was stored on the control stack.

\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/ssos-monadic2.sls}
\caption{Semantics of mutable pairs in the simple monadic language}
\label{fig:ssos-monadic2}
\end{figure}

The rules for mutable pairs are given in
Figure~\ref{fig:ssos-monadic2}. Evaluating the expression $\obj{\sf
  newpair}$ allocates a tuple with two fields $\obj{\sf fst}$ and 
$\obj{\sf
  snd}$ and yields a value $\obj{{\sf loc}\,l}$ referring to the tuple; both
fields in the tuple are initialized to the value $\obj{\sf null}$, and
each field is represented by a separate linear ${\sf cell}$ resource
(rule ${\sf ev/new}$). The expressions $\interp{x.{\sf fst}} = \lf{{\sf
  proj}\,x\,{\sf fst}}$ and $\interp{x.{\sf snd}} = \lf{{\sf proj}\,x\,{\sf
  snd}}$ expect a pair location to be bound to $\obj{x}$, and yield the value
stored in the appropriate field of the mutable pair (rule ${\sf
  ev/proj}$). The expressions $\interp{x.{\sf fst} := y} = \lf{{\sf
  set}\,x\,{\sf fst}\,y}$ and $\interp{x.{\sf snd} := y} = \lf{{\sf
  set}\,x\,{\sf snd}\,y}$ work much the same way. The difference is
that the former expressions do not change the accessed field's
contents, whereas the latter expressions replace the accessed field's
contents with the value bound to $\obj{y}$ (rule ${\sf ev/set}$).

This language specification bears some similarity to Harper's
Modernized Algol with free assignables \cite[Chapter
36]{harper12practical}. The {\it free assignables} addendum is
critical: SSOS specifications do not have a mechanism for enforcing
the stack discipline of Algol-like languages.\footnote{It is, however,
  possible to represent Algol-like languages that maintain a stack
  discipline even though the machinery of \sls~does not enforce that
  stack discipline. This is analogous to situation with pointer
  equality discussed in Section~\ref{sec:mutable-storage}, as a stack
  discipline is an invariant that can be maintained in \sls~even
  though the framework's proof theory does not enforce the invariant.}
The other major difference is that Harper's version of Algol does not
allow expressions to access the state.  For our language to behave
similarly, the expressions $\obj{\sf newpair}$, $\obj{x.{\sf fst}}$,
$\obj{x.{\sf snd}}$, $\obj{x.{\sf fst} := y}$, and $\obj{x.{\sf snd}
  := y}$ would need to be commands and not expressions; we make them
expressions only for the convenience of a more regular presentation.

\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/ssos-monadic-approx.sls}
\caption{Alias analysis for the simple monadic language}
\label{fig:ssos-monadic-approx}
\end{figure}



\subsection{Approximation and alias analysis}


To approximate the semantics of our monadic language, we can follow the
methodology from before and turn the specification persistent. A further
approximation is to remove the last premise from ${\sf ev/set}$, as
the meta-approximation theorem allows -- the only purpose of this
premise in Figure~\ref{fig:ssos-monadic2} was to consume the ephemeral
proposition ${\sf cell}\,\lf{l'}\,\lf{\it fld}\,\lf{v}$, and this is unnecessary
if ${\sf cell}$ is not an ephemeral predicate.  Having
made these two moves (turning all propositions persistent, and removing
a premise from ${\sf ev/set}$), we are left with three types of
existentially-generated variables that must be equated with concrete
terms in order for our semantics to be interpreted as a saturating
logic program:

\smallskip
\begin{itemize}
\item Variables $\lf{y}$, introduced by every rule except for ${\sf ev/ret}$,
\item Mutable locations $\lf{l}$, introduced by rule ${\sf ev/new}$, and 
\item Destinations $\lf{d}$; the only place where a destination is created
by the destination-adding transformation is in rule ${\sf ev/call}$.
\end{itemize}
\smallskip

Variables $\lf{y}$ are generated to be substituted into the body of some
command, so we could equate them with the Skolemized function body as
we did when deriving a control flow analysis example. Another option
comes from noting that, for any initial source program, every command
is associated with a particular source code location, so a simpler
alternative is just to equate the variable with that source code
location. This is why we stored labels on the stack: if we had not
done so, then the label $\obj{l}$ associated with $\obj{m}$ in the command
$\interp{{\sf bnd}^l\, x \leftarrow \lambda x.m_0 \,{\sf in}\,m}$ would
no longer be available when we needed it in rule ${\sf ev/call1}$.

We deal with mutable locations $\lf{l}$ in a similar manner: we equate them
with the label $\lf{l}$ representing the line where that cell was
generated.

There are multiple ways to deal with the destination $\lf{d_0}$ generated
in rule ${\sf ev/call}$. We want our analysis, like Aho et al.'s, to
be insensitive to control flow, so we will equate $\lf{d_0}$ with the label
$\lf{l_0}$ associated with the function we are calling.  If we instead
equated $\lf{d_0}$ with the label $\lf{l}$ associated with the call-site or with
the pair of the call site and the called function, the result would be
an analysis that is more sensitive to control flow.

The choices described above are reflected in
Figure~\ref{fig:ssos-monadic-approx}, which takes the additional step
of inlining uses of equality in the conclusions of rules. We can
invoke this specification as a program analysis by packaging a program
as a single command $\obj{m}$ and deriving a saturated process state from the
initial process state $(\lf{l_{\it init}}{:}{\sf loc}; x{:}\susp{{\sf
    eval}\,\interp{m}\,\lf{l_{\it init}}})$. 
The use of source-code position labels
makes the answers to some of the primary questions asked of an alias
analysis quite concise. For instance:

\smallskip
\begin{itemize}
\item {\it Might the first component of a pair created at label
    $\obj{l_1}$ ever reference a pair created at label $\obj{l_2}$?}
  Only if ${\sf cell}\,\lf{l_1}\,\lf{\sf fst}\,\lf{({\sf loc}\,l_2)}$
  appears in the saturated process state (and likewise for the second
  component).
\item {\it Might the first component of a pair created at label
    $\obj{l_1}$ ever reference the same object as the first component
    of a pair created at label $\obj{l_2}$?} Only if there is some
  $\lf{l'}$ such that ${\sf cell}\,\lf{l_1}\,\lf{\sf fst}\,\lf{({\sf
      loc}\,l')}$ and ${\sf cell}\,\lf{l_1}\,\lf{\sf fst}\,\lf{({\sf
      loc}\,l')}$ both appear in the saturated process state.
\end{itemize}

\section{Related work}
\label{sec:approximately-related}

The technical aspects of linear logical approximation are similar to
work done by Bozzano et al.~\cite{bozzano02effective,bozzano04model},
which was also based on the abstract interpretation of a logical
specification in linear logic.
They encode distributed systems and communication protocols in
a framework that is similar to the linear fragment of \sls~without
equality. Abstractions of those programs are then used to verify
properties of concurrent protocols that were encoded in the logic
\cite{bozzano02protocol}. 

There are a number of significant difference between our work and
Bozzano et al.'s, however. The style they use to encode protocols is
significantly different from any of the SSOS specification styles
presented in this thesis. They used a general purpose approximation,
which could therefore potentially be mechanized in the same way we
mechanized transformations like operationalization; in contrast, the
meta-approximation result described here captures a whole class of
approximations. Furthermore, Bozzano et al.'s methods are designed to
consider properties of a system as a whole, not static analyses of
individual inputs as is the case in our work.

Work by Might and Van Horn on abstracting abstract machines can be
seen as a parallel approach to our methodology in a very different
setting
\cite{might10resolving,might10abstract,might10abstracting}. Their
emphasis is on deriving a program approximation by approximating a
{\it functional} abstract interpreter for a programming language's
operational semantics. Their methodology is similar to ours in large
part because we are doing the same thing in a different setting,
deriving a program approximation by approximating a
destination-passing SSOS specification (which we could, in turn, have
derived from an ordered abstract machine by destination-adding).

Many of the steps that they suggest for approximating programs have
close analogues in our setting. For instance, their {\it
  store-allocated bindings} are analogous to the SSOS environment
semantics, and their {\it store-allocated continuations} -- which they
motivate by analogy to implementation techniques for functional
languages like SML/NJ -- are precisely the destinations that arise
naturally from the destination-adding transformation. The first
approximation step we take is forgetting about linearity in order to
obtain a (non-terminating) persistent logical specification. This step
is comparable to Might's first approximation step of ``throwing hats
on everything'' (named after the convention in abstract interpretation
of denoting the abstract version of a state space $\Sigma$ as
$\hat{\Sigma}$. The ``mysterious'' introduction of power domains that
this entails is, in our setting, a perfectly natural result of
relaxing the requirement that there be at most one persistent
proposition ${\sf bind}\,\lf{x}\,\lf{v}$ for every $\lf{x}$. 
As a final point of
comparison, the ``abstract allocation strategy'' discussed in
\cite{might10abstracting} is quite similar to our strategy of
introducing and then approximating Skolem functions as a means of
deriving a finite approximation. Our current discussion of Skolem
functions in Section~\ref{sec:0cfa} is partially inspired by the
relationship between our use of Skolemization and the discussion of
abstract allocation in \cite{might10abstracting}.

The independent discovery of a similar set of techniques for achieving
similar goals in such different settings (though both approaches were
to some degree inspired by Van Horn and Mairson's investigations of
the complexity of $k$-CFA \cite{vanhorn07relating}) is another
indication of the generality of both techniques, and the similarity
also suggests that the wide variety of approximations considered in
\cite{might10abstracting}, as well as the approximations of
object-oriented programming languages in \cite{might10abstract}, can
be adapted to this setting.
