\chapter{Introduction}
\label{chapter-introduction}

% The topic of this thesis is the specification of {\it evolving systems}

% The {\it lingua franca} of research in programming languages and
% logics is the {\it inductive definition}. Type systems are defined in
% terms of inductive definitions like $\Gamma \vdash e : \tau$
% (within the typing context $\Gamma$, the term $e$ has type $\tau$).
% % \[
% % \infer
% % {\Gamma \vdash x : \tau \mathstrut}
% % {x{:}\tau \in \Gamma \mathstrut}
% % \quad
% % \infer
% % {\Gamma \vdash {\sf z} : {\sf nat} \mathstrut}
% % {}
% % \quad
% % \infer
% % {\Gamma \vdash {\sf s}\,e : {\sf nat} \mathstrut}
% % {\Gamma \vdash e_1 : {\sf nat}
% %  &
% %  \Gamma \vdash e_2 : {\sf nat}}
% % \]
% Operational semantics are also defined in terms of inductive
% definitions: a {\it small-step} semantics has the form $e \mapsto e'$
% (the expression or machine state $e$ can transition to $e'$), and a
% {\it big-step} semantics has the form $e \Downarrow v$ (the expression
% or machine state $e$ can ultimately produce the terminal machine state
% $v$). 

% Proof assistants -- computer programs that help programming language
% researchers specify systems, explore their behavior, and prove
% properties of their behavior -- obviously must therefore be able to
% talk about inductive definitions. The most common way proof assistants
% do this is by directly incorporating a notion of inductive definition
% into their framework. Coq \cite{}, Agda \cite{}, ATS \cite{},
% Isabelle/HOL \cite{}, Matita \cite{}, and Abella \cite{} all work this
% way; inductive types are introduced

%  which covers all the theorem provers used to prove the POPLMark
% challenge except for Twelf.

% used in these domains therefore universally
% include a notion of {\it inductive definition} as the primary form of

Suppose you find yourself in possession of
\smallskip
\begin{itemize}
\item a calculator of unfamiliar design, or 
\item a new board game, or
\item the control system for an army of robots, or
\item an implementation of a security protocol, or
\item the interface to a high-frequency trading system.
\end{itemize}
\smallskip The fundamental questions are the same: {\it What does it
  do? What are the rules of the game?} The answer to this question,
whether it comes in the form of an instruction manual, a legal
document, or an ISO standard, is a {\it specification}.

Specifications must be {\it formal}, because any room for
misinterpretation could (respectively) lead to incorrect calculations,
accusations of cheating, a robot uprising, a security breach, or
bankruptcy. At the same time, specifications must be {\it clear}: in
practice, a hopelessly confusing or complex specification is no more
useful than one that is hopelessly vague. Clarity is what allows us to
communicate with each other, to use specifications to gain a common
understanding of what some system does and to think about how that
system might be changed. Formality is what allows specifications to
interact with the world of computers, to say with confidence that the
{\it implementation} of the calculator or high-frequency trading
system obeys the specification. Formality also allows specifications
to interact with the world of mathematics, and this, in turn, enables
us to make precise and accurate statements about what may or may not
happen to a given system.

The specification of many (too many!)~critical systems still remains
in the realm of English text, and the inevitable lack of formality can
and does make formal reasoning about these specifications difficult.
Notably, this is true about most of the programming languages used to
implement our calculators, program our robot army control systems,
enforce our security protocols, and interact with our high-frequency
trading systems. In recent years, however, we have finally begun to
seen the emergence of operational semantics specifications (the
``rules of the game'' for a programming language) that are truly
formal. A notable aspect of this recent work is that the formalization
effort is not done simply for formalization's sake. Ellison and Ro{\c
  s}u's formal semantics of C can be used to check individual programs
for undefined behavior, unsafe situations where the rules of the game
no longer apply and the compiler is free to do anything, including
unleashing the robot army \cite{ellison12executable}. Lee, Crary, and
Harper's formalization of Standard ML has been used to formally prove
-- using a computer to check all the proof's formal details -- a much
stronger safety property: that {\it every} program accepted by the
compiler is free of undefined behavior \cite{lee07towards}.

Mathematics, by contrast, has a century-long tradition of insisting on
absolute formality, at least in principle. Over time, this tradition
has become a collaboration between practicing mathematicians and
practicing computer scientists, because while humans are reasonable
judges of clarity, computers have an absolutely superhuman patience
for checking all the formal details of an argument.  One aspect of
this collaboration has been the development of {\it logical
  frameworks}. In a logical framework, the language of specifications
is derived from the language of logic, whigh gives specifications in a
logical framework an independent meaning based on the logic from which
the logical framework was derived. To be clear, the language of logic
is not a single, unified entity: logics are formal systems that
satisfy certain internal coherence properties, and we study many of
them. For example, the logical framework Coq is based on the Calculus
of Inductive Constructions \cite{coq10coq}, the logical framework Agda
is based on a variant of Martin-L\"of's type theory called ${\sf
  UTT}_\Sigma$ \cite{norell08towards}, and the logical Twelf is based
off of the dependent type theory $\lambda^\Pi$, also known as LF
\cite{pfenning99system}. Twelf was the basis of Lee, Crary, and
Harper's formalization of Standard ML.  % Specifications evolve
% gradually from half-baked ideas scrawled on coffee-stained napkins to
% formal specifications encoded in a logical framework. Another critical
% component of a logical framework is a methodology or philosophy that
% guides this process. LF and Twelf, in particular, have a formal theory
% of {\it adequacy} that addresses the relationship between the on-paper
% artifacts the people use to communicate with each other and the
% encoding of those artifacts in LF \cite{harper93framework}.

Why is there not a larger tradition of formally specifying the
programming languages that people actually use? Part of the answer is
that most languages that people actually use have lots of features --
like mutable state, or exception handling, or synchronization and
communication, or lazy evaluation -- that are not particularly
pleasant to specify using existing logical frameworks. Dealing with a
few unpleasant features at a time might not be much trouble, but the
combinations that appear in actual programming languages cause formal
programming language specifications to be both unclear for humans to
read and inconvinenet for formal tools to manipulate. A more precise
statement is that the addition of the aforementioned features is {\it
  non-modular}, because handling a new feature requires reconsidering
and revising the rest of the specification.  Some headway on this
problem has been made by frameworks like the K semantic framework that
are formal but not logically derived; the K semantic framework is
based on a notion of rewriting rules \cite{rosu10overview}. Ellison
and Ro\c{s}u's formalization of C was done in the K semantic
framework.

This thesis considers the specification of systems, particularly
programming languages, in logical frameworks. In particular, we
consider a particular family of logics, called {\it substructural
  logics}, in which logical propositions can be given an
interpretation as rewriting rules as detailed by Cervesato and Scedrov
\cite{cervesato09relating}. % Deriving a framework from substructural
% logics allows us to combine the formality and generality of logical
% frameworks with the modular specification that is possible in
% rewriting frameworks. With this synthesis, 
We seek to support the
following:
\begin{quote} {\bf Thesis Statement:} {\it Logical frameworks based on
    a rewriting interpretation of substructural logics are suitable
    for modular specification of programming languages and formal
    reasoning about their properties}.\footnote{The original thesis
    proposal used the phrase ``forward reasoning in substructural
    logics'' instead of the phrase ``a rewriting interpretation of
    substructural logics,'' but these are synonymous, as discussed in
    Section~\ref{sec:framework-logicprog}.}
\end{quote}

\noindent
Part~1 of the thesis concerns the design of logical frameworks that
support this rewriting interpretation and the design of the logical
framework \sls~in particular. Part~2 concerns the modular
specification of programming language features in \sls~and the
methodology by which we organize and relate styles of
specification. Part~3 discusses formal reasoning about properties of
\sls~specifications, with an emphasis on establishing invariants.

\section{Logical frameworks}

Many interesting stateful systems have a natural notion of {\it
  ordering} that is fundamental to their behavior. A very simple
example is a push-down automaton (PDA) that reads a string of symbols
left-to-right while maintaining and manipulating a separate stack of
symbols. We can represent any configuration of the PDA as a sequence
with three regions:
\[
[~\mbox{the stack}~]
~
[~\mbox{the head}~]
~
[~\mbox{the string being read}~]
\]
where the symbols closest to the head are the top of the stack and the
symbol waiting to be read from the string. If we represent the head as
a token ${\sf hd}$, we can describe the behavior (the rules of the
game) for a push-down automaton for checking a string for correct
nesting of angle braces by using two rewriting rules:
\begin{align}
\tag{push} {\sf hd}~{<} ~&\rightsquigarrow~ {<}~{\sf hd}
\\
\tag{pop} {<}~{\sf hd}~{>} ~&\rightsquigarrow~ {\sf hd}
\end{align}
The distinguishing feature of these rewriting rules is that they are
{\it local} -- they do not mention the entire stack or the entire
string, just the relevant fragment at the beginning of the string and
the top of the stack. Execution of the PDA on a particular string of
tokens then consists of (1) appending the token ${\sf hd}$ to the
beginning of the string, (2) repeatedly performing rewritings until no
more rewrites are possible, and (3) checking to see if only a single
token ${\sf hd}$ remains. One possible series of transitions that this
rewriting system can take is shown in Figure~\ref{fig:pda-transitions}

\begin{figure}
\begin{align*}
{\sf hd}~~{<}~~{<}~~{>}~~{<}~~{<}~~{>}~~{>}~~{>}
& ~~~\rightsquigarrow~~\\
{<}~~{\sf hd}~~{<}~~{>}~~{<}~~{<}~~{>}~~{>}~~{>}
& ~~~\rightsquigarrow~~\\
{<}~~{<}~~{\sf hd}~~{>}~~{<}~~{<}~~{>}~~{>}~~{>}
& ~~~\rightsquigarrow~~\\
{<}~~{\sf hd}~~{<}~~{<}~~{>}~~{>}~~{>}
& ~~~\rightsquigarrow~~\\
{<}~~{<}~~{\sf hd}~~{<}~~{>}~~{>}~~{>}
& ~~~\rightsquigarrow~~\\
{<}~~{<}~~{<}~~{\sf hd}~~{>}~~{>}~~{>}
& ~~~\rightsquigarrow~~\\
{<}~~{<}~~{\sf hd}~~{>}~~{>}
& ~~~\rightsquigarrow~~\\
{<}~~{\sf hd}~~{>}
& ~~~\rightsquigarrow~~\\
{\sf hd} &
\end{align*}
\caption{Series of PDA transitions.}
\label{fig:pda-transitions}
\end{figure}

Because our goal is to use a framework that is both simple and
logically motivated, we turn to a substructural logic called {\it
  ordered logic} (originally presented by
Lambek~\cite{lambek58mathematics}) where hypotheses have an
intrinsic notion of order. The rewriting rules we considered above can
be explained as propositions in ordered logic, where the tokens ${\sf
  hd}$, $>$, and $<$ are all treated as {\it atomic propositions}:
\begin{align*}
{\sf push} &: ~~ {\sf hd} \fuse {<} ~\lefti~ \{ {<} \fuse {\sf hd} \}
\\ 
{\sf pop} &: ~~ {<} \fuse {\sf hd} \fuse {>} ~\lefti~ \{ {\sf hd} \}
\end{align*}
The symbol $\fuse$ (pronounced ``fuse'') is the binary connective for
ordered conjunction (i.e. concatenation); it binds more tightly than
$\lefti$, a binary connective for ordered implication. The curly
braces $\{ \ldots \}$ can be ignored for now.

Our logic has first-order quantification, so we can generically
describe a more general push-down automaton that uses ${\sf left}(X)$
and ${\sf right}(X)$ to describe left and right angle braces ($X =
{\sf an}$), square braces ($X = {\sf sq}$), and parentheses ($X = {\sf
  pa}$). The string {\sf [ \textless~\textgreater~( [ ] ) ]} is then
represented by the following sequence of ordered atomic propositions:
\[
{\sf 
  left(sq) ~~
  left(an) ~~
  right(an) ~~
  left(pa) ~~
  left(sq) ~~
  right(sq) ~~
  right(pa) ~~
  right(sq)
}
\]
The following rules describe the more general push-down automaton:
\begin{align*}
{\sf push} &: ~~ \forall x.\, 
  {\sf hd} \fuse {\sf left}(x) ~\lefti~ \{ {\sf stack}(x) \fuse {\sf hd} \}
\\ 
{\sf pop} &: ~~ \forall x.\, 
  {\sf stack}(x) \fuse {\sf hd} \fuse {\sf right}(x) ~\lefti~ \{ {\sf hd} \}
\end{align*}
(This specification would still be possible in propositional ordered 
logic; we would just need one copy of the ${\sf push}$ rule and one copy
of the ${\sf pop}$ rule for each pair of braces.)
Note that while we use the fuse connective to indicate adjacent tokens
in the rules above, no fuses appear in
Figure~\ref{fig:pda-transitions}. That is because the intermediate
states are not propositions in the same way rules are
propositions. Rather, the intermediate states in
Figure~\ref{fig:pda-transitions} are {\it contexts} in ordered logic,
which we will refer to as {\it process states}. 

The most distinctive characteristic of these transition systems is
that the intermediate stages of computation are encoded in the
structure of a substructural context (a process state). This general
idea dates back to Miller \cite{miller92pi} and his Ph.D. student
Chirimar \cite{chirimar95proof}, who encoded the intermediate states
of a $\pi$-calculus and of a low-level RISC machine (respectively) as
contexts in focused classical linear logic.  Part~1 of this thesis is
concerned with the design of logical frameworks for specifying
transition systems.  In this respect, this part of the thesis follows
in the footsteps of Miller's Forum \cite{miller96forum}, Cervesato and
Scedrov's multiset rewriting language $\omega$
\cite{cervesato09relating}, and Watkins et al.'s CLF
\cite{watkins02concurrent}. The major contribution of this part of the
thesis is the development of {\it structural focalization}
\cite{simmons11structural}, which unifies Andreoli's work on focused
logics \cite{andreoli92logic} with the {\it hereditary substitution}
technique that Watkins developed in the context of CLF
\cite{watkins02concurrent}. Chapter~2 explains structural focalization
in the context of linear logic, Chapter~3 establishes focalization for
a richer substructural logic \ollll, and Chapter~4 takes focused
\ollll~and carves out \sls, a CLF-like framework of \underline{\bf
  s}ubstructural \underline{\bf l}ogical \underline{\bf
  s}pecifications.

\section{Substructural operational semantics}
\label{sec:intro-ssos}

We are not primarily interested in representing systems like PDAs;
rather, our goal is to specify the operational semantics of
programming languages. We can represent operational semantics in
\sls~in many ways, but we are particularly interested in a broad
specification style called {\it substructural operational semantics},
or SSOS
\cite{pfenning04substructural,pfenning09substructural}.\footnote{The
  term {\it substructural operational semantics} merges structural
  operational semantics \cite{plotkin04structural}, which we seek to
  generalize, and substructural logic, which forms the basis of our
  specification framework.} SSOS is a synthesis of structural
operational semantics, abstract machines, and logical specifications.

One of our running
examples will be a call-by-value operational semantics for the untyped
lambda calculus, defined by the BNF grammar:
\[
e ::= x \mid \lambda x.e \mid e_1\,e_2
\]
Taking some liberties with our representation of terms, we can
describe call-by-value evaluation for this language with the same
rewriting rules we used to describe the PDA. Instead of a head ${\sf
  hd}$, our specification uses two atomic propositions: ${\sf
  eval}(e)$ for an unevaluated expression $e$, and ${\sf retn}(v)$
for an evaluated value $v$.


The evaluation of a function is simple, as a function is already a
fully evaluated value, so we replace ${\sf eval}(\lambda x.e)$
in-place with ${\sf retn}(\lambda x.e)$:
\begin{align*}
{\sf ev/lam}&: ~~ 
  {\sf eval}\,(\lambda x.e) \lefti \{ {\sf retn}\,(\lambda x.e) \}
%
  \intertext{The evaluation of an application $e_1\,e_2$, on the other
    hand, requires us to push a new element onto the stack. In place
    of the PDA's stack of propositions ${\sf stack}(x)$ growing off to
    the left, we maintain a stack of ${\sf cont}(f)$ propositions
    growing off to the right; $f$ is called a {\it frame}. We evaluate
    $e_1\,e_2$ by evaluating $e_1$ and leaving behind a frame
    $\Box\,e_2$ that suspends the argument $e_2$ while $e_1$ is being
    evaluated to a value.}
%
{\sf ev/app}&: ~~ 
  {\sf eval}\,(e_1\,e_2) \lefti \{ {\sf eval}\,(e_1) 
     \fuse {\sf cont}\,(\Box\,e_2) \}
%
     \intertext{When a function is returned to a waiting $(\Box\,e_2)$
       frame, we switch to evaluating the function argument while
       storing the returned function in a frame $((\lambda
       x.e)\,\Box)$.}
%
{\sf ev/app1}&: ~~
  {\sf retn}\,(\lambda x.e) \fuse {\sf cont}\,(\Box\,e_2)
    \lefti \{ {\sf eval}\,(e_2) \fuse {\sf cont}\,((\lambda x.e)\,\Box) \}
%
    \intertext{Finally, when an evaluated function argument is
      returned to the waiting $((\lambda x.e)\,\Box)$ frame, we
      substitute the value into the body of the function and evaluate
      the result.}
%
{\sf ev/app2}&: ~~
  {\sf retn}\,(v_2) \fuse {\sf cont}\,((\lambda x.e)\,\Box)
    \lefti \{ {\sf eval}\,([v_2/x]e) \}
%
\end{align*}

\begin{figure}
\begin{align*}
{\sf eval}\,((\lambda x.x)\,((\lambda y.y)\,(\lambda z.e))) 
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app}$}\\
{\sf eval}\,(\lambda x.x) \quad
{\sf cont}\,(\Box\,((\lambda y.y)\,(\lambda z.e)))
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/lam}$}\\
{\sf retn}\,(\lambda x.x) \quad
{\sf cont}\,(\Box\,((\lambda y.y)\,(\lambda z.e)))
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app1}$}\\
{\sf eval}\,((\lambda y.y)\,(\lambda z.e)) \quad
{\sf cont}\,((\lambda x.x)\,\Box)
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app}$}\\
{\sf eval}\,(\lambda y.y) \quad
{\sf cont}\,(\Box\,(\lambda z.e)) \quad
{\sf cont}\,((\lambda x.x)\,\Box)
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/lam}$}\\
{\sf retn}\,(\lambda y.y) \quad
{\sf cont}\,(\Box\,(\lambda z.e)) \quad
{\sf cont}\,((\lambda x.x)\,\Box)
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app1}$}\\
{\sf eval}\,(\lambda z.e) \quad
{\sf cont}\,((\lambda y.y)\,\Box) \quad
{\sf cont}\,((\lambda x.x)\,\Box)
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/lam}$}\\
{\sf retn}\,(\lambda z.e) \quad
{\sf cont}\,((\lambda y.y)\,\Box) \quad
{\sf cont}\,((\lambda x.x)\,\Box)
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app2}$}\\
{\sf eval}\,(\lambda z.e) \quad
{\sf cont}\,((\lambda x.x)\,\Box)
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/lam}$}\\
{\sf retn}\,(\lambda z.e) \quad
{\sf cont}\,((\lambda x.x)\,\Box)
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app2}$}\\
{\sf eval}\,(\lambda z.e) 
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/lam}$}\\
{\sf retn}\,(\lambda z.e) 
& ~~~\not\rightsquigarrow~~ 
\end{align*}
\caption{SSOS evaluation of an expression to a value.}
\label{fig:ssos-example}
\end{figure}

These four rules constitute an SSOS specification of call-by-value
evaluation; an example of evaluating the expression $(\lambda
x.x)\,((\lambda y.y)\,(\lambda z.e))$ to a value under this
specification is given in Figure~\ref{fig:ssos-example}.  Again, each
intermediate state is represented by a process state or ordered
context.

The \sls~framework admits many styles of specification. The SSOS
specification above resides in the {\it concurrent} fragment of
\sls. (This rewriting-like fragment is called concurrent in part
because we can just as easily seed the process state with two
propositions ${\sf eval}(e)$ and ${\sf eval}(e')$ that will evaluate
to values concurrently, side-by-side in the process state.)  Even
within purely-concurrent specifications, there is a large design space
of potential SSOS specifications, a point that will be explained
further in Chapter~5.

On the other end of the spectrum, the {\it deductive} fragment of
\sls~supports the specification of inductive definitions by the same
methodology used to represent inductive definitions in the LF logical
framework \cite{harper93framework}.  We can therefore use the
deductive fragment of \sls~to specify a big-step operational semantics
for call-by-value evaluation by inductively defining the judgment $e
\Downarrow v$, which expresses that the expression $e$ evaluates to
the value $v$. On paper, this big-step operational semantics is
expressed with two inference rules:
\[
\infer
{\lambda x.e \Downarrow \lambda x.e \mathstrut}
{}
\quad
\infer
{e_1\,e_2 \Downarrow v \mathstrut}
{e_1 \Downarrow \lambda x.e
 &
 e_2 \Downarrow v_2
 &
 [v_2/x]e_2 \Downarrow v \mathstrut}
\]
Big-step operational semantics specifications are compact and elegant,
but they is not particularly {\it modular}. As a (rather contrived)
example, consider the addition of a incrementing counter ${\sf count}$
to the language of expressions $e$. The language keeps a numeral as a
counter, and every time ${\sf count}$ is evaluated, it returns the
value of the counter and then increments the counter.\footnote{To keep
  the language small, we can represent numerals $\underline{n}$ as
  Church numerals: $\underline{0} = (\lambda f. \lambda x. x)$,
  $\underline{1} = (\lambda f. \lambda x. f x)$, $\underline{2} =
  (\lambda f. \lambda x. f (f x))$, and so on.  Then, $\underline{n} +
  1 = \lambda f. \lambda x. f e$ if $\underline{n} = \lambda
  f. \lambda x. e$.}  To extend the big-step operational semantics
with this new feature, we have to revise all the existing rules so
that they mention the running counter:
%
\[
\infer
{({\sf count}, \underline{n}) \Downarrow 
  (\underline{n}, \underline{n} +1)}
{}
\quad 
\infer
{(\lambda x.e, \underline{n}) \Downarrow (\lambda x.e, \underline{n})
 \mathstrut}
{}
\]
\[
\infer
{(e_1\,e_2, \underline{n}) \Downarrow (v, \underline{n'}) \mathstrut}
{(e_1, \underline{n}) \Downarrow (\lambda x.e, \underline n_1)
 &
 (e_2, \underline n_1) \Downarrow (v_2, \underline n_2)
 &
 ([v_2/x]e_2, \underline n_2 ) \Downarrow (v, \underline{n'}) \mathstrut}
\]
The simple elegance of our big-step operational semantics has been
tarnished by the need to deal with state, and each new stateful
feature requires a similar revision.  In contrast, our SSOS
specification can tolerate the addition of a counter without revision
to the existing rules; we just store the counter's value in an atomic
proposition ${\sf store}(\underline{n})$ to the left of the ${\sf
  eval}(e)$ or ${\sf retn}(v)$ proposition in the ordered
context. Because the rules ${\sf ev/lam}$, ${\sf ev/app}$, ${\sf
  ev/app1}$, and ${\sf ev/app2}$ are local, they will ignore this
extra proposition, which only needs to be accessed by the rule ${\sf
  ev/count}$.
\begin{align*}
{\sf ev/count} &:~~
  {\sf store}\,\underline n \fuse {\sf eval}\,{\sf count}
    \lefti \{ {\sf store}\,(\underline n + 1) 
      \fuse {\sf retn}\,\underline n \}
\end{align*}
In Figure~\ref{fig:eval-ssos-ctr}, we give an example of evaluating
$(((\lambda x.\lambda y.y)\,{\sf count})\,{\sf count})$ to a value
(with a starting counter value of $\underline 5$). This solution is
even more contrived than the problem was: if we wanted to introduce a
{\it second} counter, where would it go? Nevertheless, the example
does foreshadow how, in Part~2 of this thesis, we will show that SSOS
specifications in \sls~allow for the modular specification of many
programming language features.

\begin{figure}
\begin{align*}
{\sf store}\,\underline 5 \quad
{\sf eval}\,(((\lambda x.\lambda y.y)\,{\sf count})\,{\sf count})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app}$}\\
{\sf store}\,\underline 5 \quad
{\sf eval}\,((\lambda x.\lambda y.y)\,{\sf count}) \quad
{\sf cont}\,(\Box\,{\sf count})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app}$}\\
{\sf store}\,\underline 5 \quad
{\sf eval}\,(\lambda x.\lambda y.y) \quad
{\sf cont}\,(\Box\,{\sf count}) \quad
{\sf cont}\,(\Box\,{\sf count})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/lam}$}\\
{\sf store}\,\underline 5 \quad
{\sf retn}\,(\lambda x.\lambda y.y) \quad
{\sf cont}\,(\Box\,{\sf count}) \quad
{\sf cont}\,(\Box\,{\sf count})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app1}$}\\
{\sf store}\,\underline 5 \quad
{\sf eval}\,{\sf count} \quad
{\sf cont}\,((\lambda x.\lambda y.y)\,\Box) \quad
{\sf cont}\,(\Box\,{\sf count})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/count}$}\\
{\sf store}\,\underline 6 \quad
{\sf retn}\,\underline 5 \quad
{\sf cont}\,((\lambda x.\lambda y.y)\,\Box) \quad
{\sf cont}\,(\Box\,{\sf count})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app2}$}\\
{\sf store}\,\underline 6 \quad
{\sf eval}\,(\lambda y.y) \quad
{\sf cont}\,(\Box\,{\sf count})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/lam}$}\\
{\sf store}\,\underline 6 \quad
{\sf retn}\,(\lambda y.y) \quad
{\sf cont}\,(\Box\,{\sf count})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app2}$}\\
{\sf store}\,\underline 6 \quad
{\sf eval}\,{\sf count} \quad
{\sf cont}\,((\lambda y.y)\,\Box)
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/count}$}\\
{\sf store}\,\underline 7 \quad
{\sf retn}\,\underline 6 \quad
{\sf cont}\,((\lambda y.y)\,\Box)
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app2}$}\\
{\sf store}\,\underline 7 \quad
{\sf eval}\,\underline 6 
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/lam}$}\\
{\sf store}\,\underline 7 \quad
{\sf retn}\,\underline 6 
& ~~~\not\rightsquigarrow~~ 
\end{align*}
\caption{Evaluation with an imperative counter.}
\label{fig:eval-ssos-ctr}
\end{figure}

An overarching theme of Part~2 is that we can have our cake and eat it
too by deploying the {\it logical correspondence}, an idea that was
developed jointly with Ian Zerny in unpublished work and is explained
in Chapter~5. As explained in Chapter~6, we can use logical
correspondence to directly connect the big-step semantics and SSOS
specifications above; in fact, we can automatically and mechanically
derive the latter from the former. Therefore, we can specify
call-by-value evaluation as a big-step semantics, and then transform
it for the purpose of modular extension. Further transformations,
developed in joint work with Pfenning \cite{simmons11logical}, create
new opportunities for modular extension; this is the topic of
Chapter~7. These transformations also allow for the derivation of
abstract analyses (such as control flow and alias analysis) from SSOS
specifications, which is the focus of Chapter~8.

\section{Invariants in substructural logic}

(XXX Come back and write this part)

