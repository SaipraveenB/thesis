\chapter{Destination-passing}
\label{chapter-destinations}

The natural notion of ordering provided by ordered linear logic is
quite convenient for encoding evolving systems that perform local
manipulations to a stack-like structure.  The push-down
automaton for generic bracket matching discussed in the introduction
demonstrated this: we can now present that specification in
Figure~\ref{fig:pda-ord} as an \sls~specification.

\begin{figure}[ht]
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/pda-ord.sls}
\caption{Ordered \sls~specification of a PDA for parenthesis matching}
\label{fig:pda-ord}
\end{figure}

\noindent
Tree structures were reasonably straightforward to encode in the
ordered context as well, as we saw from the SSOS specification for
parallel pairs in Chapter~\ref{chapter-absmachine}. 

At some point, however, the simple data structures that can be
naturally encoded in an ordered context become too limiting. When we
reach this point, we turn to {\it destinations}. Destinations (terms
of type ${\sf dest}$) are a bit like the locations $\lf l$ introduced
in the specification of mutable storage in
Section~\ref{sec:mutable-storage}. They have no constructors: they are
only introduced as variables by existential quantification, which
means they can freely be subject to unification when the conclusion of
a rule declares them to be equal (as described in
Section~\ref{sec:slsframework}). Destinations allow us to encode very
expressive structures in the linear context of \sls. Instead of using
order to capture the local relationships between different propositions,
we use destinations. 

Linear logic alone is able to 
express any (flat, concurrent) specifications that 
we could not express using ordered atomic propositions. 
We will demonstrate that fact in this chapter by 
% In Chapter~\ref{chapter-absmachine}, we showed that ordered
% abstract machines were at least as expressive as (moded) natural
% semantics by giving a transformation, operationalization, from the
% latter to the former. Analogously, in this chapter we will argue that
% concurrent specifications in linear logic are just as expressive as
% concurrent specifications in ordered logic by 
describing a transformation,
{\it destination-adding}, from specifications with ordered atomic
propositions
to specifications that only include linear and persistent atomic 
propositions. This
destination-adding transformation, which we originally presented in
\cite{simmons11logical}, turns all ordered atomic propositions into
linear atomic propositions, and tags them with two new arguments (the
destinations of the destination-adding transformation). These extra
destinations serve as a link between a formerly-ordered atomic
proposition and its two former neighbors in the ordered context.  When
we perform the destination-adding transformation on the specification
in Figure~\ref{fig:pda-ord}, we get the specification in
Figure~\ref{fig:pda-lin}.

\begin{figure}[ht]
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/pda-lin.sls}
\caption{Linear \sls~specification of a PDA for parenthesis matching}
\label{fig:pda-lin}
\end{figure}

The specification in Figure~\ref{fig:pda-lin}, like every other
specification that results from destination-adding, has no occurrences
of ${\downarrow}A^-$ and no ordered atomic propositions.  As a result,
we write \verb|hd L M| instead of \verb|$hd L M|, omitting the
optional linearity indicator \verb|$| on the linear atomic
propositions as discussed in
Section~\ref{sec:prototype}. Additionally, by the discussion in
Section~\ref{sec:perm-fragments}, we would be justified in viewing
this specification as a linear logical specification (or a CLF
specification) instead of a ordered logical specification in \sls.
This would not impact the structure of the derivations significantly;
essentially, it just means that we would write $A^+_1 \lolli \{ A^+_2
\}$ instead of $A^+_1 \lefti \{ A^+_2 \}$.  This reinterpretation was
used in \cite{simmons11logical}, but we will stick with the notation
of ordered logic for consistency, while recognizing that there is
nothing ordered about specifications like the one in
Figure~\ref{fig:pda-lin}.

When the destination-adding translation is applied to ordered abstract
machine SSOS specifications, the result is a style of SSOS
specification called {\it destination-passing}. Destination-passing
specifications were the original style of SSOS specification proposed
in the CLF technical reports~\cite{cervesato02concurrent}. Whereas the
operationalization transformation exposed the structure of natural
semantics proofs so that they could be modularly extended with
stateful features, the destination-adding translation exposes the
control structure of specifications, allowing the language to be
modularly extended with control effects and effects like
synchronization.

\section{Logical transformation: destination-adding}
\label{sec:destination-adding}

The translation we define operates on rules the form $\forall
\overline{x}. S_1 \lefti \{ S_2 \}$, where $S_1$ must contain at least
one ordered atomic proposition. The syntactic category $S$ is
a refinement of the positive types $A^+$ defined by the following
grammar:
\[
S ::= p^+_\mpers \mid p^+_\meph \mid p^+ \mid \one
\mid \lf{t} \doteq \lf{s} \mid S_1 \fuse S_2 \mid \exists \lf{x}{:}\tau. S
\]
The translation of a rule $\forall \lf{\overline{x}}. S_1 \lefti \{ S_2 \}$
is then $\forall \lf{\overline{x}}.\, \forall \lf{d_L}{:}{\sf dest}.\, \forall
\lf{d_R}{:}{\sf dest}.\, \llbracket S_1 \rrbracket^{\lf{d_L}}_{\lf{d_R}} \lefti \{
\llbracket S_2 \rrbracket^{\lf{d_L}}_{\lf{d_R}} \}$, where $\llbracket S
\rrbracket^{\lf{d_L}}_{\lf{d_R}}$ is defined in Figure~\ref{fig:destadd-pos}. It
is also necessary to transform all ordered predicates with kind
$\Pi. \lf{x_1}{:}\tau_1\ldots \Pi.\lf{x_n{}:}\tau_n.\,{\sf prop\,ord}$ that are
declared in the signature into predicates with kind
$\Pi. \lf{x_1}{:}\tau_1\ldots \Pi.\lf{x_n}{:}\tau_n.\, {\sf dest} \rightarrow
{\sf dest} \rightarrow {\sf prop\,ord}$ in order for the translation
of an ordered atomic proposition $p^+$ to remain well-formed in
the transformed signature.

\begin{figure}
\begin{align*}
\llbracket p^+ \rrbracket^{\lf{d_L}}_{\lf{d_R}} & = 
 {\sf a}\,\lf{t_1}\ldots \lf{t_n}\,\lf{d_L}\,\lf{d_R} ~~~ \mbox{(where $p^+ = {\sf a}\,\lf{t_1}\ldots \lf{t_n}$)}
\\
\llbracket p^+_\meph \rrbracket^{\lf{d_L}}_{\lf{d_R}} & = p^+_\meph \fuse \lf{d_L} \doteq \lf{d_R}
\\
\llbracket p^+_\mpers \rrbracket^{\lf{d_L}}_{\lf{d_R}} & = p^+_\mpers \fuse \lf{d_L} \doteq \lf{d_R}
\\
\llbracket \one \rrbracket^{\lf{d_L}}_{\lf{d_R}} & = \lf{d_L \doteq d_R}
\\
\llbracket \lf t \doteq \lf s \rrbracket^{\lf{d_L}}_{\lf{d_R}} & = \lf{t} \doteq \lf{s} \fuse \lf{d_L} \doteq \lf{d_R}
\\
\llbracket S_1 \fuse S_2 \rrbracket^{\lf{d_L}}_{\lf{d_R}} & = 
 \exists \lf{d_M}{:}{\sf dest}.\, 
   \llbracket S_1 \rrbracket^{\lf{d_L}}_{\lf{d_M}}
   \fuse
   \llbracket S_2 \rrbracket^{\lf{d_M}}_{\lf{d_R}}
\\
\llbracket \exists \lf{x}{:}\tau.\,S \rrbracket^{\lf{d_L}}_{\lf{d_R}} & := 
 \exists \lf{x}{:}\tau.\, \llbracket S \rrbracket^{\lf{d_L}}_{\lf{d_R}}
\end{align*}
\caption{Destination-adding transformation}
\label{fig:destadd-pos}
\end{figure}


The destination-adding translation presented here is the same as the
one presented in \cite{simmons11logical}, except that that the
transformation operated on rules of the form $\forall
\overline{x}. S_1 \righti \{ S_2 \}$ and ours will operate over rules
of the form $\forall \overline{x}. S_1 \lefti \{ S_2 \}$.\footnote{The
  monad $\{ S_2 \}$ did not actually appear in
  \cite{simmons11logical}, and the presentation took polarity into
  account but was not explicitly polarized. We are justified in
  reading the lax modality back in by the erasure arguments discussed
  in Section~\ref{sec:perm-fragments}.} As discussed in
Section~\ref{sec:defunc-uncurry}, the difference between $\righti$ and
$\lefti$ is irrelevant in this situation. The restriction to flat
specifications, on the other hand, is an actual limitation. We
conjecture that the translation presented here, and the correctness
proof presented in \cite{simmons11logical}, would extend to nested
\sls~specifications. However, the correctness proofs in that work are
already quite tedious (though our explicit notation for patterns as
partial derivations might help simplify the proof somewhat) and the
limited transformation described by Figure~\ref{fig:destadd-pos}
 is sufficient for our purposes. Therefore, we
will rely on the existing result, leaving the correctness of a more
general development for future work.

According to Figure~\ref{fig:destadd-pos}, the rule 
${\sf pop}$ in Figure~\ref{fig:pda-lin} should actually be written as
follows:
\begin{align*} 
  {\sf pop} & : 
  \forall \lf x{:}{\sf tok}.\,
  \forall \lf l{:}{\sf dest}.\,
  \forall \lf r{:}{\sf dest}.\,
  \\
  & \qquad (\exists \lf{m_1}{:}{\sf dest}.\, {\sf stack}\,\lf{x}\,\lf{l}\,\lf{m_1} \fuse
   (\exists \lf{m_2}{:}{\sf dest}.\, {\sf hd}\,\lf{m_1}\,\lf{m_2} \fuse
     {\sf right}\,\lf{x}\,\lf{m_2}\,\lf{r}))
  \\ 
  & \qquad\quad \lefti
  \{ 
    {\sf hd}\,\lf{l}\,\lf{r}
  \}
\end{align*}
The destination-adding transformation as
implemented produces rules that are
equivalent to the specification in Figure~\ref{fig:destadd-pos} but
that avoid unnecessary equalities and push existential quantifiers as
far out as possible (which includes turning existential quantifiers
$(\exists \lf x.\,A^+) \lefti B^-$ into universal quantifiers $\forall
\lf x.\,A^+ \lefti B^-$). The result is a specification, equivalent
at the level of synthetic transitions, that looks like the one in
Figure~\ref{fig:pda-lin}. We write the result of the
destination-adding transformation on the signature $\Sigma$ as ${\it
  Dest}(\Sigma)$.

We can consider a further simplification: is it necessary to generate
a new destination $\lf{m}$ by existential quantification in the head
$\exists \lf{m}.\,{\sf stack}\,\lf{x}\,\lf{l}\,\lf{m} \fuse {\sf hd}\,\lf{m}\,\lf{r}$ of ${\sf
  push}$ in Figure~\ref{fig:pda-lin}? There is already a destination
$\lf m$ mentioned in the head that will be unused in the conclusion.  It
would, in fact, be possible to avoid generating new destinations in
the transformation of rules $\forall \lf{\overline{x}}.\,S_1 \lefti \{ S_2
\}$ where the head $S_2$ contains no more ordered atomic propositions
than the premise $S_1$. 

We don't perform this simplification for a number of reasons. First
and foremost, the transformation described in
Figure~\ref{fig:destadd-pos} more closely follows the previous work by
Morrill, Moot, Piazza, and van Benthem discussed in
Section~\ref{sec:correspondence-related}, and transformation as given
simplifies the correctness proof (Theorem~\ref{thm:destcorrect}).
Pragmatically, the additional existential quantifiers also give us
more structure to work with when considering program abstraction in
Chapter~\ref{chapter-approx}. Finally, if we apply both the transformation in
Figure~\ref{fig:destadd-pos} and a transformation that reuses
destinations to an ordered abstract machine SSOS specification, the
former transformation produces results that are more in line with
existing destination-passing SSOS specifications.

% Another reason for preserving the existential quantifier in the head
% of the ${\sf push}$ rule is that it allows us to make an extension to
% the destination-adding transformation beyond what was considered in
% \cite{simmons11logical}. As long as the head of every translated rule
% contains at least one formerly-ordered atomic proposition that has
% been turned into a linear atomic propsition, it is possible to 
% without breaking Theorem~\ref{thm:destcorrect}. 

%  The correctness of the transformation 
% depends critically on the fact that every portion of the context
% that might be used to successfully right focus on a translated positive 
% proposition $\llbracket S \rrbracket^{\lf{d_L}}_{\lf{d_R}}$ is a 

To prove the correctness of destination-adding, we must describe a 
translation $\llbracket \Psi; \Delta \rrbracket$ from process states
with ordered, linear, and persistent atomic propositions to ones
with only linear and persistent atomic propositions:
\begin{align*}
\llbracket \Psi; \cdot \rrbracket & = (\Psi, \lf{d_L}{:}{\sf dest}; \cdot) 
\\
\llbracket \Psi; \Delta, x{:}\istrue{\susp{{\sf a}\,\lf{t_1}\ldots \lf{t_n}}} \rrbracket 
& = (\Psi', \lf{d_L}{:}{\sf dest}, \lf{d_R}{:}{\sf dest}; 
     \Delta', x{:}\susp{{\sf a}\,\lf{t_1}\ldots \lf{t_n}\,\lf{d_L}\,\lf{d_R}})\\
& \qquad\qquad\qquad
  \mbox{(where $\sf a$ is ordered and
  $\llbracket \Psi; \Delta \rrbracket = (\Psi', \lf{d_L}{:}{\sf dest}; \Delta') $)}
\\
\llbracket \Psi; \Delta, x{:}\istrue{S} \rrbracket 
& = (\Psi', \lf{d_L}{:}{\sf dest}, \lf{d_R}{:}{\sf dest}; 
     \Delta', x{:}\istrue{\llbracket S \rrbracket^{\lf{d_L}}_{\lf{d_R}}})\\
& \qquad\qquad\qquad
  \mbox{(where $\sf a$ is ordered and
  $\llbracket \Psi; \Delta \rrbracket = (\Psi', \lf{d_L}{:}{\sf dest}; \Delta') $)}
\\
\llbracket \Psi; \Delta, x{:}\iseph{\susp{p^+_\meph}} \rrbracket 
& = (\Psi'; \Delta', x{:}\susp{p^+_\meph})
\\ &
 \qquad\qquad\qquad \mbox{(where
       $\llbracket \Psi; \Delta \rrbracket = (\Psi'; \Delta') $)}
\\
\llbracket \Psi; \Delta, x{:}\ispers{\susp{p^+_\mpers}} \rrbracket 
& = (\Psi'; \Delta', x{:}\susp{p^+_\mpers})
\\ &
  \qquad\qquad\qquad \mbox{(where 
       $\llbracket \Psi; \Delta \rrbracket = (\Psi'; \Delta') $)}
\end{align*}

\begin{theorem}[Correctness of destination-adding]~\\\label{thm:destcorrect}
$\llbracket \Psi; \Delta \rrbracket \leadsto_{{\it Dest}(\Sigma)}
 (\Psi_l; \Delta_l)$ if and only if 
$(\Psi; \Delta) \leadsto_\Sigma (\Psi_o; \Delta_o)$ and
$(\Psi_l; \Delta_l) = \llbracket \Psi_o, \Psi''; \Delta_o \rrbracket$ 
for some variable 
context $\Psi''$ containing destinations free in the translation of
$\Delta$ but not in the translation of $\Delta_o$.
\end{theorem}

\begin{proof}
  This proof is given in detail in \cite[Appendix
  A]{simmons11logical}. It involves a great deal of tedious tracking
  of destinations, but the intuition behind that tedious development
  is reasonably straightforward.

  First, we need to prove that a right-focused proof of $\Psi; \Delta
  \vdash_{\Sigma} S$ implies that there is an analogous proof of
  $\llbracket \Psi; \Delta \rrbracket \vdash_{{\it Dest}(\Sigma)} [
  \llbracket S \rrbracket^{\lf{d_L}}_{\lf{d_R}} ]$, and conversely
  that if we can prove $\Psi; \Delta \vdash_{{\it Dest}(\Sigma)} [\llbracket
  S \rrbracket^{\lf{d_L}}_{\lf{d_R}}]$ in right focus under then
  linear translation, then it is possible to reconstruct an ordered
  context $\Psi'; \Delta'$ such that $\llbracket \Psi'; \Delta'
  \rrbracket = \Psi; \Delta$ and $\Psi'; \Delta'
  \vdash_{\Sigma} [S]$ by threading together the destinations from
  $\lf{d_L}$ to $\lf{d_R}$ in $\Delta$. Both directions are by
  structural induction on the given derivation. The critical property
  is that it is possible to reconstruct the ordered context from the
  context of any right-focus sequent that arises during translation.
  Proving that property is where the flat structure of rules is
  particularly helpful; the use of positive atomic propositions comes
  in handy too \cite[Lemma 1]{simmons11logical}.

  Second, we need to prove that patterns can be translated in both
  directions: that if $(\Psi; \Delta) \Longrightarrow (\Psi';
  \Delta_o)$ under the original signature then $P :: \llbracket \Psi;
  \Delta \rrbracket \Longrightarrow \llbracket \Psi'; \Delta_o \rrbracket$
  under the translated signature  \cite[Lemma 4]{simmons11logical}, and that if 
  $P :: \llbracket \Psi;
  \Delta \rrbracket \Longrightarrow (\Psi'; \Delta_l)$ then there 
  exists $\Delta_o$ such that 
  $(\Psi'; \Delta_o) = \llbracket \Psi'; \Delta'' \rrbracket$
  \cite[Lemma 5]{simmons11logical}. Both directions are again by induction
  over the structure of the given pattern.

  The theorem then follows directly from these two lemmas.  There
  is a trivial induction on spines to handle the sequence of
  quantifiers, but the core of a flat rule is a proposition $S_1
  \lefti \{ S_2 \}$ -- we reconstruct the ordered context from the
  value used to prove $S_1$, and then begin
  inverting with the positive proposition $S_2$ in the context.
\end{proof}


If we leave off explicitly mentioning the variable context $\Psi$, 
then the trace that represents successfully processing 
the string {\sf [ ( ) ] }
with the transformed push-down automaton 
specification in Figure~\ref{fig:pda-lin} 
is as follows (we again underline ${\sf hd}$
for emphasis):
\begin{align*}
           & y_0{:}\susp{\underline{\sf hd}\,\lf{d_0}\,\lf{d_1}},
             x_1{:}\susp{{\sf left}\,{\sf sq}\,\lf{d_1}\,\lf{d_2}},
             x_2{:}\susp{{\sf left}\,{\sf pa}\,\lf{d_2}\,\lf{d_3}},
             x_3{:}\susp{{\sf right}\,{\sf pa}\,\lf{d_3}\,\lf{d_4}},
             x_4{:}\susp{{\sf right}\,{\sf sq}\,\lf{d_4}\,\lf{d_5}}
\\
\leadsto ~ & z_1{:}\susp{{\sf stack}\,{\sf sq}\,\lf{d_0}\,\lf{d_6}},
             y_1{:}\susp{\underline{\sf hd}\,\lf{d_6}\,\lf{d_2}},
             x_2{:}\susp{{\sf left}\,{\sf pa}\,\lf{d_2}\,\lf{d_3}},
             x_3{:}\susp{{\sf right}\,{\sf pa}\,\lf{d_3}\,\lf{d_4}},
             x_4{:}\susp{{\sf right}\,{\sf sq}\,\lf{d_4}\,\lf{d_5}}
\\
\leadsto ~ & z_1{:}\susp{{\sf stack}\,{\sf sq}\,\lf{d_0}\,\lf{d_6}},
             z_2{:}\susp{{\sf stack}\,{\sf pa}\,\lf{d_6}\,\lf{d_7}},
             y_2{:}\susp{\underline{\sf hd}\,\lf{d_7}\,\lf{d_3}},
             x_3{:}\susp{{\sf right}\,{\sf pa}\,\lf{d_3}\,\lf{d_4}},
             x_4{:}\susp{{\sf right}\,{\sf sq}\,\lf{d_4}\,\lf{d_5}}
\\
\leadsto ~ & z_1{:}\susp{{\sf stack}\,{\sf sq}\,\lf{d_0}\,\lf{d_6}},
             y_3{:}\susp{\underline{\sf hd}\,\lf{d_6}\,\lf{d_4}},
             x_4{:}\susp{{\sf right}\,{\sf sq}\,\lf{d_4}\,\lf{d_5}}
\\
\leadsto ~ & y_4{:}\susp{\underline{\sf hd}\,\lf{d_0}\,\lf{d_5}}
\end{align*}
One reason for leaving off the variable context $\Psi$ in this example
is that by the end it contains the LF variables $\lf{d_1}$,
$\lf{d_2}$, $\lf{d_3}$, $\lf{d_4}$, $\lf{d_5}$, $\lf{d_6}$, and
$\lf{d_7}$, none of which are actually present in the substructural
context $y_4{:}\susp{\underline{\sf hd}\,\lf{d_0}\,\lf{d_5}}$. We can
informally think of these destinations as having been ``garbage
collected,'' but this notion is not supported by the formal system we
described in Chapter~\ref{chapter-framework}.


\begin{figure}[t]
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/dest-vestige.sls}
\caption{Translation of Figure~\ref{fig:cbv-ev-ssos-fun} with vestigial destinations}
\label{fig:dest-vestige}
\end{figure}

\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/dest-cbv.sls}
\caption{Translation of Figure~\ref{fig:cbv-ev-ssos-fun} without vestigial destinations}
\label{fig:dest-cbv}
\end{figure}



\subsection{Vestigial destinations}
\label{sec:vestigial}

When we apply the translation of expressions to the call-by-value
lambda calculus specification from Figure~\ref{fig:cbv-ev-ssos-fun},
we get the specification in Figure~\ref{fig:dest-vestige}. Because
${\sf eval}$ and ${\sf retn}$ are always unique and always appear at
the leftmost end of this substructural context, this specification has
a quirk: the second argument to ${\sf eval}$ and ${\sf retn}$ is
always $\lf{d'}$, and the destination never changes; it is essentially
a vestige of the destination-adding transformation. As long as we are
transforming a sequential ordered abstract machine, we can eliminate
this vestigial destination, giving us the specification in
Figure~\ref{fig:dest-cbv}. This extra destination is {\it not}
vestigial when we translate a parallel specification, but as we
discuss in Section~\ref{sec:modular-parallelism}, we don't necessarily
want to apply destination-adding to parallel ordered abstract machines
anyway.

\subsection{Persistent destination passing}
\label{sec:persistentdestpass}

When we translate our PDA specification, it is actually not necessary
to translate ${\sf hd}$, ${\sf left}$, ${\sf right}$ and ${\sf stack}$ as
linear atomic propositions. If we translate ${\sf hd}$ as
a linear predicate but translate the other predicates as persistent
predicates, it will still be the case that there is always exactly one
linear atomic proposition ${\sf hd}\,\lf{d_L}\,\lf{d_R}$ 
in the context, at most one
${\sf stack}\,\lf{x}\,\lf{d}\,\lf{d_L}$ 
proposition with the same destination $\lf{d_L}$, 
and at most one ${\sf right}\,\lf{x}\,\lf{d_R}\,\lf{d}$ or 
${\sf left}\,\lf{x}\,\lf{d_R}\,\lf{d}$ 
with the same destination $\lf{d_R}$. This means it is still the case that the
PDA accepts the string if and only if there is the following series of 
transitions:
\begin{align*}
 %(d_0{:}{\sf dest}, \ldots, d_{n+1}{:}{\sf dest}; 
(    x{:}\susp{{\sf hd}\,\lf{d_0}\,\lf{d_1}}, 
    y_1{:}\susp{{\sf left}\,\lf{x_1}\,\lf{d_1}\,\lf{d_2}},
    \ldots&,
    y_n{:}\susp{{\sf right}\,\lf{x_n}\,\lf{d_n}\,\lf{d_{n+1}}})% )
%\\
% &
~~ \leadsto^* ~~
%   (\Psi; 
(\Gamma, z{:}\susp{{\sf hd}\,\lf{d_0}\,\lf{d_{n+1}}})%)
\end{align*}
Unlike the entirely-linear PDA specification, the final state may include
some additional 
persistent propositions, represented by $\Gamma$. Specifically, the final state
contains all the original ${\sf left}\,\lf{x}\,\lf{d_i}\,\lf{d_{i+1}}$ and
${\sf right}\,\lf{x}\,\lf{d_i}\,\lf{d_{i+1}}$ propositions 
along with all the ${\sf stack}\,\lf{x}\,\lf{d}\,\lf{d'}$ 
propositions that were created
during the course of evaluation.

% \begin{figure}[t]
% \fvset{fontsize=\small,boxwidth=229pt}
% \VerbatimInput{sls/pda-pers.sls}
% \caption{Linear/persistent \sls~specification of a PDA for parenthesis
%   matching.}
% \label{fig:pda-pers}
% \end{figure}

I originally conjectured that a version of
Theorem~\ref{thm:destcorrect} would hold in any specification that
turned some ordered atomic propositions linear and others
persistent just as long as at least one atomic proposition in
the premise of every rule remained linear after transformation.  
This would have given a
generic justification for turning ${\sf left}$, ${\sf right}$ and ${\sf
  stack}$ persistent in Figure~\ref{fig:pda-lin} and to turning ${\sf
  cont}$ persistent in Figure~\ref{fig:dest-cbv}. However, that
condition is not strong enough.  To see why, consider a signature with
one rule, ${\sf a} \fuse {\sf b} \fuse {\sf a} \lefti \{ {\sf b} \}$,
where ${\sf a}$ and ${\sf b}$ are ordered atomic propositions.  We can
construct the following trace:
\begin{align*}
& (x_1{:}\susp{\sf a}, x_2{:}\susp{\sf b}, x_3{:}\susp{\sf a}, 
  x_4{:}\susp{\sf b}, x_5{:}\susp{\sf a})
\leadsto 
(x{:}\susp{\sf b},
  x_4{:}\susp{\sf b}, x_5{:}\susp{\sf a})
\not\leadsto  
\intertext{From the same starting point, exactly one
other trace is possible:}
& (x_1{:}\susp{\sf a}, x_2{:}\susp{\sf b}, x_3{:}\susp{\sf a}, 
  x_4{:}\susp{\sf b}, x_5{:}\susp{\sf a})
\leadsto 
(x_1{:}\susp{\sf a}, x_2{:}\susp{\sf b}, x{:}\susp{\sf b})
\not\leadsto 
\end{align*}
However, if we perform the destination-passing transformation, letting
${\sf a}\,\lf{d}\,\lf{d'}$ be a persistent atomic proposition and letting ${\sf
  b}\,\lf{d}\,\lf{d'}$ be a linear atomic proposition, then we have a series of
transitions in the transformed specification that can reuse the atomic
proposition ${\sf a}\,\lf{d_2}\,\lf{d_3}$ in a way that doesn't correspond to
any series of transitions in ordered logic:
\begin{align*}
&  x_1{:}\susp{{\sf a}\,\lf{d_0}\,\lf{d_1}}, 
   x_2{:}\susp{{\sf b}\,\lf{d_1}\,\lf{d_2}}, 
   x_3{:}\susp{{\sf a}\,\lf{d_2}\,\lf{d_3}}, 
   x_4{:}\susp{{\sf b}\,\lf{d_3}\,\lf{d_4}}, 
   x_5{:}\susp{{\sf a}\,\lf{d_4}\,\lf{d_5}}
\\ \leadsto~
&  x_1{:}\susp{{\sf a}\,\lf{d_0}\,\lf{d_1}}, 
   \underline{x{:}\susp{{\sf b}\,\lf{d_0}\,\lf{d_3}}}, 
   x_3{:}\susp{{\sf a}\,\lf{d_2}\,\lf{d_3}}, 
   x_4{:}\susp{{\sf b}\,\lf{d_3}\,\lf{d_4}}, 
   x_5{:}\susp{{\sf a}\,\lf{d_4}\,\lf{d_5}}
\\ \leadsto~
&  x_1{:}\susp{{\sf a}\,\lf{d_0}\,\lf{d_1}}, 
   x{:}\susp{{\sf b}\,\lf{d_0}\,\lf{d_3}}, 
   x_3{:}\susp{{\sf a}\,\lf{d_2}\,\lf{d_3}}, 
   \underline{x'{:}\susp{{\sf b}\,\lf{d_2}\,\lf{d_5}}}, 
   x_5{:}\susp{{\sf a}\,\lf{d_4}\,\lf{d_5}}
\end{align*}
In the
first process state, there is a path 
$\lf{d_0}, \lf{d_1}, \lf{d_2}, \lf{d_3}, \lf{d_4}, \lf{d_5}$ through
the context that reconstructs the ordering in the original ordered context.
In the second process state, there is still a path 
$\lf{d_0}, \lf{d_3}, \lf{d_4}, \lf{d_5}$ that
allows us to reconstruct the ordered context
$(x{:}\susp{\sf b},
  x_4{:}\susp{\sf b}, x_5{:}\susp{\sf a})$ by ignoring the persistent
propositions associated with $x_1$ and $x_3$. 
However, in the third process state above, no path exists, so the final
state cannot be reconstructed as any ordered context. 

It would be good to identify a condition that allowed us to
selectively turn some ordered propositions persistent when
destination-adding without violating (a version of)
Theorem~\ref{thm:destcorrect}. In the absence of such a generic
condition, it is still straightforward to see that performing
destination-passing and then turning some propositions persistent is
an {\it abstraction}: if the original system can make a series of
transitions, the transformed system can simulate those transitions,
but the reverse may not be true. In any case, we can observe that, for
many of systems we are interested in, the partially-persistent
destination-passing specification can only make transitions that were
possible in the ordered specification.  The push-down automata with
persistent ${\sf stack}$, ${\sf left}$, and ${\sf right}$ is one
example of this, and we can similarly make the ${\sf cont}$ predicate
persistent in SSOS~specifications without introducing any new
transitions. Turing the ${\sf cont}$ predicate persistent will in
fact be necessary for the discussion of first-class
continuations in Section~\ref{sec:dest-continuations}.

\section{Exploring the richer fragment}

In \cite{simmons11logical}, we were interested in exact logical
correspondence between ordered abstract machine SSOS specifications
and destination-passing SSOS specifications. (Destination-adding was
useful in that context because it exposes information about the
control structure of computations; this control structure can be
harnessed by the program abstraction methodology described in
Chapter~\ref{chapter-approx} to derive program analyses.) In keeping
with our broader use of the logical correspondence, this section will
cover programming language features that are not easily expressible
with ordered abstract machine SSOS specifications but that can be
easily expressed with destination-passing SSOS
specifications. Consequently, these are features that can be modularly
added to (sequential) ordered abstract machine specifications that
have undergone the destination-adding transformation.

The semantics of parallelism and failure presented in
Section~\ref{sec:modular-parallelism} are new. The semantics of
futures (Section~\ref{sec:dest-futures}) and synchronization
(Section~\ref{sec:dest-synch}) are a based on the specifications first
presented in the CLF tech report \cite{cervesato02concurrent}. The
semantics of first-class continuations
(Section~\ref{sec:dest-continuations}) were presented previously in
\cite{pfenning04substructural,pfenning09substructural}. In
destination-passing semantics, when we are dealing with fine-grained
issues of control flow, the interaction of programming language
features becomes more delicate.  Parallel evaluation, recoverable
failure, and synchronization are compatible features, as are
synchronization and futures. Failure and first-class continuations are
also compatible. We will not handle other interactions, though it
would be interesting to explore the adaptation of Moreau and Ribbens'
abstract machine for Scheme with parallel evaluation and ${\sf
  callcc}$ as a substructural operational semantics
\cite{moreau96semantics}.

\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/dest-pair.sls}
\caption{Destination-passing semantics for parallel evaluation of pairs}
\label{fig:dest-pair}
\end{figure}

\subsection{Alternative semantics for parallelism and failure}
\label{sec:modular-parallelism}

In Section~\ref{sec:failure}, we discussed how parallel evaluation and
recoverable failure can be combined in an ordered abstract machine
SSOS specification. Due to the fact that the two parts of a parallel
ordered abstract machine are separated by an arbitrary amount of
ordered context, some potentially desirable ways of
integrating parallelism and failure were difficult or impossible to
express, however. 

Once we transition to destination-passing SSOS specifications, it is
possible to give a more direct semantics to parallel evaluation that
better facilitates talking about failure. Instead of having the stack
frame associated with parallel pairs be ${\sf cont}\,\lf{\sf pair1}$ (as
in Figure~\ref{fig:ssos-minml-core}) or ${\sf cont2}\,\lf{\sf pair1}$ (as
discussed in Section~\ref{sec:failure}), we create a continuation
${\sf cont2}\,\lf{\sf pair1}\,\lf{d_1}\,\lf{d_2}\,\lf{d}$ 
with {\it three} destinations;
$\lf{d_1}$ and $\lf{d_2}$ represent the return destinations for the two
subcomputations, whereas $\lf{d}$ represents the destination to which the
evaluated pair is to be returned. This strategy applied to the
parallel evaluation of pairs is shown in Figure~\ref{fig:dest-pair}.

In ordered specifications, an ordered atomic proposition can be
directly connected to at most two other ordered propositions: the
proposition immediately to the left in the ordered context, and the
proposition immediately to the right in the ordered context. What
Figure~\ref{fig:dest-pair} demonstrates is that, with destinations, a
linear proposition can be locally connected to {\it any finite number}
of other propositions. Whereas in ordered abstract machine
specifications the parallel structure of a computation had to be
reconstructed by parsing the context in postfix, a
destination-passing specification uses destinations to thread together
the treelike dependencies in the context. It would presumably be
possible to consider a different version of the parallel
operationalization that targeted this desirable form of 
parallel destination-passing
specification specifically, but we will not present such a
transformation in this thesis. 

\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/dest-fail-paror.sls}
\caption{Integration of parallelism and exceptions; signals failure as
  soon as possible}
\label{fig:dest-fail-paror}
\end{figure}

Using destination-based parallel continuations, we give, in
Figure~\ref{fig:dest-fail-paror}, a semantics for recoverable failure
that eagerly returns errors from either branch of a parallel
computation. The rules ${\sf ev/errorL}$ and ${\sf ev/errorR}$
immediately pass on errors returned to a frame where the computation
forked.  Those two rules also leave behind a linear proposition ${\sf
  terminate}\,\lf{d}$ that will abort the other branch of computation if it
returns successfully (rule ${\sf term/retn}$) or with an error (rule
${\sf term/err}$). It would also be possible to add rules
like $\forall \lf{d}.\,\forall\lf{d'}.\,{\sf cont}\,\lf{d'}\,\lf{d} \fuse {\sf terminate}\,\lf{d} \lefti \{ {\sf
  terminate}\,\lf{d'} \}$ that actively aborted the useless branch instead
of passively waiting for it to finish.

\subsection{Synchronization}
\label{sec:dest-synch}

The CLF tech report gives a destination-passing presentation of
nearly the full set of Concurrent ML primitives, omitting only
negative acknowledgements \cite{cervesato02concurrent}. We will
present an \sls~version of that Concurrent ML specification as a part
of the hybrid specification in Appendix~\ref{appendix-hybrid}.
In Figure~\ref{fig:dest-synch}, rather than reprising that
specification, we present an extremely simple form of synchronous
communication.


\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/dest-synch.sls}
\caption{Semantics of simple synchronization}
\label{fig:dest-synch}
\end{figure}

New channels are created by evaluating $\interp{{\sf chan}\,c.e} =
\lf{{\sf chan}\,\lambda c.\,\interp{e}}$, which introduces a new channel
(an LF term of the type ${\sf channel}$ that has no constructors) and
substitutes it for the bound variable $\lf{c}$ in $\obj{e}$. 
Synchronization happens when
there is both a send $\obj{{\sf send}\,c\,e}$ being evaluated
in one part of the process state and a
receive $\obj{{\sf recv}\,c}$ with the same
channel being evaluated in a different part of the process state. 
The expression $\obj{e}$ will first evaluate to a value $\obj{v}$ 
(rule ${\sf ev/send}$). Communication is
driven by rule ${\sf ev/send1}$, which allows computation to continue
in both the sender and the receiver.

Synchronous communication introduces the possibility of
deadlocks. Without synchronous communication, the presence of a
suspended atomic proposition ${\sf eval}\,\lf{e}\,\lf{d}$ always
indicates the possibility of some transition, and the combination of a
proposition ${\sf retn}\,\lf{v}\,\lf{d}$ and a continuation ${\sf
  cont}\,\lf{f}\,\lf{d}\,\lf{d'}$ can either immediately transition or
else are permanently in a stuck
state. In~\cite{pfenning09substructural}, this observation motivated a
classification of atomic propositions as {\it active} propositions
like ${\sf eval}\,\lf{e}\,\lf{d}$ that independently drive
computation, {\it passive} propositions like ${\sf
  cont}\,\lf{f}\,\lf{d'}\,\lf{d}$ that do not drive computation, and
{\it latent} propositions like ${\sf retn}\,\lf{f}\,\lf{d}$ that may
or may not drive computation based on the ambient environment of
passive propositions. The specification in Figure~\ref{fig:dest-synch}
does not respect this classification because a proposition of the form
${\sf eval}\,\lf{({\sf recv}\,c)}\,\lf{d}$ cannot immediately
transition. We could restore this classification by having a rule
$\forall \lf{c}.\,\forall \lf{d}.\, {\sf eval}\,\lf{({\sf
    recv}\,c)}\,\lf{d} \lefti \{ {\sf await}\,\lf{c}\,\lf{d} \}$ for
some new passive linear predicate ${\sf await}$ and then replacing the
premise ${\sf eval}\,\lf{({\sf recv}\,C)}\,\lf{D}$ in ${\sf ev/send1}$
with ${\sf await}\,\lf{C}\,\lf{D}$.

\subsubsection{Labeled transitions}

Substructural operational semantics specifications retain much of the
flavor of abstract machines, in that we are usually manipulating
expressions along with their continuations. In ordered specifications,
continuations are connected to evaluating expressions and returning
values only by their relative positions in the ordered context; in
destination-passing specifications, expressions and values are
connected to continuations by the threading of destinations.

Abstract machines are not always the most natural
way to express a semantics. This observation is part of what
motivated our discussion of the operationalization transformation from
natural semantics (motto: ``natural'' is our first name!) 
and our informal
discussion of statefully-modular natural semantics in
Section~\ref{sec:enriching-natsem}.  In Chapter~\ref{chapter-absmachine}, 
we showed that the
continuation-focused perspective of SSOS allowed us to expose
computation to the ambient state. With the example of synchronization
above, we see that destination-passing SSOS specifications also expose
computations in the process state to {\it other computations}, which
is what allows the synchronization in rule ${\sf ev/send1}$ to take
place.

In small-step operational semantics, {\it labeled deduction} is used
to describe specifications like the one above. At a high level,
in a labeled transition system we
inductively define a small step judgment $\obj{e
  ~{\stackrel{\footnotesize{\it lab}}{\longmapsto}}~ e'}$ with the
property that \smallskip
\begin{itemize}
\item $\obj{e ~{\stackrel{\footnotesize{c{\textbf{!}}v}}{\longmapsto}}~
e'}$ if $\obj{e}$ steps to $\obj{e'}$ by reducing some subterm 
$\obj{{\sf send}\,c\,v}$,
to $\obj{\langle\rangle}$,
\item $\obj{e
~{\stackrel{\footnotesize{c{\textbf{?}}v}}{\longmapsto}}~ e'}$ if $\obj{e}$
steps to $\obj{e'}$ by reducing some subterm $\obj{{\sf recv}\,c}$ to 
$\obj{v}$, and 
\item $\obj{e_1}$ in parallel with $\obj{e_2}$ (and possibly also in
  parallel with some other $\obj{e_3}$, $\obj{e_4}$, etc.) can step to
  $\obj{e_1'}$ in parallel with $\obj{e_2'}$ (and in parallel with an
  unchanged $\obj{e_3}$, $\obj{e_4}$, etc.) if $\obj{e_1
    ~{\stackrel{\footnotesize{c{\textbf{!}}v}}{\longmapsto}}~ e_1'}$
  and $\obj{e_2
    ~{\stackrel{\footnotesize{c{\textbf{?}}v}}{\longmapsto}}~ e_2'}$.
\end{itemize}
\smallskip 
%
Labels essentially serve to pass messages up through the inductive
structure of a proposition.  In destination-passing SSOS semantics, on
the other hand, the internal structure of $e$ is spread out as an
series of frames throughout the context, and so the innermost redexes
of terms can be directly connected. It would be interesting (but
probably quite nontrivial) to consider a translation from labeled
deduction systems to destination-passing SSOS specifications along the
lines of the operationalization transformation.


\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/dest-futures.sls}
\caption{Semantics of call-by-future functions}
\label{fig:dest-futures}
\end{figure}

\subsection{Futures}
\label{sec:dest-futures}

Futures can be seen as a parallel version of call-by-value, and the
presentation in Figure~\ref{fig:dest-futures} can be compared to the
environment semantics for call-by-value in
Figure~\ref{fig:ssos-by-env}. We introduce future-functions as a new
kind of function $\lf{{\sf flam}\,\lambda x.e}$ comparable to plain-vanilla
call-by-value functions $\lf{{\sf lam}\,\lambda x.e}$, lazy call-by-need
functions $\lf{{\sf lazylam}\,\lambda x.e}$, and environment-semantics
functions $\lf{{\sf envlam}\,\lambda x.e}$. As in the environment semantics
specification, when a call-by-future function returns to a frame
$\interp{\Box\,e_2} = \lf{{\sf app1}\,\interp{e_2}}$,
 we create a new expression $\lf{x}$
by existential quantification. However, instead of suspending the
function body on the stack as we did in Figure~\ref{fig:ssos-by-env},
in Figure~\ref{fig:dest-futures} we create a new destination $\lf{\it
  dfuture}$ and start evaluating the function argument towards that
destination (rule ${\sf ev/fapp1}$). We also create a linear
proposition -- ${\sf promise}\,\lf{\it dfuture}\,\lf{x}$ -- that will take
any value returned to $\lf{\it dfuture}$ and permanently bind it to $\lf{x}$
(rule ${\sf ev/promise}$). As a proposition that only exists during
the course of evaluating the argument, ${\sf promise}$ is analogous to
the black hole in our specification of lazy call-by-need.

\begin{figure}
\begin{center}
\begin{tikzpicture}
\draw (0,0) node[right]{\small $x_1{:}\susp{{\sf eval}\,\interp{(\lambda x. {\sf s}\,x)\,({\sf s}\,{\sf z})}\,\lf{d_1}}$};
\draw (0,-.7) node[right]{\small $x_2{:}\susp{{\sf eval}\,\interp{\lambda x. {\sf s}\,x}\,\lf{d_2}}, ~~ x_3{:}\susp{{\sf cont}\,\interp{\Box\,({\sf s}\,{\sf z})}\,\lf{d_2}\,\lf{d_1}}$};
\draw (0,-1.4) node[right]{\small $x_3{:}\susp{{\sf retn}\,\interp{\lambda x. {\sf s}\,x}\,\lf{d_2}}, ~~ x_3{:}\susp{{\sf cont}\,\interp{\Box\,({\sf s}\,{\sf z})}\,\lf{d_2}\,\lf{d_1}}$};
%
\draw (0,-2.1) node[right]{\small $x_4{:}\susp{{\sf eval}\,\lf{({\sf succ}\,\lf x)}\,\lf{d_1}},$};
\draw (0,-2.8) node[right]{\small $\textcolor{grayout}{x_4{:}\susp{{\sf eval}\,{({\sf succ}\, x)}\,{d_1}},}$};
\draw (0,-3.5) node[right]{\small $
   x_4{:}\susp{{\sf eval}\,\lf{\lf x}\,\lf{d_5}}, ~~ 
   x_9{:}\susp{{\sf cont}\,\lf{\sf succ1}\,\lf{d_5}\,\lf{d_1}},$};
\draw (0,-4.2) node[right]{\small $\textcolor{grayout}{
   x_4{:}\susp{{\sf eval}\,{ x}\,{d_5}}, ~~ 
   x_9{:}\susp{{\sf cont}\,{\sf succ1}\,{d_5}\,{d_1}},}$};
\draw (0,-4.9) node[right]{\small $\textcolor{grayout}{
   x_4{:}\susp{{\sf eval}\,{ x}\,{d_5}}, ~~ 
   x_9{:}\susp{{\sf cont}\,{\sf succ1}\,{d_5}\,{d_1}},}$};
\draw (0,-5.6) node[right]{\small $\textcolor{grayout}{
   x_4{:}\susp{{\sf eval}\,{ x}\,{d_5}}, ~~ 
   x_9{:}\susp{{\sf cont}\,{\sf succ1}\,{d_5}\,{d_1}},}$};
\draw (0,-6.3) node[right]{\small $
   x_{13}{:}\susp{{\sf retn}\,\interp{\sf s\,z}\,\lf{d_5}}, ~~ 
   x_9{:}\susp{{\sf cont}\,\lf{\sf succ1}\,\lf{d_5}\,\lf{d_1}}, ~~
   x_{12}{:}\ispers{\susp{{\sf bind}\,\lf{x}\,\interp{\sf s\,z}}}$};
\draw (0,-7.0) node[right]{\small $
   x_{14}{:}\susp{{\sf retn}\,\interp{\sf s\,(s\,z)}\,\lf{d_1}}, ~~ 
   x_{12}{:}\ispers{\susp{{\sf bind}\,\lf{x}\,\interp{\sf s\,z}}}$};
%
\draw (6.2,-2.1) node[right]{\small $x_5{:}\susp{{\sf eval}\,\interp{\sf s\,z}\,\lf{d_3}}, ~~ x_6{:}\susp{{\sf promise}\,\lf{d_3}\,\lf{x}}$};
\draw (6.2,-2.8) node[right]{\small $
   x_7{:}\susp{{\sf eval}\,\interp{\sf z}\,\lf{d_4}}, ~~ 
   x_8{:}\susp{{\sf cont}\,\lf{\sf succ1}\,\lf{d_4}\,\lf{d_3}}, ~~ 
   x_6{:}\susp{{\sf promise}\,\lf{d_3}\,\lf{x}}$};
\draw (6.2,-3.5) node[right]{\small $\textcolor{grayout}{
   x_7{:}\susp{{\sf eval}\,\softinterp{\sf z}\,{d_4}}, ~~ 
   x_8{:}\susp{{\sf cont}\,{\sf succ1}\,{d_4}\,{d_3}}, ~~ 
   x_6{:}\susp{{\sf promise}\,{d_3}\,{x}}}$};
\draw (6.2,-4.2) node[right]{\small $
   x_{10}{:}\susp{{\sf retn}\,\interp{\sf z}\,\lf{d_4}}, ~~ 
   x_8{:}\susp{{\sf cont}\,\lf{\sf succ1}\,\lf{d_4}\,\lf{d_3}}, ~~ 
   x_6{:}\susp{{\sf promise}\,\lf{d_3}\,\lf{x}}$};
\draw (6.2,-4.9) node[right]{\small $
   x_{11}{:}\susp{{\sf retn}\,\interp{\sf s\,z}\,\lf{d_3}}, ~~ 
   x_6{:}\susp{{\sf promise}\,\lf{d_3}\,\lf{x}}$};
\draw (6.2,-5.6) node[right]{\small $
   x_{12}{:}\ispers{\susp{{\sf bind}\,\lf{x}\,\interp{\sf s\,z}}}$};
%
\draw[dashed] (15.8, -3.8) -- (0, -3.8) -- (0,-1.8) -- (15.8, -1.8) -- (15.8, -3.8);
\draw[dashed] (15.8, -3.9) -- (0, -3.9) -- (0,-5.9) -- (15.8, -5.9) -- (15.8, -3.9);
%
\draw (15.5,0) node[left]{\small \it In this phase,};
\draw (15.5,-.5) node[left]{\small \it the two computations};
\draw (15.5,-1) node[left]{\small \it proceed in parallel};
\draw[->] (12,-.7) -- (11,-1.7);
%
\draw (15.5,-6.5) node[left]{\small \it In this phase,};
\draw (15.5,-7) node[left]{\small \it the primary computation is stuck};
\draw (15.5,-7.5) node[left]{\small \it while it waits on the promise};
\draw[->] (11.8,-6.8) -- (11,-6.0);
%
\draw (.5,-7.7) node[right]{\small \it Remember: ${\sf bind}$ is persistent, all other propositions are linear};
\draw[->] (8,-7.4) -- (9.7,-6.6);
\draw[->] (7.8,-7.4) -- (7.5,-7.2);
\end{tikzpicture}
\end{center}
\caption{Series of process states in an example call-by-future evaluation}
\label{fig:examplefuturefunciton}
\end{figure}

Futures use destinations to create new and potentially disconnected
threads of computation, which can be seen in the example evaluation of
$\obj{(\lambda x. {\sf s}\,x)\,({\sf s}\,{\sf z})}$ -- where
$\obj{\lambda x.e}$ is interpreted as a future function $\lf{\sf
  flam}$ instead of $\lf{\sf lam}$ as before -- given in
Figure~\ref{fig:examplefuturefunciton}. That figure illustrates how
spawning a future splits the destination structure of the ordered
context into two disconnected threads of computation.  This was not
possible in the ordered framework where every computation had to be
somewhere specific in the ordered context relative to the current
computation -- either to the left, or to the right. These threads are
connected not by destinations but by the variable $\lf x$, which the
primary computation needs the future to return before it can proceed.

Note the similarity between the commented-out rule fragment in
Figure~\ref{fig:dest-futures} and the commented out rule fragments in
the specifications of call-by-need evaluation
(Section~\ref{sec:call-by-need}). In the call-by-need specifications,
needing an unavailable value was immediately fatal. With
specifications, needing an unavailable value is not immediately fatal:
the main thread of computation is stuck, but only until the future's
promise is fulfilled.

The destination-passing semantics of futures interact seamlessly with
the semantics of synchronization and parallelism, but not with the
semantics of recoverable failure: we would have to make some choice
about what to do when a future signals failure. 

%\footnote{``Like lazy evaluation, futures can be used to compute 
%nonstrict functions -- functions that terminate despite the 
%possible nontermination of the computation of one or more argument
%values.'' \cite{halsted85multilisp}}

\subsection{First-class continuations}
\label{sec:dest-continuations}

First-class continuations are a sophisticated control feature.  {\it
  Continuations} are another name for the stacks $\obj{k}$ in abstract
machine semantics with states $\obj{k \rhd e}$ and $\obj{k \lhd v}$
(and also, potentially, $\obj{k {\blacktriangleleft}}$ if we want to
be able to return errors, as discussed in
Section~\ref{sec:failure}). First-class continuations introduce a new
value, $\obj{{\sf contn}\,k}$, to the language. Programmers cannot
write continuations $\obj{k}$ directly, just as they cannot write
locations $\obj{l}$ directly; rather, the expression $\interp{{\sf
    letcc}\,x.e} = \lf{{\sf letcc}\,\lambda x.\interp{e}}$ captures
the current expression as a continuation:
\begin{align*}
  \obj{k \rhd {\sf letcc}\,x.e} ~~&\obj{\mapsto}~~ \obj{k \rhd [{\sf contn}\,k/x]e}
  \intertext{There is a third construct, $\interp{{\sf
        throw}\,e_1\,{\sf to}\,e_2} = \lf{{\sf throw}\,\interp{e_1}\,\interp{e_2}}$ that
    evaluates $\obj{e_1}$ to a value $\obj{v_1}$, evaluates $\obj{e_2}$ to a
    continuation value $\obj{{\sf cont}\,k'}$, and then throws away the
    current continuation in favor of returning $\obj{v_1}$ to $\obj{k'}$:}  
  \obj{k \rhd {\sf throw}\,e_1\,{\sf to}\,e_2} ~~&\obj{\mapsto}~~
  \obj{(k; {\sf throw}\,\Box\,{\sf to}\,e_2) \rhd e_1}
  \\
  \obj{(k; {\sf throw}\,\Box\,{\sf to}\,e_2) \lhd v_1} ~~&\obj{\mapsto}~~ \obj{(k; {\sf
    throw}\,v_1\,{\sf to}\,\Box) \rhd e_2}
  \\
  \obj{(k; {\sf throw}\,v_1\,{\sf to}\,\Box) \lhd {\sf contn}\,k'} ~~&\obj{\mapsto}~~
  \obj{k' \lhd v_1}
\end{align*}
When handled in a typed setting, a programming language with
first-class continuations can be seen as a Curry-Howard interpretation
of classical logic.

\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/dest-letcc.sls}
\caption{Semantics of first-class continuations (with ${\sf letcc}$)}
\label{fig:dest-letcc}
\end{figure}


In destination-passing SSOS specifications, we never represent
continuations or control stacks $\obj{k}$ directly. However, we showed
in Section~\ref{sec:nat-ssos-adequacy} that a control stack $\obj{k}$
is encoded in the context as a series of ${\sf cont}$ frames. In a
destination-passing specification, it is therefore reasonable to
associate a $\obj{k}$ continuation with the destination $\lf{d}$ that
points to the topmost frame ${\sf cont}\,\lf{f}\,\lf{d}\,\lf{d'}$ in
the stack $\obj{k}$ encoded in the process state. Destinations 
stand for continuations in much the same way that introduced
variables $\lf{x}$ in the environment semantics stand for the values
$\lf{v}$ they are bound to through persistent ${\sf
  bind}\,\lf{x}\,\lf{v}$ propositions. In Figure~\ref{fig:dest-letcc},
the rule ${\sf ev/letcc}$ captures the current continuation $\lf{d}$
as an expression $\lf{{\sf cont}\,d}$ that is substituted into the
subexpression. In rule ${\sf ev/throw2}$, the destination $\lf{\it
  dk}$ held by the value $\lf{{\sf contn}\,{\it dk}}$ gets the value
$\lf{v_1}$ returned to it; the previous continuation, represented by
the destination $\lf{d}$, is abandoned.

Just as it is critical for the ${\sf bind}$ predicate in the
environment semantics to be persistent, it is necessary, when dealing
with first-class-continuations, to have the ${\sf cont}$ predicate be
persistent. As discussed in Section~\ref{sec:persistentdestpass}, it
does not change the behavior of any SSOS specifications we have
discussed if linear ${\sf cont}$ predicates are turned into persistent
${\sf cont}$ predicates.

Turning ${\sf cont}$ into a persistent predicate does not greatly
influence the transitions that are possible, so in a sense we have not
changed our SSOS semantics very much in order to add first-class
continuations. However, the implicit representation of stacks in the
context does complicate adequacy arguments for the semantics in
Figure~\ref{fig:dest-letcc} relative to the transition rules given
above. We will return to this point in Section~\ref{sec:gen-letcc}
when we discuss generative invariants that apply to specifications using
first-class continuations.
