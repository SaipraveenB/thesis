\chapter{On logical correspondence}
\label{chapter-correspondence}

In Part 1, we defined \sls, a logical framework of substructural
logical specifications. For the purposes of this thesis, we are
primarily interested in using \sls~as a framework for specifying the
operational semantics of programming languages, especially stateful
and concurrent programming languages. This is not a new idea: one of
the original case studies on CLF specification described the semantics
of Concurrent ML \cite{cervesato02concurrent} in a specification style
termed {\it substructural operational semantics}, or SSOS, by Pfenning
\cite{pfenning04substructural}. 

The design space of substructural operational semantics is extremely
rich, and many styles of SSOS specification have been proposed
previously. It is therefore helpful to have design principles that
allow us to both {\it classify} different styles of presentation and
{\it predict} what style(s) we should adopt based on what our goals
are. In this chapter, we sketch out a classification scheme for
substructural operational semantics based on three major specification
styles:

% Ignoring \smallskip to fit thesis on page nicely
\begin{itemize}
\item The {\it natural semantics}, or big-step operational semantics,
  is an existing and well-known specification style (and not a
  substructural operational semantics). It is convenient for the
  specification of pure programming languages.

\item The {\it ordered abstract machine semantics} is a generalization
  of abstract machine semantics that can be naturally specified in
  \sls; this specification style naturally handles stateful and
  parallel programming language features
  \cite{pfenning09substructural}.

\item The {\it destination-passing semantics} is the style of
  substructural operational semantics first explored in CLF by
  Cervesato et al.~\cite{cervesato02concurrent}. It allows for the
  natural specification of features that incorporate 
  communication and non-local transfer of control. % Destination-passing
  % semantics are classified into two subspecies: those with linear
  % continuations and those with persistent continuations. Persistent
  % continuations are necessary to give a substructural operational
  % semantics to languages with first-class continuations.
\end{itemize}



\noindent
Each of these three styles is, in a very formal sense, more
expressive than the last: there are automatic and
provably-correct transformations from the less expressive styles
(natural semantics and ordered abstract machines) to the more
expressive styles (ordered abstract machines and destination-passing,
respectively).  Our investigation of provably-correct transformations
on \sls~specifications therefore justifies our classification scheme
for SSOS specifications. We call this idea the {\it logical
  correspondence}, and it is the focus of this part of the thesis:

\smallskip
\begin{quote} 
  {\bf Thesis (Part II):} {\it A logical framework based on a rewriting
  interpretation of substructural logic supports many styles of
  programming language specification. These styles can be formally
  classified and connected by considering general transformations on
  logical specifications.}
\end{quote} 
\smallskip

% \noindent
% A secondary thesis discussed in Chapter~\ref{chapter-approx} is that generally
% applicable transformations on logical specifications are also useful
% for deriving manifestly correct {\it program analyses} from
% substructural operational semantics specifications.


In this introductory chapter, we will outline our use of logical
correspondence and connect it to previous work. The development of the
logical correspondence as presented in this chapter, as well as the
operationalization and defunctionalization transformations presented
in the next chapter, represent joint work with Ian Zerny.



\section{Logical correspondence}
\label{section-introtologicalcorrespondence}

As stated above, we will primarily discuss and connect three different
styles that are used for specifying the semantics of programming
languages. The two styles of SSOS semantics, ordered abstract machines
and destination-passing semantics, are considered because they do a
good job of subsuming existing work on substructural operational
semantics, a point we will return to at the end of this
section. We consider natural semantics, a high-level, declarative style of
specification that was inspired by Plotkin's structural operational
semantics (SOS) \cite{plotkin04structural,kahn87natural},
because natural semantics specifications are the easiest style to connect
to substructural operational semantics. While we hope to extend the
logical correspondence to other specification styles, such extensions
are outside the scope of this thesis.

While Kahn et al.~defined the term broadly, natural semantics has been
consistently connected with the big-step operational semantics style
discussed in the introduction, where the judgment $\obj{e \Downarrow
  v}$ expresses that the the expression
$\obj{e}$ evaluates to the value $\obj{v}$:
\[
\infer[{\sf ev/lam}]
{\obj{\lambda x. e \Downarrow \lambda x. e} \mathstrut}
{}
\quad
\infer[{\sf ev/app}]
{\obj{e_1\,e_2 \Downarrow v} \mathstrut}
{\obj{e_1 \Downarrow \lambda x.e}
 &
 \obj{e_2 \Downarrow v_2}
 &
 \obj{[v_2/x]e \Downarrow v} \mathstrut}
\]
% Plotkin's SOS, in contrast, has been consistently identified with a
% small-step operational semantics style in which a judgment $e \mapsto
% e'$ expresses that the expression $e$ makes a single transition to the
% expression $e'$, and an auxilary judgment $e\,{\sf value}$ expresses
% that the expression $e$ is a value and not expected to make any
% further transitions. The usual SOS specification for call-by-value
% evaluation of the lambda calculus looks like this:
% \[
% \infer%[{\sf value/lam}]
% {\lambda x.e\,{\sf value} \mathstrut}
% {}
% \quad
% \infer%[{\sf step/app1}]
% {e_1\,e_2 \mapsto e_1'\,e_2 \mathstrut}
% {e_1 \mapsto e_1' \mathstrut}
% \quad
% \infer%[{\sf step/app2}]
% {v_1\,e_2 \mapsto v_1\,e_2' \mathstrut}
% {v_1\,{\sf value}
%  &
%  e_2 \mapsto e_2' \mathstrut}
% \quad
% \infer%[{\sf step/appred}]
% {(\lambda x.e)\,v_2 \mapsto [v_2/x]e \mathstrut}
% {v_2\,{\sf value}}
% \]
Early work on natural semantics emphasized a dual interpretation of
specifications. The primary interpretation of natural semantics
specifications was {\it operational}. Natural semantics were
implemented in the (non-logical) specification framework TYPOL that
compiled natural semantics specifications into Prolog
programs; the backward-chaining Prolog interpreter then gave
an operational semantics to the specification \cite{clement85natural}. 
It is also possible to view natural
semantics specifications as inductive definitions; this interpretation
allows proofs about terminating evaluations
to be performed by induction over the structure of a
natural semantics derivation \cite{clement86simple}.

The operational interpretation of natural semantics assigns a more
specific meaning to expressions than the inductive definition does.
For example, the rule ${\sf ev/app}$ as an inductive definition does
not specify whether $\obj{e_1}$ or $\obj{e_2}$ should be evaluated in
some particular order or in parallel; the TYPOL-to-Prolog compiler
could have reasonably made several choices in such a situation. More
fundamentally, the logic programming interpretation inserts semantic
information into to a natural semantics specification that is not
present when we view the specification as an inductive definition
(though it might be just as accurate to say that the logic programming
interpretation preserves meaning that is lost when the specification
is viewed as an inductive definition). The interpretation of the rules
above as an inductive definition does not allow us to distinguish
non-termination (searching forever for a $\obj{v}$ such that $\obj{e
  \Downarrow v}$) from failure (concluding finitely that there is no
$\obj{v}$ such that $\obj{e \Downarrow v}$).  The logic programming
interpreter, on the other hand, will either succeed, run forever, or
give up, thereby distinguishing two cases that are indistinguishable
when the specification is interpreted as an inductive definition.

We will present a transformation from \sls-encoded natural semantics
specifications into ordered abstract machines by a transformation
called {\it operationalization}. The basic idea of operationalization
is to model backward-chaining logic programming (in the sense of
Section~\ref{sec:framework-logicprog-deductive}) as forward-chaining
logic programming (in the sense of
Section~\ref{sec:framework-logicprog-trace}). The transformation
reifies and exposes the internal structure of backward-chaining
search, making evaluation order and parallelism explicit. That exposed
structure, in turn, enables us to reason about the difference between
non-termination and failure. In turn, ordered abstract machine
specifications can be transformed into destination-passing
specifications by a transformation called {\it destination-adding},
which reifies and exposes control flow information that is implicit in
the ordered context of an ordered abstract machine.
Destination-passing specifications can then be transformed into a
collecting semantics by {\it approximation}, which lets us obtain
program analyses like control flow analysis. The operationalization
and destination-adding transformations have been implemented within
the \sls~prototype.\robnote{Finish implement destination-adding or
  don't say that you did.} Approximation, on the other hand, requires
significant input from the user and so is less reasonable to implement
as an automatic transformation.


\begin{figure}[t]
\begin{center}
\begin{tikzpicture}
\draw (1.75,5.8) node{Natural semantics};
\draw[thick](-.5cm,4cm) -- (-.5cm,5.5cm) -- (4cm,5.5cm) 
  -- (4cm,4cm) -- (-.5cm,4cm);
%
\draw (11.75,-.7) node{Program analyses};
\draw[thick](9.5cm,-1cm) -- (9.5cm,-2.5cm) -- (14cm,-2.5cm) 
  -- (14cm,-1cm) -- (9.5cm,-1cm);
%
\draw[thick](0cm,0cm) -- (0cm,3cm);
\draw (2.25,3.7) node{Ordered};
\draw (2.25,3.3) node{abstract machines};
%
\draw[thick](4.5cm,0cm) -- (4.5cm,3cm);
\draw (9,3.7) node{Destination-passing};
\draw (6.75,3.3) node{\it (linear continuation)};
%
\draw[dotted](9cm,0cm) -- (9cm,3cm);
\draw (11.25,3.3) node{\it (persistent continuation)};
%
\draw[thick](13.5cm,0cm) -- (13.5cm,3cm);
%
\draw[thick](0,0) -- (13.5,0);
\draw[dotted] (0,1.5) -- (13.5,1.5);
\draw[thick](0,3) -- (13.5,3);
%
\draw (-.8,2.3) node{\it (nested)};
%
\draw (-.8,0.8) node{\it (flat)};
%
\draw[double,->] (3.9,2) -- (3.9,1);
\draw[double,->] (6.75,2) -- (6.75,1);
\draw[double,->] (9.6,2) -- (9.6,1);
\draw (6.75,1.6) node {\fboxsep=0pt\fbox{\colorbox{white}{\rule[-0.5ex]{0em}{2.5ex}\bf ~Defunctionalization~(Sec.~\ref{sec:defunctionalization})~}}};
%
\draw (1.75,5.1) node {\bf Operationalization};
\draw (1.75,4.6) node {\bf (Sec.~\ref{sec:operationalization})~};
\draw[double,->,rounded corners=2cm] (.5,4.8) -- (-1,3) -- (1.5,2.5);
%
\draw (1.6,1) node {\bf Destination-};
\draw (1.6,.5) node {\bf adding (Sec.~\ref{sec:destination-adding})};
\draw[double,->] (3.2,.6) -- (5.5,.6);
%\draw[double,->] (3.2,.6) -- (10,.6);
%\draw[double,->] (3.2,.4) -- (5.5,.4);
%
\draw[double,->,rounded corners=2cm] (9.5,.3) -- (7.5,-1) -- (11.1,-1.6);
\draw[double,->,rounded corners=2.5cm] (7.5,.3) -- (5.1,-.6) -- (11.1,-1.6);
\draw (7,-1.2) node {\fboxsep=0pt\fbox{\colorbox{white}{\rule[-0.5ex]{0em}{2.5ex}\bf ~Approximation~(Sec.~\ref{sec:abstraction})~}}};
\end{tikzpicture} 
\end{center}
\caption{Major transformations on \sls~specifications}
\label{fig:class-transform}
\end{figure}


These major transformations are presented
graphically in Figure~\ref{fig:class-transform} in terms of the
three classification styles -- natural semantics, ordered abstract machines,
and destination-passing -- discussed above.
There are many other smaller design decisions that can be made in the
creation of a substructural operational semantics, two of which are
represented in this figure. One distinction, destination-passing
with linear continuations versus persistent continuations, has to 
do with whether it is possible to return to a previous point a
programs execution structure and is discussed, along with first-class
continuations, in Section~\ref{sec:dest-continuations}. 

Another
distinction is between  {\it nested} and {\it flat}
specifications. This distinction applies to all concurrent
\sls~specifications, not just those that specify substructural
operational semantics. Flat
specifications include rewriting rules $\left( p_1 \fuse \ldots \fuse
  p_n \lefti \{ q_1 \fuse \ldots \fuse q_m \} \right)$ where the head
of the rule $\{ q_1 \fuse \ldots \fuse q_m \}$ contains only atomic
propositions. Nested \sls~specifications, on the other hand,
contain {\it rules} in the conclusions of rules; when the rule fires,
the resulting process state contains the rule. 
A rule $A^+ \lefti \{ B^+ \}$ in the context can only fire if a piece 
of the context
matching $A^+$ appears to its left, so  $\left(x{:}\susp{\sf p_1(\lf{c})}, ~
y{:}\istrue{\left( {\sf p_1(\lf{c})} \lefti \{ {\sf p_2(\lf{c})} \} \right)}\right)
\leadsto \left(z{:}\susp{\sf p_2(\lf{c})}\right)$, whereas 
$\left(y{:}\istrue{\left( {\sf p_1(\lf{c})} \lefti \{ {\sf p_2(\lf{c})} \} \right)},~
x{:}\susp{\sf p_1(\lf{c})}\right)
\not \leadsto$. Another example of the evolution of a 
process state with nested rules is given 
in Figure~\ref{fig:ho-evo-ex}.
%
The choice of nested versus flat specification does not impact
expressiveness, but it does influence our ability to read
specifications (opinions differ as to which style is clearer), as well
as our ability to reason about specifications. The methodology of
describing the invariants of substructural logical specifications with
{\it generative signatures}, which we introduced in
Section~\ref{sec:sls-adequate} and which we will consider further in
Chapter~\ref{chapter-gen}, 
seems better-adapted to describing the invariants of flat
specifications.

\begin{figure}[t]
{\small \begin{align*}
x_1{:}\istrue{\susp{{\sf p_2}(\lf{\sf c})}}, ~
x_2{:}\istrue{\susp{{\sf p_1}(\lf{\sf c})}}, ~
x_3{:}\istrue{(\forall \lf{x}.\,{\sf p_1}(\lf{x}) 
                \lefti \{ {\sf p_2}(\lf{x}) \lefti \{ {\sf p_3}(\lf{x}) \} \})}, ~
x_4{:}\istrue{({\sf p_3}(\lf{\sf c}) \lefti \{ {\sf p_4} \})} & \\
\leadsto ~~ 
x_1{:}\istrue{\susp{{\sf p_2}(\lf{\sf c})}}, ~
x_5{:}\istrue{({\sf p_2}(\lf{\sf c}) \lefti \{ {\sf p_3}(\lf{\sf c}) \})}, ~
x_4{:}\istrue{({\sf p_3}(\lf{\sf c}) \lefti \{ {\sf p_4} \})} & \\
\leadsto ~~ 
x_6{:}\istrue{\susp{{\sf p_3}(\lf{\sf c})}}, ~
x_4{:}\istrue{({\sf p_3}(\lf{\sf c}) \lefti \{ {\sf p_4} \})} & \\
\leadsto ~~ 
x_7{:}\istrue{\susp{{\sf p_4}}} & 
\end{align*}}\vspace{-18pt}
\caption{Evolution of a nested \sls~process state}
\label{fig:ho-evo-ex}
\end{figure}

Other distinctions between SSOS specifications can be understood in
terms of nondeterministic choices that can be made by the various
transformations we consider. For example, the operationalization
transformation can produce ordered abstract machines that evaluate
subcomputations in parallel or in sequence. In general, one source
specification (a natural semantics or an ordered abstract machine
specification) can give rise to several different target
specifications (ordered abstract machine specifications or
destination-passing specifications). The correctness of the
transformation then acts as a simple proof of the equivalence of the
several target specifications. (The prototype implementations of these
transformations only do one thing, but the nondeterministic
transformations we prove correct would justify giving the user a set
of additional controls -- for instance, the user could make the
operationalization transformation be tail-call-optimizing or
not and parallelism-enabling or not.)

The nondeterministic choices that transformations can make give us a
rigorous vocabulary for describing choices that otherwise seem
ad hoc. An example of this can be found in the paper that introduced
the destination-adding and approximation transformations
\cite{simmons11logical}. In that article, we had to motivate an ad-hoc
change to the usual abstract machine semantics. In this thesis, by the
time we encounter a similar specification in Chapter~\ref{chapter-approx}, 
we will be
able to see that this change corresponds to omitting tail-recursion
optimization in the process of operationalization.

Our taxonomy does a good job of capturing the scope of existing work
on SSOS specifications.  Figure~\ref{fig:class-prevwork} shows
previous published work on SSOS specifications mapped onto a version
of the diagram from Figure~\ref{fig:class-transform}.  With the
possible exception of certain aspects of the SSOS presentation in
Pfenning's course notes \cite{pfenning12substructural}, the taxonomy
described above captures the scope of previous work.


\begin{figure}[ht]
\begin{center}
\begin{tikzpicture} 
\draw[thick](0cm,0cm) -- (0cm,3cm);
\draw (2.25,3.7) node{Ordered};
\draw (2.25,3.3) node{abstract machines};
%
\draw[thick](4.5cm,0cm) -- (4.5cm,3cm);
\draw (9,3.7) node{Destination-passing};
\draw (6.75,3.3) node{\it (linear continuation)};
%
\draw[dotted](9cm,0cm) -- (9cm,3cm);
\draw (11.25,3.3) node{\it (persistent continuation)};
%
\draw[thick](13.5cm,0cm) -- (13.5cm,3cm);
%
\draw[thick](0,0) -- (13.5,0);
\draw[dotted] (0,1.5) -- (13.5,1.5);
\draw[thick](0,3) -- (13.5,3);
%
\draw (-.8,2.3) node{\it (nested)};
%
\draw (-.8,0.8) node{\it (flat)};
%
\draw (3,1.2) node{\cite{pfenning04substructural}};
\draw (3,.75) node{\cite{pfenning09substructural}};
\draw (3,.3) node{\cite{simmons11logical}};
\draw (3,.3) node{\cite{simmons11logical}};
\draw (7.8,.3) node{\cite{pfenning12substructural}};
\draw (6.4,2.5) node{\cite{cervesato02concurrent}};
\draw (6.4,2.05) node{\cite{schacknielsen07induction}};
\draw (3,-.4) node{\bf Increasing expressiveness};
%
\pgfsetarrowsstart{latex} 
\pgfsetlinewidth{.3pt} 
\pgfusepath{stroke} 
\draw (1.2,1.2) -- (2.2,1.2);
\draw (10,1.2) -- (3.8,1.2);
\draw (1.2,.75) -- (2.2,.75);
\draw (10,.75) -- (3.8,.75);
\draw (1.2,.3) -- (2.2,.3);
\draw (6,.3) -- (3.8,.3);
\draw[rounded corners=4pt] (6.4,-.2) -- (6.8,.3) -- (7,.3);
\draw (8.9,.3) -- (8.6,.3);
\draw (4.7,2.5) -- (5.35,2.5);
\draw (4.7,2.05) -- (5.7,2.05);
\draw (8.1,2.5) -- (7.45,2.5);
\draw (8.1,2.05) -- (7.1,2.05);
%
\draw (12,-.4) -- (5.5,-.4);
\end{tikzpicture} 
\end{center}
\caption{Classification of existing work on SSOS specifications}
\label{fig:class-prevwork}
\end{figure}





\section{Related work}
\label{sec:correspondence-related}

This part of the thesis draws from many different sources of
inspiration. In this section, we survey this related work and, where
applicable, outline how our use of logical correspondence differs from
existing work.

\subsection*{Partiality in deductive computation}

The genesis of the operationalization transformation discussed in
Chapter~\ref{chapter-absmachine} 
can be found in the treatment of the operational semantics
of LF in Tom Murphy VII's thesis~\cite{murphy08modal}; this treatment
can be seen as a synthesis of the operational interpretation of
natural semantics explored in Cl\'ement's et al.'s early work on
natural semantics in TYPOL and the approach to theorem proving
pioneered by Twelf~\cite{pfenning99system}.

In his thesis, Murphy described a natural semantics for Lambda 5, a
distributed programming language, and encoded that specification in
Twelf. He then wanted to interpret that natural semantics as an {\it
  operational} semantics for Lambda 5 in the style of Cl\'ement et
al., which is a natural application of Twelf's logic program
interpretation \cite{michaylov91natural}.  However, Murphy also wanted
to prove a safety property for his language in Twelf, and the usual
approach to theorem proving in Twelf involves treating specifications
as inductive definitions. As discussed above, natural semantics do not
distinguish non-termination (which is safe) from failure (which
indicates underspecification and is therefore unsafe).

Theorem proving in Twelf involves interpreting proofs as backward
chaining logic programs that do not backtrack (recall that we called
this the {\it flat resolution} interpretation in
Section~\ref{sec:framework-logicprog-deductive}).  Murphy was able to
use the checks Twelf performs on proofs to describe a special purpose
partiality directive. If a logic program passed his series of checks,
Murphy could conclude that well-moded, flat resolution would
never fail and never backtrack, though it might diverge. This check
amounted to a proof of safety (progress and preservation) for the
operational interpretation of his natural semantics via flat
resolution. It seems that every other existing proof of
safety\footnote{Progress in particular is the theorem of concern:
  proving preservation for a big-step operational semantics is
  straightforward.} for a big-step operational semantics is either
classical (like Leroy and Grall's approach, described below) or else
depends on a separate proof of equivalence with a small-step
operational semantics.

Murphy's proof only works because his formulation of Lambda 5 was {\it
  intrinsically typed}, meaning that, using the facilities provided by
LF's dependent types, he enforced that only well-typed terms could
possibly be evaluated. (His general proof technique should apply more
generally, but it would take much more work to express the check in
Twelf.)  The operationalization transformation is a way to
automatically derive a correct small-step semantics from the big-step
semantics by making the internal structure of a backward chaining
computation explicit as a specification in the concurrent fragment of
\sls. Having made this structure accessible, we can explicitly
represent complete, unfinished, and stuck (or failing) computations as
concurrent traces and reason about these traces with a richer set of
tools than the limited set Murphy successfully utilized.

\subsection*{A coinductive interpretation}

Murphy proved safety for a natural semantics specification by
recovering the original operational interpretation of natural semantics
specifications as logic programs and then using Twelf's facilities for
reasoning about logic programs.  Leroy and Grall, in
\cite{leroy09coinductive}, suggest a novel {\it coinductive}
interpretation of natural semantics specifications. Coevaluation $\obj{e
\Downarrow^{\sf co} v}$ is defined as the {\it greatest} fixed point of
the following rules:
\[
\infer%[{\sf evco/lam}]
{\obj{\lambda x. e \Downarrow^{\sf co} \lambda x. e} \mathstrut}
{}
\quad
\infer%[{\sf evco/app}]
{\obj{e_1\,e_2 \Downarrow^{\sf co} v} \mathstrut}
{\obj{e_1 \Downarrow^{\sf co} \lambda x.e}
 &
 \obj{e_2 \Downarrow^{\sf co} v_2}
 &
 \obj{[v_2/x]e \Downarrow^{\sf co} v} \mathstrut}
\]
Aside from the $\obj{\sf co}$ annotation and the different interpretation,
these rules are syntactically identical to the natural semantics above
that were implicitly given an inductive interpretation.

Directly reinterpreting the inductive specification as a coinductive
specification doesn't quite produce the right result in the end. For
some diverging terms like $\obj{\omega} = \obj{(\lambda x.\,x\,x)\,(\lambda
x.\,x\,x)}$, we can derive $\obj{\omega \Downarrow^{\sf co} e}$ for any
expression $\obj{e}$, including expressions that are not values and
expressions with no relation to the original term. Conversely, there
are diverging terms $\obj{\it Div}$ such that $\obj{{\it Div} \Downarrow^{\sf
  co} e}$ is not derivable for {\it any} $\obj{e}$.\footnote{Leroy and Grall
  discuss a counterexample due to Filinski: $\obj{{\it Div}} = \obj{{\it Y}{\it
    F}x}$, where $\obj{\it Y}$ is the fixed-point combinator $\obj{\lambda
  f.\,(\lambda x.\,f\,(\lambda v.\,(x\,x)\,v))\,(\lambda
  x.\,f\,(\lambda v.\,(x\,x)\,v))}$ and $\obj{\it F}$ is 
  $\obj{\lambda f.\,\lambda
  x.\,(\lambda g.\,\lambda y.\,g\,y)\,(f\,x)}$
  \cite{leroy09coinductive}.} As a result, Leroy and Grall also give a
coinductive definition of diverging terms $\obj{e \Downarrow^\infty}$ that
references the inductively-defined evaluation judgment 
$\obj{e \Downarrow v}$:
\[
\infer%[{\sf div/app1}]
{\obj{e_1\,e_2 \Downarrow^\infty} \mathstrut}
{\obj{e_1 \Downarrow^\infty} \mathstrut}
\quad
\infer%[{\sf div/app2}]
{\obj{e_1\,e_2 \Downarrow^\infty} \mathstrut}
{\obj{e_1 \Downarrow v_1}
 & 
 \obj{e_2 \Downarrow^\infty} \mathstrut}
\quad
\infer%[{\sf div/app3}]
{\obj{e_1\,e_2 \Downarrow^\infty} \mathstrut}
{\obj{e_1 \Downarrow \lambda x.\,e}
 & 
 \obj{e_2 \Downarrow v_2}
 &
 \obj{[v_2/x]e \Downarrow^\infty} \mathstrut}
\]
Now diverging expressions are fully characterized as derivations for
which $\obj{e \Downarrow^\infty}$ is derivable with an infinite derivation
tree. With this definition, Leroy and Grall prove a type safety
property: if $\obj{e}$ has type $\obj{\tau}$, then either 
$\obj{e \Downarrow v}$ or $\obj{e
\Downarrow^{\infty}}$.  However, the disjunctive character of this
theorem means that a constructive proof of type safety would be
required to take a typing derivation $\obj{e : \tau}$ as input and produce
as output either a proof of termination $\obj{e \Downarrow v}$ or a proof of
divergence $\obj{e \Downarrow^\infty}$. This implies that a constructive
type safety theorem would need to decide termination, and so it is
unsurprising that type safety is proved classically by Leroy and
Grall.

We suggest that the operationalization transformation, seen as a
logical extension to Murphy's methodology, is superior to the
coinductive (re)interpretation as a way of understanding the behavior
of diverging evaluations in the natural semantics. Both approaches
reinterpret natural semantics in an operational way, but the
operationalization transformation gives us a satisfactory treatment of
diverging terms without requiring the definition of an additional
coinductive judgment $\obj{e \Downarrow^\infty}$. And even {\it with} the
addition of the coinductively defined judgment $\obj{e \Downarrow^\infty}$,
coinductive big-step operational semantics have significant issues
handling nondeterministic languages, a point that we will elaborate on
in Section~\ref{sec:absmachine-nondeterminism}.

\subsection*{The functional correspondence}

The ordered abstract machine that results from our operationalization
transformation corresponds to a standard abstract machine model (a
statement that is made precise
Section~\ref{sec:nat-ssos-adequacy}). In this sense, the logical
correspondence has a great deal in common with the {\it functional
  correspondence} of Ager, Danvy, Midtgaard, and
others~\cite{ager03functional,ager04functional,ager05functional,
  danvy08defunctionalized,danvy12interderiving}. 

The goal of the functional correspondence is to encode various styles
of semantic specifications (natural semantics, abstract machines,
small-step structural operational semantics, environment semantics,
etc.)~as functional programs. It is then possible to show that these
styles can be related by off-the-shelf and fully correct
transformations on functional programs. The largest essential
difference between the functional and logical correspondences, then,
is that the functional correspondence acts on functional programs,
whereas the logical correspondence acts on specifications encoded in a
logical framework (in our case, the logical framework \sls).

The functional correspondence as given assumes that semantic
specifications are adequately represented as functional programs; the
equivalence of the encoding and the ``on paper'' semantics is an
assumed prerequisite. In contrast, by basing the logical
correspondence upon the \sls~framework, we make it possible to reason
formally and precisely about adequate representation by the
methodology outlined in Section~\ref{sec:sls-adequate}. The functional
correspondence also shares some of the coinductive reinterpretation's
difficulties in dealing with nondeterministic and parallel execution.
The tools we can use to express the semantics are heavily
influenced by the semantics of the host programming language, and so
the specifics of the host language can make it dramatically more or
less convenient to encode nondeterministic or parallel programming
language features.

\subsection*{Transformation on specifications}

Two papers by Hannan and Miller \cite{hannan92operational} and Ager
\cite{ager04natural} are the most closely related to our
operationalization transformation. Both papers propose
operationalizing natural semantics specifications as abstract machines
by provably correct and general transformations on logical
specifications (in the case of Hannan and Miller) or on specifications
in the special-purpose framework of L-attributed natural semantics (in
the case of Ager).  A major difference in this case is that both lines
of work result in {\it deductive} specifications of abstract
machines. Our translation into the concurrent fragment of \sls~has the
advantage of exploiting parallelism, and also opens up specifications
to the modular inclusion of stateful and concurrent features, as we
will foreshadow in Section~\ref{sec:the-point-is-modular-extension}
below and discuss further in
Section~\ref{sec:richer-ordered-abstract}.

The transformation we call defunctionalization in
Section~\ref{sec:defunctionalization}, as well as its inverse,
refunctionalization, makes appearances throughout the literature under
various names. The transformation is not strictly analogous to
Reynold's defunctionalization transformations on functional programs
\cite{reynolds72definitional}, but it is based upon the same idea: we
take an independently transitioning object like a function (or, in our
case, negative propositions in the process state) and turn it into
data and an application function.  In our case, the data is a positive
atomic proposition in the process state and the application function
is a rule in the signature that explains how the positive atomic
proposition can participate in transitions.  The role of
defunctionalization within our work on the logical correspondence is
very similar to the role of Reynold's defunctionalization within work
on the functional correspondence
\cite{danvy08defunctionalized}. Defunctionalization is related to the
process of representing a process calculus object in the chemical
abstract machine \cite{berry90chemical}.  It is also related to a
transformation discussed by Miller in~\cite{miller02higherorder} in
which new propositions are introduced and existentially quantified
locally in order to hide the internal states of processes.

The destination-adding transformation described in
Section~\ref{sec:destination-adding} closely follows the contours of
work by Morrill, Moot, and Piazza on translating ordered logic into
linear logic \cite{morrill95higher,moot01linguistic}. That work is, in
turn, based on van Benthem's relational models of ordered logic
\cite{vanbenthem91relational}. Their transformations handle a more
uniform logical fragment, whereas the transformation we describe
handles a specific (though useful) fragment of the much richer logic
of \sls~propositions.

Related work for program analyses methodology covered in 
Chapter~\ref{chapter-approx} is
discussed further in Section~\ref{sec:approximately-related}.

\section{Transformation and modular extension}
\label{sec:the-point-is-modular-extension}

\begin{figure}
\begin{center}
\begin{tikzpicture}
%\draw[thick](-4.5cm,0cm) -- (-4.5cm,3cm);
\draw (-2.25,3.3) node{Natural semantics};
%
\draw (-2.25,2.4) node{Call-by-value functions};
\draw[double,->] (-.15,2.4) -- (6.5,2.4);
\draw (-2.25,1.7) node{Numbers \& base types};
\draw[double,->] (-.15,1.7) -- (6.5,1.7);
\draw (-2.25,1) node{Recursion};
\draw[double,->] (-.15,1) -- (6.5,1);
%
\draw[dotted](0cm,-4.6cm) -- (0cm,3cm);
\draw (2.25,3.7) node{Ordered};
\draw (2.25,3.3) node{abstract machines};
%
\draw (2.25,.3) node{Mutable storage};
\draw[double,->] (4.35,.3) -- (6.5,.3);
\draw (2.25,-.4) node{Call-by-need};
\draw (2.25,-.8) node{suspensions};
\draw[double,->] (4.35,-.6) -- (6.5,-.6);
\draw (2.25,-1.5) node{Recoverable failure};
\draw[double,->] (4.35,-1.5) -- (6.5,-1.5);
%
\draw[thick](4.5cm,-4.6cm) -- (4.5cm,3cm);
\draw (6.75,3.3) node{Destination-passing};
%
\draw (6.75,-2.2) node{Parallel pairs};
\draw (6.75,-2.9) node{Interaction of};
\draw (6.75,-3.3) node{parallelism \& failure};
\draw (6.75,-4) node{Synchronization};
%
\draw[thick](9cm,-4.6cm) -- (9cm,3cm);
%
%\draw[thick](13.5cm,0cm) -- (13.5cm,3cm);
%
%\draw[thick](-4.5,0) -- (9,0);
%\draw[dotted] (-4.5,1.5) -- (13.5,1.5);
\draw[dotted](-4.5,3) -- (4.5,3);
\draw[thick](4.5,3) -- (9,3);
\draw[thick](4.5,-4.6) -- (9,-4.6);
\draw (6.75, -5.2) node{\bf Common Spec};
\draw[->,thick,rounded corners=.45cm] (5.3, -5.2) -- (4.8, -5.2) -- (4.8,-4.7);
\draw[->,thick,rounded corners=.45cm] (8.2, -5.2) -- (8.7, -5.2) -- (8.7,-4.7);
\end{tikzpicture} 
\end{center}
\caption{Using the logical correspondence for modular language extension}
\label{fig:modularcompose}
\end{figure}

All the related work described in the previous section is concerned with {\it
  correspondence}. That is, the authors were interested in the process
of transforming natural semantics into abstract machines and in the
study of abstract machines that are in the image of this
translation. It is possible to view the logical correspondence in the
same light, but that is not how logical correspondence will be used in
this thesis.  Indeed, it is not our intent to advocate strongly for the
use of natural semantics specifications at all; recall that natural
semantics were used to illustrate problems with {\it non}-modularity
in language specification in Section~\ref{sec:intro-ssos}.

Instead, we will view the transformations illustrated as arrows in
Figure~\ref{fig:class-transform} in an expressly directed fashion,
operationalizing natural semantics as ordered abstract machines and
transforming ordered abstract machines into destination-passing
semantics without giving too much thought to the opposite
direction. In the context of this thesis, the reason that
transformations are important is that they {\it expose more of the
  semantics to manipulation and modular extension}.  The
operationalization transformation in Chapter~\ref{chapter-destinations} 
exposes the order of
evaluation, and the \sls~framework then makes it possible to modularly
extend the language with stateful features: this is exactly what we
demonstrated in Section~\ref{sec:intro-ssos} and will
demonstrate again in Section~\ref{sec:richer-ordered-abstract}.  The
destination-adding transformation exposes the control structure of
programs; this makes it possible to discuss first-class continuations
as well as the interaction of parallelism and failure (though not
necessarily at the same time, as discussed in
Section~\ref{sec:dest-continuations}).  The control structure exposed
by the destination-adding transformation is the basis of the control
flow analysis in Chapter~\ref{chapter-approx}.
% This is also what
% ultimately distinguishes this work from the work of Hannan, Miller,
% and Ager, as the abstract machine semantics they derive from natural
% semantics are not amenable to modular extension and abstract
% interpretation.

In the next three chapters that make up Part~2 of this thesis, we will
present natural semantics specifications and substructural operational
semantics specifications in a number of styles. We do so with the
confidence that these specifications can be automatically transformed
into the ``lowest common denominator'' of flat destination-passing
specifications.  Certainly, this means that we should be unconcerned
about using a higher-level style such as the ordered abstract machine
semantics, or even natural semantics, when that seems appropriate. If
we need the richer structure of destination-passing semantics later
on, the specification can be automatically transformed.  Using the
original, high-level specifications, the composition of different
language features may appear to be a tedious or error-prone process of
revision, but after transformation into the lowest-common-denominator
specification style, composition can be performed by simply
concatenating the specifications.

Taking this idea to its logical conclusion,
Appendix~\ref{chapter-appendix-hybrid} presents the hybrid operational
semantics specification mapped out in
Figure~\ref{fig:modularcompose}. Individual language features are
specified at the highest-level specification style that is reasonable
and then automatically transformed into a single compatible
specification by the transformations implemented in the
\sls~prototype.\robnote{Implement destination-adding or backtrack on
  this statement.} In such a specification, a change to a high-level
feature (turning call-by-value functions to call-by-name functions,
for instance) can be made at the level of natural semantics and
then propagated by transformation into the common (destination-passing
style) specification.



