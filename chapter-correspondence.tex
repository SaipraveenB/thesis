\chapter{On logical correspondence}

In Part 1, we defined \sls, a logical framework of substructural
logical specifications. For the purposes of this thesis, we are
primarily interested in using \sls~as a framework for specifying the
operational semantics of programming languages, especially stateful
and concurrent programming languages. This is not a new idea: one of
the original case studies on CLF specification described the semantics
of Concurrent ML \cite{cervesato02concurrent} in a specification style
termed {\it substructural operational semantics}, or SSOS, by Pfenning
\cite{pfenning04substructural}. The general idea of representing the
intermediate states of a computation as contexts in substructural
logic dates back to Miller \cite{miller92pi} and his Ph.D. student
Chirimar \cite{chirimar95proof}, who encoded the intermediate states
of a $\pi$-calculus and of a low-level RISC machine (respectively) as
contexts in focused classical linear logic.

However, the design space of substructural operational semantics is
extremely rich. In order to make sense of the design space, it is
helpful to have design principles that allow us to both {\it classify}
different styles of presentation and {\it predict} what style(s) we
should adopt based on what our goals are. In this thesis, I propose a
classification scheme for substructural operational semantics based on
the consideration of three major specification styles.  Each of these
styles is more expressive than the last.

\begin{itemize}
\item The {\it natural semantics}, or big-step operational semantics,
  is an existing and well-known specification style (and not a
  substructural operational semantics). It is convenient for the
  specification of pure programming languages.

\item The {\it ordered abstract machine semantics} is a generalization
  of abstract machine semantics that can be naturally specified in
  \sls; this specification style naturally handles stateful and
  parallel programming language features
  \cite{pfenning09substructural}.

\item The {\it destination-passing semantics} is the style of
  substructural operational semantics first explored in CLF by
  Cervesato et al.~\cite{cervesato02concurrent}. It allows for the
  natural specification of features that incorporate synchronous
  communication and non-local transfer of control. Destination-passing
  semantics are classified into two subspecies: those with linear
  continuations and those with persistent continuations. Persistent
  continuations are necessary to give a substructural operational
  semantics to languages with first-class continuations.
\end{itemize}


\begin{figure}
\begin{center}
\begin{tikzpicture} 
\draw[thick](0cm,0cm) -- (0cm,3cm);
\draw (2.25,3.7) node{Ordered};
\draw (2.25,3.3) node{abstract machines};
%
\draw[thick](4.5cm,0cm) -- (4.5cm,3cm);
\draw (9,3.7) node{Destination-passing};
\draw (6.75,3.3) node{\it (linear continuation)};
%
\draw[dotted](9cm,0cm) -- (9cm,3cm);
\draw (11.25,3.3) node{\it (persistent continuation)};
%
\draw[thick](13.5cm,0cm) -- (13.5cm,3cm);
%
\draw[thick](0,0) -- (13.5,0);
\draw[dotted] (0,1.5) -- (13.5,1.5);
\draw[thick](0,3) -- (13.5,3);
%
\draw (-.8,2.45) node{\it (higher-};
\draw (-.8,2.05) node{\it order)};
%
\draw (-.8,0.95) node{\it (first-};
\draw (-.8,0.55) node{\it order)};
%
\draw (3,1.2) node{\cite{pfenning04substructural}};
\draw (3,.75) node{\cite{pfenning09substructural}};
\draw (3,.3) node{\cite{simmons11logical}};
\draw (3,.3) node{\cite{simmons11logical}};
\draw (7.8,.3) node{\cite{pfenning12substructural}};
\draw (6.4,2.5) node{\cite{cervesato02concurrent}};
\draw (6.4,2.05) node{\cite{schacknielsen07induction}};
\draw (3,-.4) node{\bf Increasing expressiveness};
%
\pgfsetarrowsstart{latex} 
\pgfsetlinewidth{.3pt} 
\pgfusepath{stroke} 
\draw (1.2,1.2) -- (2.2,1.2);
\draw (10,1.2) -- (3.8,1.2);
\draw (1.2,.75) -- (2.2,.75);
\draw (10,.75) -- (3.8,.75);
\draw (1.2,.3) -- (2.2,.3);
\draw (6,.3) -- (3.8,.3);
\draw[rounded corners=4pt] (6.4,-.2) -- (6.8,.3) -- (7,.3);
\draw (8.9,.3) -- (8.6,.3);
\draw (4.7,2.5) -- (5.35,2.5);
\draw (4.7,2.05) -- (5.7,2.05);
\draw (8.1,2.5) -- (7.45,2.5);
\draw (8.1,2.05) -- (7.1,2.05);
%
\draw (12,-.4) -- (5.5,-.4);
\end{tikzpicture} 
\end{center}
\caption{Classification of existing work on SSOS specifications.}
\label{fig:class-prevwork}
\end{figure}

This classification of substructural operational semantics is
illustrated graphically in Figure~\ref{fig:class-prevwork}. Existing
published work on substructural operational semantics specifications
is placed in the figure, with arrows indicating the range of styles
that are considered. With the possible exception of certain aspects of
the SSOS presentation in Pfenning's course notes
\cite{pfenning12substructural}, the taxonomy described above neatly
captures previous work. 

The statement that each specification style is strictly more
expressive than the last is formal: there are automatic and
provably-correct transformations from the expressive styles (natural
semantics and ordered abstract machines) to the more expressive
formalisms (ordered abstract machines and destination-passing).  The
investigation of provably-correct transformations on
\sls~specifications is therefore the means by which we classify of
SSOS semantics. We call this methodology the {\it logical
  correspondence}, and it is the focus of this portion of the thesis,
which justifies the following argument:

\begin{quote} 
  {\bf Thesis (part 2):} A logical framework based on forward
  reasoning in substructural logic supports many styles of programming
  language specification. These styles can be formally classified and
  connected by considering general transformations on logical
  specifications. 
 
  Generally applicable transformations on logical
  specifications are also useful for deriving manifestly correct
  program analyses from those operational semantics specifications.
\end{quote} 

\noindent
In this introductory chapter, we will outline our use of logical
correspondence and connect it to previous work. The development of the
logical correspondence as presented in this chapter, and the
operationalization and defunctionalization transformations presented
in the next chapter, represent joint work with Ian Zerny.

\section{Logical correspondence}

As stated above, we will primarily discuss and connect three different
styles that are used specifying the operational semantics of
programming languages. Natural semantics is a high-level, declarative
style of specification. For illustration, the call-by-value natural
semantics for the untyped lambda calculus is comprised of two rules:
\[
\infer[{\sf ev/lam}]
{\lambda x. e \Downarrow \lambda x. e \mathstrut}
{}
\quad
\infer[{\sf ev/app}]
{e_1\,e_2 \Downarrow v \mathstrut}
{e_1 \Downarrow \lambda x.e
 &
 e_2 \Downarrow v_2
 &
 [v_2/x]e \Downarrow v \mathstrut}
\]
This inductive definition assigns meaning to all the terminating
expressions in the lambda calculus. However, natural semantics are not
{\it operational} semantics, except in the very loose sense that they
are {\it moded} in the sense of Section~\ref{sec:framework-modes}: we
can think of the $e$ in $e \Downarrow v$ as being an input and the $v$
as being output. The order of evaluation is unspecified, however.  In
rule ${\sf ev/app}$, the subexpressions $e_1$ and $e_2$ can be
evaluated to values in either order and can even be evaluated in
parallel.

We turn natural semantics into ordered abstract machines by
a transformation called {\it operationalization}, and turn ordered
abstract machine specifications into destination-passing
specifications by a transformation called {\it destination-adding}.
Destination-passing specifications can then be transformed into a
collecting semantics by the simple transformation of {\it
  abstraction}, after which they can be further abstracted to obtain
program analyses like control flow analysis. These major
transformations are presented graphically in
Figure~\ref{fig:class-transform}.

\begin{figure}
\begin{center}
\begin{tikzpicture}
\draw (1.75,5.8) node{Natural semantics};
\draw[thick](-.5cm,4cm) -- (-.5cm,5.5cm) -- (4cm,5.5cm) 
  -- (4cm,4cm) -- (-.5cm,4cm);
%
\draw (11.75,-.7) node{Program analyses};
\draw[thick](9.5cm,-1cm) -- (9.5cm,-2.5cm) -- (14cm,-2.5cm) 
  -- (14cm,-1cm) -- (9.5cm,-1cm);
%
\draw[thick](0cm,0cm) -- (0cm,3cm);
\draw (2.25,3.7) node{Ordered};
\draw (2.25,3.3) node{abstract machines};
%
\draw[thick](4.5cm,0cm) -- (4.5cm,3cm);
\draw (9,3.7) node{Destination-passing};
\draw (6.75,3.3) node{\it (linear continuation)};
%
\draw[dotted](9cm,0cm) -- (9cm,3cm);
\draw (11.25,3.3) node{\it (persistent continuation)};
%
\draw[thick](13.5cm,0cm) -- (13.5cm,3cm);
%
\draw[thick](0,0) -- (13.5,0);
\draw[dotted] (0,1.5) -- (13.5,1.5);
\draw[thick](0,3) -- (13.5,3);
%
\draw (-.8,2.45) node{\it (higher-};
\draw (-.8,2.05) node{\it order)};
%
\draw (-.8,0.95) node{\it (first-};
\draw (-.8,0.55) node{\it order)};
%
\draw[double,->] (3.9,2) -- (3.9,1);
\draw[double,->] (6.75,2) -- (6.75,1);
\draw[double,->] (9.6,2) -- (9.6,1);
\draw (6.75,1.6) node {\fboxsep=0pt\fbox{\colorbox{white}{\rule[-0.5ex]{0em}{2.5ex}\bf ~Defunctionalization~(Sec.~\ref{sec:defunctionalization})~}}};
%
\draw (1.75,5.1) node {\bf Operationalization};
\draw (1.75,4.6) node {\bf (Sec.~\ref{sec:operationalization})~};
\draw[double,->,rounded corners=2cm] (.5,4.8) -- (-1,3) -- (1.5,2.5);
%
\draw (1.6,1) node {\bf Destination-};
\draw (1.6,.5) node {\bf adding (Sec.~\ref{sec:destination-adding})};
\draw[double,->] (3.2,.6) -- (10,.6);
\draw[double,->] (3.2,.4) -- (5.5,.4);
%
\draw[double,->,rounded corners=2cm] (9.5,.3) -- (7.5,-1) -- (11.1,-1.6);
\draw[double,->,rounded corners=2.5cm] (7.5,.3) -- (5.1,-.6) -- (11.1,-1.6);
\draw (7,-1.2) node {\fboxsep=0pt\fbox{\colorbox{white}{\rule[-0.5ex]{0em}{2.5ex}\bf ~Abstraction~(Sec.~\ref{sec:abstraction})~}}};
\end{tikzpicture} 
\end{center}
\caption{Major transformations on \sls~specifications.}
\label{fig:class-transform}
\end{figure}

There are many other smaller design decisions that can be made in the
creation of a substructural operational semantics. Only one is
graphically represented in
Figures~\ref{fig:class-prevwork}~and~\ref{fig:class-transform}, the
distinction between {\it higher-order} or {\it first-order}
specifications. This distinction applies to all concurrent
\sls~specifications, not just those that specify substructural
operational semantics. First-order
specifications are like rewriting rules $\left( p_1 \fuse \ldots \fuse
  p_n \lefti \{ q_1 \fuse \ldots \fuse q_m \} \right)$, where the head
of the rule $\{ q_1 \fuse \ldots \fuse q_m \}$ contains only atomic
propositions. Higher-order \sls~specifications, on the other hand,
contain {\it rules} in the conclusions of rules; when the rule fires,
the resulting process state contains the rule. An example is given 
in Figure~\ref{fig:ho-evo-ex}.
Order matters in higher-order process states: $\left(x{:}\susp{\sf p1(c)}, ~
y{:}\istrue{\left( {\sf p1(c)} \lefti \{ {\sf p2(c)} \} \right)}\right)
\leadsto \left(z{:}\susp{\sf p2(c)}\right)$, whereas 
$\left(y{:}\istrue{\left( {\sf p1(c)} \lefti \{ {\sf p2(c)} \} \right)},~
x{:}\susp{\sf p1(c)}\right)
\not \leadsto$. 
%
The choice of higher-order versus first-order specification does not
impact expressiveness, but it does influence our ability to read
specifications (opinions differ as to which style is clearer) and 
the ways we reason about them.

\begin{figure}
\begin{align*}
x_1{:}\susp{{\sf p2}({\sf c})}, ~~
x_2{:}\susp{{\sf p1}({\sf c})}, ~~
x_3{:}\istrue{(\forall x.\,{\sf p_1}(x) 
                \lefti \{ {\sf p_2}(x) \lefti \{ {\sf p_3}(x) \} \})}, ~~
x_4{:}\istrue{({\sf p3}({\sf c}) \lefti \{ {\sf p_4} \})} & \\
\leadsto ~~~ 
x_1{:}\susp{{\sf p2}({\sf c})}, ~~
x_5{:}\istrue{({\sf p_2}({\sf c}) \lefti \{ {\sf p_3}({\sf c}) \})}, ~~
x_4{:}\istrue{({\sf p3}({\sf c}) \lefti \{ {\sf p_4} \})} & \\
\leadsto ~~~ 
x_6{:}\susp{{\sf p3}({\sf c})}, ~~
x_4{:}\istrue{({\sf p3}({\sf c}) \lefti \{ {\sf p_4} \})} & \\
\leadsto ~~~ 
x_7{:}\susp{{\sf p_4}} & 
\end{align*}
\caption{Evolution of a higher-order \sls~process state (${\sf p1}$, ${\sf
  p2}$ and ${\sf p3}$ are all ordered atomic propositions).}
\label{fig:ho-evo-ex}
\end{figure}

Other distinctions between \sls~specifications can also be understood
in terms of nondeterministic choices that can be made by the various
transformations we consider: for instance, the operationalization
transformation can produce ordered abstract machines that evaluate
subcomputations in parallel or in sequence or not, and the
destination-adding transformation can make continuations either linear
or persistent. The existence of these nondeterministic choices have two
important consequences. First, it is generally the case that one source
specification (a natural semantics or an ordered abstract machine
specification) can give rise to several different target
specifications (ordered abstract machine specifications or
destination-passing specifications). The correctness of the
transformation also acts as a simple proof of the equivalence of the
several target specifications.

The other interesting consequence of nondeterministic choices is the
different options give us a rigorous vocabulary for describing choices
that otherwise seem ad-hoc. An example of this can be found in the
paper that introduced the destination-adding and abstraction
transformations \cite{simmons11logical}. In that article, we had to
motivate an ad-hoc change to the usual abstract machine semantics. In
this thesis, by the time we encounter a similar specification in
Chapter 8, we will be able to see that this distinction corresponds to
the choice of whether or not tail-recursion optimization is performed
during proof search.

\section{Related work}

This part of the thesis document document draws from many different
sources of inspiration. In this section, we survey this related work
and, where applicable, outline how our use of logical correspondence
differs from existing work.

\subsection{Partiality in deductive computation}

The genesis of the operationalization transformation discussed in
Chapter~6 can be found in the treatment of the operational semantics
of LF in Tom Murphy VII's thesis \cite{murphy08modal}. In his thesis,
Murphy described a natural semantics for Lambda 5, a distributed
programming language, and then wanted to interpret that natural
semantics as an {\it operational} semantics for Lambda 5. As discussed
above, natural semantics are not operational. However, by interpreting
the natural semantics as a moded logic program, Murphy stipulated that
the operational semantics were precisely those given by moded, {\it
  non-backtracking} deductive proof search. Then, by modifying the
checks that Twelf performs with a special purpose partiality
directive, he was able to check that moded proof search would never
fail and never backtrack, though it might diverge. This check amounted
to a proof of safety (progress and preservation) for his natural
semantics. I believe his mechanized proof to be the only existing
non-classical proof of safety for a big-step operational semantics.

Murphy's proof only works because his formulation of Lambda 5 was {\it
  intrinsically typed}, meaning that, using the facilities provided by
LF's dependent types, he enforced that only well-typed terms could
possibly be evaluated. His approach would also work for our untyped
lambda calculus above, as closed terms in the untyped lambda calculus
can never get stuck. However, as soon as we extend the untyped lambda
calculus with some other values (numbers, Booleans, pairs, etc\ldots)
or even just with a nonsense term ${\sf junk}$, then progress and
preservation apply only to well-typed and not to arbitrary terms. In
this case, Murphy's partiality checks no longer hold, so we cannot use
his methodology to reason about safety. 

One way to look at the operationalization transformation presented in
Chapter~6 is as a way to make the internal structure of a deductive
computation explicit as a concurrent computation. Having done so, we
can explicitly represent complete, unfinished, and stuck (or failing)
computations as concurrent traces and reason about these traces with a
richer set of tools than the limited set Murphy successfully utilized.

\subsection{A coinductive interpretation}

Murphy proved safety for a natural semantics specification by
reinterpreting the inductive definition of the natural semantics as a
logic program and reasoning about the logic program. Leroy and Grall,
in \cite{leroy09coinductive}, use a different reinterpretation: they
reinterpret the inductive definition of natural semantics as 
a {\it coinductive} specification, that is, the {\it greatest} fixed point
of the following rules.
\[
\infer%[{\sf evco/lam}]
{\lambda x. e \Downarrow^{\sf co} \lambda x. e \mathstrut}
{}
\quad
\infer%[{\sf evco/app}]
{e_1\,e_2 \Downarrow^{\sf co} v \mathstrut}
{e_1 \Downarrow^{\sf co} \lambda x.e
 &
 e_2 \Downarrow^{\sf co} v_2
 &
 [v_2/x]e \Downarrow^{\sf co} v \mathstrut}
\]
Of course, aside from the ${\sf co}$ annotation and the different
interpretation, these rules are syntactically identical to the inductively
defined natural semantics above.

Directly reinterpreting the inductive specification as an inductive
specification doesn't quite produce the right result: for some
diverging terms like $\omega = (\lambda x.\,x\,x)\,(\lambda
x.\,x\,x)$, we can derive $\omega \Downarrow^{\sf co} e$ for any
expression $e$, including expressions that are not values and
expressions like ${\sf junk}$ with no relation to the original
term. Conversely, there are diverging terms ${\it Div}$ such that
${\sf div} \Downarrow^{\sf co} e$ is not derivable for {\it any}
$e$.\footnote{Leroy and Grall discuss a counterexample due to Filinski
  where ${\it Div} = {\it Y}{\it F}x$ where $\it Y$ is the fixed-point
  combinator $\lambda f.\,(\lambda x.\,f\,(\lambda
  v.\,(x\,x)\,v))\,(\lambda x.\,f\,(\lambda v.\,(x\,x)\,v))$ and $\it
  F$ is $\lambda f.\,\lambda x.\,(\lambda g.\,\lambda
  y.\,g\,y)\,(f\,x)$ \cite{leroy09coinductive}.} As a result, Leroy
and Grall also give a greatest-fixed-point definition of diverging
terms $e \Downarrow^\infty$ that is based on the least-fixed-point
specification $e \Downarrow v$.
\[
\infer%[{\sf div/app1}]
{e_1\,e_2 \Downarrow^\infty}
{e_1 \Downarrow^\infty}
\quad
\infer%[{\sf div/app2}]
{e_1\,e_2 \Downarrow^\infty}
{e_1 \Downarrow (\lambda x.\,e)
 & 
 e_2 \Downarrow^\infty}
\quad
\infer%[{\sf div/app3}]
{e_1\,e_2 \Downarrow^\infty}
{e_1 \Downarrow (\lambda x.\,e)
 & 
 e_2 \Downarrow v_2
 &
 [v_2/x]e \Downarrow^\infty}
\]
Now diverging expressions are fully characterized as derivations for
which $e \Downarrow^\infty$ is derivable by infinite derivation trees,
thereby capturing both converging and diverging executions.  With this
definition, Leroy and Grall prove a type safety property: if $e$ has
type $\tau$, then either $e \Downarrow v$ or $e \Downarrow^{\infty}$.
However, the disjunctive character of this theorem means that a
constructive proof of type safety would be required to take a typing
derivation $e : \tau$ as input and produce as output either a proof of
termination $e \Downarrow v$ or a proof of divergence $e
\Downarrow^\infty$. This seems to imply that a constructive type
safety theorem would additionally decide termination, and so it is
unsurprising that type safety is proved classically by Leroy and Grall.

We suggest that the operationalization transformation, seen as a
logical extension to Murphy's methodology, is superior to the
coinductive (re)interpretation as a way of understanding the behavior
of infinite terms in the natural semantics. Both approaches
reinterpret the logical artifact of natural semantics in an
operational way, but the operationalization transformation gives us a
satisfactory treatment of diverging terms without requiring the
definition of an additional coinductive judgment $e
\Downarrow^\infty$.

\subsection{The functional correspondence}

The ordered abstract machine that results from our operationalization
transformation corresponds to a standard abstract machine model (a
statement we will make precise in
Section~\ref{sec:nat-ssos-adequacy}). In this sense, the logical
correspondence has a great deal in common with the {\it functional
  correspondence} of Ager, Danvy, Midtgaard, and
others~\cite{ager03functional,ager04functional,ager05functional,
  danvy08defunctionalized,danvy12interderiving}. 

The goal of the functional correspondence is encode various styles of
semantic specifications (natural semantics, abstract machines,
small-step structural operational semantics, environment semantics,
etc.) as functional programs. It is then possible to show that these
styles can be related by off-the-shelf and fully correct
transformations on functional programs. The largest essential difference
between the functional and logical correspondences, then, is that
the functional correspondence acts on functional programs, whereas
the logical correspondence acts on specifications encoded in a logical
framework (in our case, the logical framework \sls). 

While the functional and logical correspondences can be 


However, the functional correspondence as given assumes that 
semantic specifications are adequately represented as functional
programs. 
%
The
%interpretations
representations and the adequacy of the encoding with respect to the ``on paper''
semantics is an assumed prerequisite in existing work. 
% tighten this next sentence up
This is potentially
problematic: the function space of a general programming language
is much more open-ended than necessary or even desirable. The correspondence
remains a useful methodology, but is not 
directly applicable to the analysis 
(particularly mechanized analysis) of semantic specifications.


\subsection{Transformation on specifications}

 This aspect of
operationalization -- taking a logical specification of a language's
natural semantics and transforming it to obtain an abstract machine --
has been explored by others, including Hannan and Miller
\cite{hannan92operational} and Ager \cite{ager04natural} have also
proposed the idea of operationalizing natural semantics specifications
as abstract machines by provably correct and general transformations
on logical specifications (in the case of Hannan and Miller) or on the
special-purpose framework of L-attributed natural semantics (in the
case of Ager). 

A major difference in this case is that both lines of
work result in {\it deductive} specifications of abstract
machines. Our translation into concurrent specifications has the
advantage of exploiting parallelism, as demonstrated in
Section~\ref{sec:trans-par} below, and also opens up specifications to
the modular inclusion of stateful and concurrent features, as we will
discuss in Section~\ref{sec:richer-ordered-abstract}.


\section{Expressiveness and modular extension}

Much of the work in the previous section concerned itself with {\it
  correspondence} -- in both the work of Ager et al.~and the work of
Hannan and Miller, two or more specifications (often natural semantics
and abstract machines) are presented and then shown to correspond
exactly. But it is not my intent to advocate strongly for the use of
natural semantics specifications; recall that natural semantics were
used to illustrate problems with {\it non}-modularity in language
specification in Section~\ref{sec:modularnonmodular}. Instead, our
use 

The functional correspondence is largely concerned with the 

\sls~is very
The potential design space of substructural operational semantics is
quite large. 

I propose that different styles of
\sls~specification can be productively classified in terms of the
transformations that turn one classification style into another. Most
transformations will not have inverses, so this methodology gives us a
formal notion of which styles are more expressive than others.
Considering logical transformations also can lower the cost of
mis-prediction. If one begins a development in an overly-restrictive
style, the development can be transformed into a more expressive style
by an automatic transformation.




