\chapter{On logical correspondence}

In Part 1, we defined \sls, a logical framework of substructural
logical specifications. For the purposes of this thesis, we are
primarily interested in using \sls~as a framework for specifying the
operational semantics of programming languages, especially stateful
and concurrent programming languages. This is not a new idea: one of
the original case studies on CLF specification described the semantics
of Concurrent ML \cite{cervesato02concurrent} in a specification style
termed {\it substructural operational semantics} by Pfenning
\cite{pfenning04substructural}. The general idea of representing the
intermediate states of a computation as contexts in substructural
logic dates back to Miller \cite{miller92pi} and his Ph.D. student
Chirimar \cite{chirimar95proof}, who encoded the intermediate states
of a $\pi$-calculus and of a low-level RISC machine (respectively) as
contexts in focused classical linear logic.

The logical framework \sls~provides an extremely rich set
of tools for specifying properties of programming languages. In this
chapter and the next chapter, we will consider three styles of
specification that can be adequately represented in the
\sls~framework; each specification style is strictly more expressive
than the last.

\begin{itemize}
\item The {\it natural semantics}, or big-step operational semantics,
  is an existing and well-known specification style that is convenient
  for the specification of pure and deterministic programming
  languages.

\item The {\it ordered abstract machine semantics} is a generalization
  of abstract machine semantics that can be naturally specified in
  \sls; this specification style naturally handles stateful and
  parallel programming language features, as well as nondeterminism
  \cite{pfenning09substructural}.

\item The {\it destination-passing semantics} is the style of
  substructural operational semantics first explored in CLF by
  Cervesato et al.~\cite{cervesato02concurrent}. It allows for the
  natural specification of features that incorporate synchronous
  communication and non-local transfer of control.
\end{itemize}

\noindent
The statement that each specification style is strictly more
expressive than the last is formal: by using fully automatic and
provably-correct transformations, natural semantics can be
mechanically transformed to ordered abstract machine semantics and
ordered abstract machine semantics can be mechanically transformed
into destination-passing semantics. These transformations are, in
turn, instances of general transformations on \sls~specifications.

In this chapter, we explore transformations on \sls~specifications
that operationalize deductive computation (backwards-chaining proof
search for propositions of negative atomic type) as concurrent
computation (forward-chaining proof search for propositions of
positive type). These transformations have the effect of mapping
natural semantics specifications into abstract machine specifications.

This chapter represents joint work with Ian Zerny.

\subsection*{Chapter outline}

Three transformations that perform operationalizion are presented in
Section~\ref{sec:operationalization}: a simple transformation, a
tail-recursion optimized transformation, and a parallel
transformation. When one of these transformations are applied to a
natural semantics encoded in \sls, the result is an ordered abstract
machine semantics encoded in SLS. In order to obtain the ordered
abstract machine semantics for the call-by-value lambda calculus that
was presented in Section~\ref{sec:intro-ssos}, we introduce a second
transformation, defunctionalization, in
Section~\ref{sec:defunctionalization}.

In Section~\ref{sec:nat-ssos-adequacy}, I take a bit of a detour to
discuss the traditional presentation of abstract machines. The ordered
abstract machine semantics presented in Section~\ref{sec:intro-ssos}
is now identifiable as the call-by-value natural semantics for the
lambda calculus after being transformed, first by the
(tail-recursion-optimizing) operationalizion transformation and second
by the defunctionalization transformation. I claim it is {\it also} a
substructural operational semantics corresponding to the a standard
call-by-value abstract machine semantics for the lambda calculus, but
making this precise requires a discussion of adequacy for
\sls~encodings. I also discuss the connections between our logical
correspondence and the {\it functional correspondence} investigated by
Ager, Danvy, Midtgaard, and others.

Section~\ref{sec:richer-ordered-abstract} gets to the heart of why we
are interested in the transformations discussed in this chapter: the
ordered abstract machine semantics specification that we get when we
operationalize a natural semantics specification is amenable to the
modular addition of features that could not be modularly integrated
into the natural semantics specification. We explore a selection of
these features, including mutable storage, call-by-need evaluation,
and recoverable failure.

Finally, in Section~\ref{sec:othertransform}, we discuss the
consequences of defining operationalization as a general
transformation on \sls~specifications rather than a special-purpose
transformation on natural semantics.

% \begin{figure}[t]
% \begin{minipage}[b]{0.45\linewidth}
% \fbox{$e \Downarrow v$}

% \bigskip

% \[
% \infer[{\sf ev/lam}]
% {\lambda x. e \Downarrow \lambda x. e \mathstrut}
% {}
% \]\smallskip
% \[
% \infer[{\sf ev/app}]
% {e_1\,e_2 \Downarrow v \mathstrut}
% {e_1 \Downarrow \lambda x.e
%  &
%  e_2 \Downarrow v_2
%  &
%  [v_2/x]e \Downarrow v \mathstrut}
% \]

% \bigskip

% \bigskip

% \[
% \infer[{\sf ev/choose1}]
% {e_1 \arb e_2 \Downarrow v}
% {e_1 \Downarrow v}
% \]

% \[
% \infer[{\sf ev/choose2}]
% {e_1 \arb e_2 \Downarrow v}
% {e_2 \Downarrow v}
% \]

% \end{minipage}
% \hspace{0.5cm}
% \begin{minipage}[b]{0.55\linewidth}
% \fvset{fontsize=\small,boxwidth=auto}
% \VerbatimInput{sls/cbv-ev-arb.sls}
% \end{minipage}
% \caption{Natural semantics for the lambda calculus (call-by-value).}
% \label{fig:ns}
% \end{figure}

\section{Logical transformation: operationalization}
\label{sec:operationalization}

The intuition behind operationalization is rather simple: we examine
the behavior of a deductive computation and then encode that
operational intuition as a concurrent computation. 

Before presenting the general transformation, we will motivate this
transformation using the call-by-value natural semantics for the
untyped lambda calculus:
\[
\infer[{\sf ev/lam}]
{\lambda x. e \Downarrow \lambda x. e \mathstrut}
{}
\quad
\infer[{\sf ev/app}]
{e_1\,e_2 \Downarrow v \mathstrut}
{e_1 \Downarrow \lambda x.e
 &
 e_2 \Downarrow v_2
 &
 [v_2/x]e \Downarrow v \mathstrut}
\]
The natural semantics are moded with $e$ as an input and $v$ as an
output, so it is meaningful to talk about being given a particular
expression $e$ and using deductive computation to search for a $v$
such that $e \Downarrow v$ is derivable. Consider a recursive search
procedure implementing this particular deductive computation:
\begin{itemize}
\item
      If $e = \lambda x. e'$, 
      it is possible to derive 
      $\lambda x. e' \Downarrow \lambda x. e'$
      with the rule ${\sf ev/lam}$.
\item
       If $e = e_1\,e_2$,
       attempt to derive 
       $e_1\,e_2 \Downarrow v$
       using the rule ${\sf ev/app}$ by doing the following:
    \begin{enumerate}
    \item Search for a $v_1$ such that 
          $e_1 \Downarrow v_1$ is derivable.
    \item Assert that $v_1 = \lambda x.e'$ for some
          $e'$; fail if it is not.
    \item Search for a $v_2$ such that 
          $e_2 \Downarrow v_2$ is derivable.
    \item Search for a $v$ such that 
          $[v_2/x]e \Downarrow v$ is derivable.
    \end{enumerate}
% \item
%       If $e = e_1 \arb e_2$,
%       attempt to derive $e_1 \arb e_2 \Downarrow v$ using
%       the rule ${\sf ev/choose1}$ by searching for a 
%       $v$ such that $e_1 \Downarrow v$ is derivable.
% \item
%       If $e = e_1 \arb e_2$,
%       attempt to derive $e_1 \arb e_2 \Downarrow v$ using 
%       the rule ${\sf ev/choose2}$ by searching for a 
%       $v$ such that $e_2 \Downarrow v$ is derivable.  
\end{itemize}
%
The goal of the operationalization transformation is to implement this
deductive computation as a concurrent computation. The first step in
doing so is to introduce two new ordered atomic propositions.  The
proposition ${\sf eval}\,\interp{e}$ is the starting point, indicating
that we want to search for a $v$ such that $e \Downarrow v$, and the
proposition ${\sf retn}\,\interp{v}$ indicates the successful
completion of this procedure. Therefore, searching for a $v$ such that
$e \Downarrow v$ is derivable will be analogous to building a trace $T
:: x_e{:}{\sf eval}\,\interp{e} \leadsto^* x_v{:}{\sf
  retn}\,\interp{e}$ with concurrent computation.

Representing the first case is straightforward: if we are evaluating
$\lambda x.e$, then we have succeeded and can return $\lambda x.e$. 
This is encoded in the rule ${\sf ev/lam}$. 
\[
{\sf ev/lam} : {\sf eval}\,({\sf lam}\,\lambda x.\,E\,x)
   \lefti \{ {\sf retn}\,({\sf lam}\,\lambda x.\,E\,x) \}
\]
Because the second rule involves both recursion and multiple subgoals,
we will generalize our picture of the process state to allow us to store a
stack of unfinished work in the ordered context, growing out to the
right. Our new understanding, then, is that contexts either have the
form $x{:}{\sf eval}\,\interp{e}, \Delta$ or the form $x{:}{\sf
  retn}\,\interp{v}, \Delta$. In the process of concurrently
computing a trace $x_e{:}{\sf eval}\,\interp{e_1\,e_2}, \Delta
\leadsto^* x_r{:}{\sf retn}\,\interp{v}, \Delta$, each of the
recursive calls to the search procedure will involve a sub-trace of the
form
%
\[x_e{:}{\sf eval}\,\interp{e'}, y{:}\istrue{A^-}, \Delta
  \leadsto^*
  x_r{:}{\sf retn}\,\interp{v'}, y{:}\istrue{A^-}, \Delta\]
%
where $A^-$ is a negative proposition that is prepared to interact
with the final ${\sf retn}\,\interp{v'}$ proposition to kickstart the
rest of the computation.

It's helpful to work backwards: in the fourth step, we have found
$E\,x = \interp{e}$ and $V_2 = \interp{v_2}$, and the recursive call
is to ${\sf eval}\,\interp{[v_2/x]e}$, which is the same thing as
${\sf eval}\,(E\,V_2)$. If the recursive call successfully returns,
the context will contain a proposition of the form ${\sf retn}\,V$
where $V = \interp{v}$, and the search procedure as a whole will be
complete with the answer being $v$.  Thus, the negative proposition
that implements the continuation can be written as $(\forall v. {\sf
  retn}\,v \lefti \{ {\sf retn}\,v \})$. The positive proposition that
will create this sub-computation can be written as follows:
\begin{align*}
{\it Step_4}(E,V_2) & \equiv {\sf eval}\,(E\,V_2) 
\fuse {\downarrow}(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})
%
\intertext{Moving backwards, in the third step we have a $E_2 =
  \interp{e_2}$ that we were given and $E\,x = \interp{e}$ that we
  have computed. The recursive call is to ${\sf
    eval}\,\interp{e_2}$, and assuming that it completes, we need
  to begin the fourth step. The positive proposition that will 
  create this sub-computation can be written as follows:}
%
{\it Step_3}(E_2,E) & \equiv {\sf eval}\,E_2 
\fuse {\downarrow}(\forall v. {\sf retn}\,v_2 \lefti \{ {\it Step_4}(E,v_2) \})
%
\intertext{Finally, the first two steps can be handled together. We have
$E_1 = \interp{e_1}$ and $E_2 = \interp{e_2}$; the recursive
call is to ${\sf eval}\,\interp{e_1}$. Once the
recursive call completes, we can enforce that the returned value has
the form $\interp{\lambda x.e}$ in the continuation before proceeding
to the continuation.}
{\it Step_{1,2}}(E_1, E_2) & \equiv {\sf eval}\,E_1
\fuse {\downarrow}(\forall e. {\sf retn}\,({\sf lam}\,\lambda x.\,e\,x)
\lefti \{ {\it Step_3}(E_2, e)\})
\end{align*}
Thus, the rule implementing this entire portion of the search
procedure is 
\[
\forall e_1.\,\forall e_2.\,
{\sf eval}\,({\sf app}\,E_1\,E_2) \righti \{ {\it
  Step_{1,2}}(E_1, E_2) \}
\]
The \sls~encoding of our example natural
semantics is shown in Figure~\ref{fig:example-transform-cbv} alongside
the transformed specification, which has the form of an ordered
abstract machine semantics.

\begin{figure}

%\fvset{fontsize=\small,boxwidth=229pt}
\fvset{fontsize=\small,boxwidth=187pt}
\BVerbatimInput{sls/cbv-ev.sls}
\fvset{fontsize=\small,boxwidth=auto}
\BVerbatimInput{sls/cbv-ev-ssos.sls}

\caption{A natural semantics for CBV and the corresponding abstract machine.}
\label{fig:example-transform-cbv}
\end{figure}

The intuitive connection between natural semantics specifications and
concurrent specifications has been explored previously and
independently by Schack-Nielsen \cite{schacknielsen07induction} and by
Cruz and Favonia \cite{cruz12parallel}; Schack-Nielsen proves the
equivalence of the two specifications, whereas Cruz and Favonia used
the connection informally. The contribution of this chapter is to
describe a general transformation (of which
Figure~\ref{fig:example-transform-cbv} is one instance) and to prove
this transformation correct in general. From the other direction,
Hannan and Miller \cite{hannan92operational} and Ager
\cite{ager04natural} have also proposed the idea of operationalizing
natural semantics specifications as abstract machines by provably
correct and general transformations on logical specifications (in the
case of Hannan and Miller) and the special-purpose framework of
L-attributed natural semantics (in the case of Ager). The major
difference in this case is that both lines of work result in {\it
  deductive} specifications of abstract machines. Our translation into
concurrent specification has the advantage of exploiting parallelism,
as demonstrated in Section~\ref{sec:trans-par} below, and also opens
up specifications to the modular inclusion of stateful and concurrent
features, as we will discuss in
Section~\ref{sec:richer-ordered-abstract}.

In Section~\ref{sec:trans-subset} we will present the subset of
specifications that our transformation handles, in
Section~\ref{sec:trans-basic} we will give the basic transformation
and prove the transformation correct, and in
Sections~\ref{sec:trans-tail}~and~\ref{sec:trans-par} we present
tail-recursive and parallelism-enabling variants of the transformation.

\subsection{Transformable specifications}
\label{sec:trans-subset}

The starting point for the operationalization transformation is a
deductive specification that is well-moded in the sense described in
Section~\ref{sec:framework-modes}. Every declared negative predicate
will either remain defined by deductive proofs (we write the atomic
propositions built with these predicates as $p_d^-$, $d$ for
deductive) or will be transformed so that it is concurrently defined
(we write the atomic propositions built with these predicates as
$p_c^-$, $c$ for concurrent).

For the purposes of describing and proving the correctness of the
operationalization transformation, we will assume that all transformed
atomic propositions $p_d^-$ have two arguments where the first
argument is moded as an input and the second is an output. That is,
their predicates are declared as follows:
\begin{align*}
& {\sf a} : \tau_1 \rightarrow \tau_2 \rightarrow {\sf prop}.\\
& {\sf \#mode~a~{+}~{-}}.
\end{align*}
Without dependency, the two-place case is sufficient to for
describing $n$-place relations.\footnote{As an example, to handle
  addition on natural numbers, defined as a three-place relation ${\sf
    add} : {\sf nat} \rightarrow {\sf nat} \rightarrow {\sf nat}
  \rightarrow {\sf type}$ with its usual mode (${\sf
    add}~{+}~{+}~{-}$), we define a unique type ${\sf add\_in}$ with
  one binary constructor ${\sf add\_c} : {\sf nat} \rightarrow {\sf
    nat} \rightarrow {\sf add\_in}$. Then we can declare (${\sf add'}
  : {\sf add\_in} \rightarrow {\sf nat} \rightarrow {\sf type}$) with
  mode (${\sf add'}~{+}~{-}$).}  It should be possible to handle
dependent predicates (that is, those with declarations of the form
${\sf a} : \Pi x{:}\tau_1.\,\tau_2(x) \rightarrow {\sf type}$) but we
will not do so here.

The restriction on signatures furthermore enforces that all rules must
be of the form ${\sf r} : C$ or ${\sf r} : D$, where $C$ and $E$ are
refinements of the negative propositions of \sls~that are defined as
follows:
\begin{align*}
C & ::= p^-_{c} 
    \mid \forall x{:}\tau.\, C
    \mid p^+_\mpers \lefti C
    \mid {!}p^-_c \lefti C
    \mid {!}G \lefti C \\
D & ::= p^-_{d}
    \mid \forall x{:}\tau.\, D
    \mid p^+_\mpers \lefti D
    \mid {!}p^-_c \lefti D
    \mid {!}G \lefti C \\
G & ::= p^-_d 
    \mid \forall x{:}\tau.\, G
    \mid p^+_\mpers \lefti G
    \mid {!}D \lefti G
\end{align*}
If {\it all} propositions are to remain deductive, then the
propositions $p^-_c$ and $C$ are irrelevant, and this restriction
describes all persistent, deductive specifications -- essentially, any
specification that could be executed by the standard logic programming
interpretation of LF \cite{pfenning98elf}. On the other hand, if all
propositions are to be transformed, then the propositions $p^-_d$ and
$D$ are irrelevant and this restriction amounts to restricting
specifications to the Horn fragment.

All propositions $C$ are equivalent (at the level of synthetic
inference rules) to propositions of the form $\forall
\overline{x_0}\ldots \forall \overline{x_n}.\,A^+_n \lefti \ldots
\lefti A^+_1 \lefti {\sf a}\,t_{0}\,t_{n-1}$,
where the $\forall \overline{x_i}$ are shorthand for a series of
universal quantifiers $\forall {x_{i1}}{:}{\tau_{i1}} \ldots \forall
{x_{\it ik}}{:}{\tau_{\it ik}}$ and where each variable in
$\overline{x_i}$ does not appear in $t_0$ (unless $i = 0$)
nor in any $A^+_i$ with $j < i$ but does appear in either $A^+_i$ (or
$t_0$ if $i = 0$). Therefore, when we consider moded proof
search, the variables bound in $\underline{x_0}$ are all fixed by the
query and those bound in the other $\underline{x_i}$ are all fixed by
the output position of the $i^{\rm th}$ premise.

\subsection{Basic transformation}
\label{sec:trans-basic}

The basic operationalization transformation $\transbasic{\Sigma}$
operates on signatures $\Sigma$ that have the form described above. We
will first give the transformation on signatures; the transformation
of rule declarations ${\sf r} : C$ is the most important and involves
a definition that will be presented next.

\begin{itemize}
\item $\transbasic{\cdot} = \cdot$
\item $\transbasic{\Sigma, {\sf a} : \tau_1 \rightarrow \tau_2
    \rightarrow {\sf prop}} = \transbasic{\Sigma}, {\sf eval\_a} :
  \tau_1 \rightarrow {\sf prop\,ord}, {\sf retn\_a} : \tau_2
  \rightarrow {\sf prop\,ord}$ \\ {\it (if $\sf a$ is one of the
    predicates that we are translating)}
\item $\transbasic{\Sigma, {\sf a} : K} = \transbasic{\Sigma}, {\sf a}
  : K$ {\it (otherwise)}
\item $\transbasic{\Sigma, {\sf c} : \tau} = \transbasic{\Sigma}, {\sf
    c} : \tau$ 
\item $\transbasic{\Sigma, {\sf r} : C} = \transbasic{\Sigma}, {\sf r}
  : \forall \overline{x_0}.\, {\sf eval\_a}\,t_0 \lefti \llbracket A^+_1,
  \ldots, A^+_n \rrbracket (t_{n+1}, {\sf id})$ \\ {\it (where $C$ is
    equivalent to $\forall \overline{x_0}\ldots \forall
    \overline{x_n}.\, A^+_n \lefti \ldots \lefti A^+_1 \lefti {\sf
      a}\,t_{0}\,t_{n+1}$)}
\item $\transbasic{\Sigma, {\sf r} : D} = \transbasic{\Sigma}, {\sf r}
  : D'$\\{\it (where $D'$ is the proposition $D$ where all instances of
  $p^-_c = {\sf a}\,t_1\,t_2$ have been replaced by ${\sf
    eval\_a}\,t_1 \lefti \{ {\sf retn\_a}\,t_2 \}$)}
\end{itemize}

The transformation of a proposition $C$ of the form $\forall
\overline{x_0}\ldots \forall \overline{x_n}.\,A^+_n \lefti \ldots
\lefti A^+_1 \lefti {\sf a}\,t_{0}\,t_{n+1}$ involves the definition
$\opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma}$, where $\sigma$
substitutes only for variables in $\overline{x_j}$ where $j < i$. The
function is defined inductively on the length of the sequence
$A^+_i,\ldots,A^+_n$.

\begin{itemize}
\item $\opbasic{}{t_{n+1}}{\sigma} = \{ {\sf retn\_a}\,(\sigma{t_{n+1}}) \}$
\item $\opbasic{p^+_\mpers,A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma} 
  = \forall \overline{x_i}.\, \sigma{p^+}_\mpers \lefti \opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma}$
\item $\opbasic{{!}p^-_c,A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma}$
  \\
  $~ \qquad = \{ {\sf eval\_b}\,({\sigma}t^{\it in}_i) \fuse
  (\forall\overline{x_i}.\, {\sf retn\_b}\,(\sigma{t^{\it out}_i})
  \lefti \opbasic{A^+_{i+1},\ldots,A^+_n}{t_{n+1}}{\sigma}) \}$\\
  {\it (where $p^-_c$ is ${\sf b}\,t^{\it in}_i\,t^{\it out}_i$)}
\item $\opbasic{{!}G,A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma}
  = \forall \overline{x_i}.\, {!}\sigma{G} \lefti \opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma}$
\end{itemize}

\noindent
This operation is slightly more general than it needs to be to
describe the transformation on signatures, because the substitution
$\sigma$ will always just be the identity substitution ${\sf id}$.
Non-identity substitutions arise during the proof of correctness, which
is why we introduced them here.

We have already given an example of the this basic operationalization
transformation, as Figure~\ref{fig:example-transform-cbv} is an
instance of this transformation.

\subsubsection{Correctness}

XXX Correctness

\subsection{Tail-recursion}
\label{sec:trans-tail}

Consider again our motivating example, the procedure for that takes
expressions $e$ and searches for expressions $v$ such that $e
\Downarrow v$ is derivable. If we were to implement that procedure as
a functional program, the procedure would be {\it tail-recursive}. In
the procedure that handles the case when $e = e_1\,e_2$, the last step
is that the search procedure is involed recursively. If and when that
callee returns $v$, then the caller will also return $v$.

Tail-recursion is significant in functional programming because
tail-recursive calls can be implemented without allocating a stack
frame: when a compiler makes this more efficient choice, we say it is
performing {\it tail-recursion optimization}.\footnote{Or {\it tail
    call optimization}, as a tail-recursive function call is just a
  specific instance of a tail call.} An analagous opportunity for
tail-recursion optimization also arises in our logical compilation
procedure. In our motivating example, the last step in the $e_1\,e_2$
case was operationlized as a positive proposition of the form ${\sf
  eval}\,(E\, V) \fuse (\forall v.\,{\sf retn}\,v \lefti \{ {\sf
  retn}\,v \})$. In a successful search, the process state 
\[ x{:}{\sf
  eval}\,(E\, V), y{:}\istrue{(\forall v.\,{\sf retn}\,v \lefti \{
  {\sf retn}\,v \})}, \Delta\]
will concurrently compute until the
state 
\[ x'{:}{\sf retn}\,V, y{:}\istrue{(\forall v.\,{\sf retn}\,v \lefti
  \{ {\sf retn}\,v \})}, \Delta\] is reached, at which point the next
step \[y'{:}{\sf retn}\,V, \Delta\] is reached in one step by focusing
on $y$. 

If we instead operationalize the last step in the $e_1\,e_2$ case as
simply ${\sf eval}\,(E\,V)$, we will reach the same final state with
one less transition. The tail-recursion optimizing version of the
operationalization transformation, $\transtail{\Sigma}$, creates
concurrent computations that avoid these useless steps.

We cannot perform tail recursion in general. The obvious reason for
this to be the case is when the output of the last subgoal is
different from the output of the goal. For example, the rule ${\sf r}
: \forall{x}.\,\forall{y}.\,{!}{\sf a}\,x\,y \lefti {\sf a}\,({\sf
  c}\,x)\,({\sf c}\,y)$, will translate to
\[ {\sf r} : \forall{x}.\,{\sf eval\_a}\,({\sf c}\,x) \lefti \{ {\sf
  eval\_a}\,x \fuse (\forall y.\, {\sf retn\_a}\,y \lefti \{ {\sf
  retn\_a}\,({\sf c}\,y) \} ) \} \] There is no opportunity for
tail-recursion optimization, because the output of the last search
procedure, $t^{\it out}_n = y$, is different than the value returned
down the stack, $t_{n+1} = {\sf c}\,y$. This case corresponds to
functional programs that cannot be tail-call optimized.

More subtly, we cannot even eliminate all cases where $t^{\it out}_n =
t_{n+1}$ unless these functions are {\it fully general}. We say that
$t_{n+1}$ with type $\tau$ is fully general if all of its free
variables are in $\overline{x_n}$ (and therefore not fixed by the
input or any other premises) and if, for any variable-free term $t'$
of type $\tau$, there exists a substituion $\sigma$ such that $t =
{\sigma}t_{n+1}$. The simplest example way to do this is to force
$t_{n+1} = t^{\it out}_n = y$ where $y = \overline{x_n}$, but it is
also possible to have a fully general $t_{n+1} = {\sf c}\,y_1\,y_2$
if, for instance, ${\sf c}$ has type $\tau_1 \rightarrow \tau_2
\rightarrow {\sf foo}$ and there are no other constructors of type
${\sf foo}$. This condition doesn't have an analouge in functional
programming, because it corresponds to the possibility that 
moded deduction computation can perform pattern matching on 
{\it outputs}.

The tail-recursive procedure can be described by adding a new 
case to the definition of 
$\opbasic{A^+_i,\ldots,A^+_n}{t_{n+1}}{\sigma}$:

\begin{itemize}
\item $\opbasic{{!}{\sf a}\,t^{\it in}_n\,t_{n+1}}{t_{n+1}}{\sigma} 
  = \{{\sf retn\_a}\,(t^{\it in}_n)\}$
\\
  {\it (where $t_{n+1}$ is fully general)}
\end{itemize}
This case overlaps with the third case of the definition given
in Section~\ref{sec:trans-basic}, which represents that tail-recursion
optimization can be applied or not in a non-determinstic manner.

\subsubsection{Example}

A dramatic illustration of tail-call optimization can be given if we
consider a big-step evaluation function that is based on a small-step
structural operational semantics (SOS) specification. SOS
specifications involve an inductively proposition encoding single-step
evaluation ${\sf step} : {\sf exp} \rightarrow {\sf exp} \rightarrow
{\sf prop}$ (moded ${\sf exp}\,{+}\,{-}$), as well as an is-value
function ${\sf value} : {\sf exp} \rightarrow {\sf prop}$ (moded ${\sf
  value}\,{+}$). We will not define these propositions here, but 
we do so later on in Section~\ref{}.

Given the small-step ${\sf step}$ predicate, it is easy to define 
big-step evaluation ${\sf ev}\,\interp{e}\,\interp{v}$ as a series of
small steps:

\smallskip
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-sos-steps.sls}
\smallskip

\begin{figure}
\begin{minipage}[b]{0.55\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-sos-proc2.sls}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-sos-proc.sls}
\end{minipage}
\caption{The transformation of a trivial big-step semantics, both
  without (left) and with (right) tail-recursion optimization.}
\label{fig:sos-tailrecursion}
\end{figure}

Running this specification through the operationalization
transformation and only operationalizing the ${\sf ev}$ predicate
results in what I consider to be the strictly most boring
substructural operational semantics specification, shown in
Figure~\ref{fig:sos-tailrecursion} both without the tail-recursion
optimization (left) and with the tail-recursion optimization (right).

The tail-recursion optimized translation is definitely superior for
this example. Concurrent proofs for the non-tail-recursion-optimized
specification build up an enormous stack of useless copies of the
proposition $(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})$:
\begin{align*}
& x_1{:}{\sf eval}\,\interp{e_1} \\
\leadsto ~ & x_2{:}{\sf eval}\,\interp{e_2}, 
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} \\
\leadsto ~ & x_3{:}{\sf eval}\,\interp{e_3}, 
  y_2{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})}, 
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} \\
\leadsto ~ & \cdots\\
\leadsto ~ & x_n{:}{\sf eval}\,\interp{v}, 
  y_{n-1}{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})}, 
  \cdots,
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} \\
\leadsto ~ & z_n{:}{\sf retn}\,\interp{v}, 
  y_{n-1}{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})}, 
  \cdots,
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} \\
\leadsto ~ & \cdots \\
\leadsto ~ & z_3{:}{\sf retn}\,\interp{v}, 
  y_2{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})}, 
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} \\
\leadsto ~ & z_2{:}{\sf retn}\,\interp{v}, 
  y_1{:}\istrue{(\forall v. {\sf retn}\,v \lefti \{ {\sf retn}\,v \})} \\
\leadsto ~ & z_1{:}{\sf retn}\,\interp{v}
\end{align*}
In contrast, the tail recursion optimized version on the right hand
side of Figure~\ref{fig:sos-tailrecursion} takes half as many steps,
and each step is smaller, simpler, and the overall trace does a better
job of actually capturing the linear computation that is actually
involved in describing a big-step semantics using a small-step
structural operational semantics:
\[
x_1{:}{\sf eval}\,\interp{e_1} 
 ~\leadsto~
x_2{:}{\sf eval}\,\interp{e_2}
 ~\leadsto~
x_3{:}{\sf eval}\,\interp{e_3}
 ~\leadsto~ \cdots ~\leadsto~
x_n{:}{\sf eval}\,\interp{v}
 ~\leadsto~ 
z{:}{\sf retn}\,\interp{v}
\]

\subsubsection{Correctness}

XXX Correctness

\subsection{Parallelism}
\label{sec:trans-par}

\subsection{Non-determinism and the natural semantics}

For the purposes of illustration, we will extend the language of
expressions with a nondeterministic choice operator $e_1 \arb e_2$
(encoded in LF as ${\sf choose}\,\interp{e_1}\,\interp{e_2}$) and an
extra expression ${\sf junk}$ that does not evaluate to anything: this
ensures that stuck states are possible, as search for a $v$ such that
$(\lambda x.\,x)\,{\sf junk} \Downarrow v$ is derivable will fail.

\begin{figure}[t]
\begin{minipage}[b]{0.45\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-arb.sls}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.55\linewidth}
\fvset{fontsize=\small,boxwidth=auto}
\VerbatimInput{sls/cbv-arb-ssos.sls}
\end{minipage}
\caption{Natural semantics for the lambda calculus (call-by-value).}
\label{fig:ns-arb}
\end{figure}

About the only thing we specified was parallel search for successful
proofs or not: we can handle the evaluation of $e_1 \arb e_2$ either
by committed choice (picking one of the two expressions $e_1$ or $e_2$
to evaluate and never reconsidering that decision), by backtracking
(attempting to evaluate one of $e_1$ or $e_2$, then attempting to
evaluate the other one if that first choice gets stuck) or by
breadth-first search (looking for derivations of $e_1$ and $e_2$ at
the same time), then a backtracking backward-chaining interpreter or a
breadth-first

. However

The operationalization transformation 



\section{Logical transformation: defunctionalization}
\label{sec:defunctionalization}

\section{Natural semantics and abstract machines}
\label{sec:nat-ssos-adequacy}

\[
\infer
{k \rhd (\lambda x. e) ~\mapsto~ k \lhd (\lambda x. e) \mathstrut} 
{}
\quad
\infer
{k \rhd (e_1\,e_2) ~\mapsto~ (k; \Box\,e_2) \rhd e_1 \mathstrut}
{}
\]\[
\infer
{(k; \Box\,e_2) \lhd (\lambda x.e) ~\mapsto~ (k; (\lambda x.e)\Box) \rhd e_2
 \mathstrut}
{}
\quad
\infer
{(k; (\lambda x.e)\Box) \lhd v_2 ~\mapsto~ k \rhd [v_2/x]e
 \mathstrut}
{}
\]




\subsection{Adequacy}

XXX REREAD ANDERS'S PAPER

To make a carefully-chosen analogy, various on-paper styles of
specifying the operational semantics are well-known and visually
recognizable.  To anyone literate in the conventions of programming
languages researchers, a quick glance should be sufficient to classify
the following two rules as natural semantics (or big-step semantics)
for the call-by-value lambda calculus:


Natural semantics are a clean, high-level, and declarative way of
describing the semantics of a simple, pure programming languages, but
they do not scale particularly well with the addition of effects like
state and exceptions. Worse, natural semantics are mostly hopeless in
the face of languages features that incorporate nondeterminism or
advanced control (such as first-class continuations). 

Thus, a researcher interested in a simple, high level specification of
the core features of a functional programming language might
reasonably predict that natural semantics would be a good solution to
their problem; one example is Murphy VII, who used natural semantics
for the high-level formalization of Lambda 5 in his thesis
\cite{murphy08modal}.

Another style used to specify the operational semantics of programming
languages, he {\it abstract
  machine} semantics, is slightly less canonical but nevertheless has
an identifiable set of conventions. The following is an abstract 
machine semantics for our call-by-value lambda calculus:


 for programming languages, t, is slightly less canonical, but abstract machine
specifications nevertheless also have a set of common
conventions. There are two states in an abstract machine
specification, $k \rhd e$ (the expression $e$ is evaluating on top of
stack $k$) and $k \lhd v$ (the value $v$ is being returned to the top
of the stack $k$). The first rule says that, as a function $\lambda x.e$
is already a value, we proceed by returning it to the stack, whereas
for an application $e_1\,e_2$, 

\begin{figure}
\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/cbv-ev-ssos-fun.sls}
\caption{An ordered abstract machine semantics for the CBV lambda calculus.}
\end{figure}

Input

\fvset{fontsize=\small,boxwidth=229pt}
\VerbatimInput{sls/cbv-ev-ssos-gen.sls}

\begin{theorem}
If $x{:}\istrue{\sf gen} \leadsto \Delta$
\end{theorem}

There needs to be some structure and
at least


the design space
of 

 the general idea
of representing intermediate states of a computation as 
substructural process states 

This chapter represents joint work with Ian Zerny.


\subsection{The functional correspondence}

The potential design space of substructural operational semantics is
quite large.  In order to make sense of this space, it is helpful to
have design principles that allow us to both {\it classify} different
styles of presentation and {\it predict} what style(s) we should adopt
based on what our goals are. I propose that different styles of
\sls~specification can be productively classified in terms of the
transformations that turn one classification style into another. Most
transformations will not have inverses, so this methodology gives us a
formal notion of which styles are more expressive than others.
Considering logical transformations also can lower the cost of
mis-prediction. If one begins a development in an overly-restrictive
style, the development can be transformed into a more expressive style
by an automatic transformation.

The organizing principle that we put forth in this chapter and the
next one is called the {\it logical correspondence}, by analogy with
the {\it functional correspondence} of Ager, Danvy, Midtgaard, and
others~\cite{ager03functional,ager04functional,ager05functional,
  danvy08defunctionalized} in which existing styles of specification
are related to each other by way of transformations on functional
programs. It is not our goal here to detail the parallels between the
logical and functional correspondence; rather, we want to emphasize
the use of the logical correspondence to relate substructural
operational to each other and to the natural semantics.

Natural semantics specifications are not substructural operational
specifications: the LF encodings of such specifications can generally
be done just as well in LF or in the purely-persistent, non-modal
fragment of \sls. It is not my purpose to advocate for natural
semantics; recall that natural semantics were used to illustrate
problems with {\it non}-modularity in language specification in
Section~\ref{sec:modularnonmodular}. Instead, we are trying to


de-mystify operational semantics by showing that they are 

Instead, we are formalizing the
intuition that, on a ad-hoc basis, it is clear how t 

But natural semantics are
quite standard and widely understood, and 


 and are easy to talk about
using 

reasonably illustrate


\section{Exploring the richer fragment}
\label{sec:richer-ordered-abstract}

\subsection{Mutable storage}
\label{sec:mutable-storage}

No check for pointer inequality! This is a fundamental restriction of
the fact that we're using existential quantification rather than some
form of nominal quantification. (Hack due to Favonia and Bob, personal
communication, but dates back earlier - was it one of Karl's papers?
Cheney cites it in nominal abstraction.)

\subsection{Call-by-need evaluation}

\subsection{Recoverable failure}

\subsection{Environment semantics}

\subsection{Looking back at natural semantics}
\label{sec:enriching-natsem}

\section{Partial transformation}
\label{sec:othertransform}

\subsection{Evaluation contexts}

Thus far, we have considered big-step operational semantics and abstract
machines, neglecting the third great tradition of programming language
specification, {\it structural operational semantics}. Structural
operational semantics (SOS) define single-step evaluation inductively over
the structure of expressions; the SOS semantics for our running example
language is the following:
\[
\infer
{\lambda x.e\,{\sf value} \mathstrut}
{}
\quad
\infer
{e_1\,e_2 \mapsto e_1'\,e_2 \mathstrut}
{e_1 \mapsto e_1' \mathstrut}
\quad
\infer
{e_1\,e_2 \mapsto e_1\,e_2' \mathstrut}
{e_1\,{\sf value}
 &
 e_2 \mapsto e_2' \mathstrut}
\quad
\infer
{(\lambda x. e)v \mapsto [v/x]e \mathstrut}
{v\,{\sf value} \mathstrut}
\]
This inductive specification is adequately encoded on the left-hand
side of Figure~\ref{fig:cbv-sos}, along with the proposition \Verb|ev|
that describes a big-step operational semantics in terms of repeated
application of the small-step operational semantics.

\begin{figure}[tp]
\fvset{fontsize=\small,boxwidth=229pt}
\BVerbatimInput{sls/cbv-sos.sls}
\BVerbatimInput{sls/cbv-sos-eval.sls}
\caption{Small-step evaluation, and one corresponding abstract machine.}
\label{fig:cbv-sos}
\end{figure}

\fvset{fontsize=\small}

There are a couple of possibilities for how the 
One obvious way to proceed is to simply translate the big-step portion
of our semantics as encoded 


If we just translate the ${\sf ev}$ portion of the semantics (using
the tail-recursion optimizing translation), then we will get what is
probably fair to call the most boring possible substructural
operational semantics: 

\smallskip
\VerbatimInput{sls/cbv-sos-proc.sls}
\smallskip

\noindent
Under this semantics, the substructural context contains a single
resource, \Verb|eval-steps(E)|, which takes steps according to the
rules of the small-step structural operational semantics until a value
is reached, at which point the context contains \Verb|retn-steps(V)|.


\begin{figure}[t]
\VerbatimInput{sls/cbv-sos-defun.sls}
\caption{The defunctionalized abstract machine from Figure~\ref{fig:cbv-sos}.}
\label{fig:cbv-sos-defun}
\end{figure}

The interesting observations are to be had from the other direction: what if

\subsection{Temporal logic}

The natural semantics of \rowan~are not, on a superficial level,
significantly more complex than other natural semantics. However, it
turns out that the usual set of techniques for adding state to a
natural semantics break down, and discussing a \rowan-like logic with
state remained a challenge for many years.\robnote{Figure out from
  Rowan what the recent work he told you about was.} Through the
logical correspondence, it is easy to see why: the natural SSOS
specification of \rowan~integrates both concurrent and deductive
reasoning in an arbitrarily nested way. In fact, Figure XXX is the
only SLS specification in this thesis that exhibits this form of
recursive dependency between concurrent and deductive reasoning.  In
particular, the \rowan~specification is way out of the image of the
extended natural semantics we considered in
Section~\ref{sec:enriching-natsem}. The natural encoding in state lies
in the ambient substructural context of a concurrent computation, but
that ambient computation cannot properly enter into a deductive
sub-computation. If we tried to add state to \rowan~the same way we
added it in Section~\ref{sec:mutable-storage}, the entire store
would effectively leave scope whenever computation considered
the subterm $e$ of ${\sf next}(e)$. That consideration happens
as deductive reasoning, not as concurrent reasoning!

 it is the only we
will consider in this thesis that has with property.

It's hard to include state in temporal logic! But the logical correspondence
helps us understand why: the natural SSOS specification of 